[
  {
    "objectID": "pages/cnns.html",
    "href": "pages/cnns.html",
    "title": "4 - Convolutional Neural Networks",
    "section": "",
    "text": "CNNs work similarly to MLPs: They consist of neurons with weights and biases arranged in layers. CNNs also have an output with which a differentiable loss function can be calculated so that the weights and biases can be adjusted using backpropagation.\nUnlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).\nThe input to an MLP is a vector \\(\\mathbf{x}^{(i)}\\), which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers), see Figure 1.\n\n\n\n\n\n\nFigure 1: Source: Li (2022)\n\n\n\nThe fully connected layers can only process 1-D vectors. Therefore, images \\(\\in \\mathbb{R}^{H \\times W \\times C}\\) must be flattened into 1-D vectors \\(\\in \\mathbb{R}^p\\). Here, \\(p= H \\times W \\times C\\). This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see Figure 3 for an illustration in an MLP and Figure 2 for an illustration on a linear model).\n\n\n\n\n\n\nFigure 2: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 3: Source: Li (2023)\n\n\n\nFor larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.\nA single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see Figure 4). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.\n\n\n\n\n\n\nFigure 4: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.\n\n\n\n\nCNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.\nFigure 5 shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.\n\n\n\n\n\n\nFigure 5: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: Li (2022).\n\n\n\nSometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block."
  },
  {
    "objectID": "pages/cnns.html#architecture",
    "href": "pages/cnns.html#architecture",
    "title": "4 - Convolutional Neural Networks",
    "section": "",
    "text": "CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.\nFigure 5 shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.\n\n\n\n\n\n\nFigure 5: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: Li (2022).\n\n\n\nSometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block."
  },
  {
    "objectID": "pages/cnns.html#hyper-parameters",
    "href": "pages/cnns.html#hyper-parameters",
    "title": "4 - Convolutional Neural Networks",
    "section": "Hyper-Parameters",
    "text": "Hyper-Parameters\nTo define a convolutional layer, various hyperparameters need to be set. Some of the most important ones are:\n\nDepth\nPadding\nStride\nKernel Size\n\nDepth determines how many filters are to be learned and thus defines the dimensionality (\\(C_{\\text{out}}\\)) of the output volume (the number of activation maps).\nStride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation. If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height.\nPadding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. Figure 11 illustrates the problem. Figure 12 shows an example with padding.\nFigure 13 shows the interplay between stride and padding.\n\n\n\n\n\n\nFigure 11: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 12: Left: Input (yellow) with zero-padding (1, 1) (white border), middle: Filter, right: Output.\n\n\n\n\n\n\n\n\n\nFigure 13: Stride with padding. The red mark indicates the filter value position on the input activations.\n\n\n\nDumoulin and Visin (2016) has created some animations for better understanding of convolutions and published them here: https://github.com/vdumoulin/conv_arithmetic.\n\nCalculations\nThe dimensionality of the activation maps can be calculated using the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernels)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along the spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along the spatial dimensions)\n\nScenario: stride = 1 and without padding\n\\[\\begin{equation}\no = (i - k) + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 14: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\nScenario: stride = 1 with padding\n\\[\\begin{equation}\no = (i - k) + 2p + 1\n\\end{equation}\\]\nScenario: Half (same) Padding -&gt; \\(o = i\\)\nValid for any \\(i\\) and for odd \\(k = 2n + 1, n ∈ N\\), \\(s = 1\\) and \\(p = \\lfloor k/2\\rfloor = n\\).\n\\[\\begin{align}\no &= i + 2 \\lfloor k/2\\rfloor − (k − 1) \\\\\no &= i + 2 n - 2 n \\\\\no &= i\n\\end{align}\\]\n\n\n\n\n\n\nFigure 15: Convolving a 3x3 kernel over a 5x5 input with 1x1 padding and stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\nScenario: Full Padding\nThe dimensionality of the output activation can also be increased.\nValid for any \\(i\\) and \\(k\\), \\(s = 1\\) and \\(p = k - 1\\).\n\\[\\begin{align}\no &= i + 2 (k − 1) - (k - 1) \\\\\no &= i + (k - 1)\n\\end{align}\\]\n\n\n\n\n\n\nFigure 16: Convolving a 3x3 kernel over a 5x5 input with 2x2 padding and stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\nScenario: No Padding, stride &gt; 1\nValid for any \\(i\\) and \\(k\\), \\(s &gt; 1\\) and \\(p = 0\\).\n\\[\\begin{equation}\no = \\lfloor\\frac{i - k}{s}\\rfloor + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 17: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 2x2. Source Dumoulin and Visin (2016)\n\n\n\nScenario: padding and stride &gt; 1\nValid for any \\(i, k, s, p\\).\n\\[\\begin{equation}\no = \\lfloor \\frac{i + 2p - k}{s} \\rfloor + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 18: Convolving a 3x3 kernel over a 5x5 input with 1x1 zero-padding and stride 2x2. Source Dumoulin and Visin (2016)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this formula, all scenarios are covered!\n\\[\\begin{equation}\no = \\lfloor \\frac{i + 2p - k}{s} \\rfloor + 1\n\\end{equation}\\]\n\n\n\n\nQuiz\n\n\n\n\n\n\nQuestion\nInput: 3 x 32 x 32\nConvolution: 10 filters with 5x5 kernel size, stride=1, and padding=2\nWhat is the size of the activation map?\n\n\n\n\n\n\n\n\n\nQuestion\nInput: 3 x 32 x 32\nConvolution: 10 filters with 5x5 kernel size, stride=1, and padding=2\nHow many weights are there?"
  },
  {
    "objectID": "pages/cnns.html#properties",
    "href": "pages/cnns.html#properties",
    "title": "4 - Convolutional Neural Networks",
    "section": "Properties",
    "text": "Properties\n\nLocal (Sparse) Connectivity & Parameter Sharing\nFully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.\nParameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.\n\n\n\n\n\n\nNote\n\n\n\nSometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).\n\n\nThe following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)\n        self.hidden_layer2 = nn.Linear(64, 32)\n        self.output_layer = nn.Linear(32, 10)\n     \n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.hidden_layer1(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x\n\nnet = MLP()\nprint(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n==========================================================================================\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.output_layer = nn.Linear(16 * 8 * 8 , 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.flatten(x)\n        x = self.output_layer(x)\n        return x\n\ncnn = CNN()\nprint(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n==========================================================================================\n\n\n\n\n\n\n\n\nFigure 19: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).\n\n\n\n\n\n\n\n\n\nQuestion\nHow should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?\n\n\n\n\n\nTranslation Invariance / Equivariance\nTranslation invariant is a function that produces the same value under translations \\(g()\\) of the input \\(x\\):\n\\[\\begin{equation}\nf(g(x))=f(x)\n\\end{equation}\\]\nTranslation equivariant is a function that produces the same value under translations \\(g()\\) of the input \\(x\\), provided that it is also shifted by \\(g()\\):\n\\[\\begin{equation}\nf(g(x))=g(f(x))\n\\end{equation}\\]\nConvolutions are translation equivariant, as illustrated well in the following example:\n\n\n\n\nStacking & Receptive Field\nMultiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. Figure 20 illustrates the result.\n\n\n\n\n\n\nFigure 20: Source: Johnson (2019)\n\n\n\nA convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality \\(H \\times W \\times C\\)! (There are also variants in higher dimensions.)\nHowever, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).\nThe receptive field defines which inputs influence the activations of a neuron. Figure 21 illustrates the concept.\n\n\n\n\n\n\nFigure 21: Source: Johnson (2019)\n\n\n\nStacking multiple convolutions increases the receptive field of a neuron relative to the original input (see Figure 22).\n\n\n\n\n\n\nFigure 22: Source: Johnson (2019)"
  },
  {
    "objectID": "pages/cnns.html#variations",
    "href": "pages/cnns.html#variations",
    "title": "4 - Convolutional Neural Networks",
    "section": "Variations",
    "text": "Variations\n\nDilated Convolutions\nDilated convolutions have an additional hyperparameter, the dilation. Dilated convolutions have holes between the rows and columns of a kernel. This increases the receptive field without having to learn more parameters. This variant is also called à trous convolution. Figure 23, Figure 24, and Figure 25 show examples.\n\n\n\n\n\n\nFigure 23: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2. Source Dumoulin and Visin (2016)\n\n\n\n\n\n\n\n\n\nFigure 24: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.\n\n\n\n\n\n\n\n\n\nFigure 25: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.\n\n\n\n\n\n1x1 (pointwise) Convolutions\n1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (\\(C\\)) of activation maps with few parameters. For example, activation maps of dimensionality (\\(C \\times H \\times W\\)) can be changed to a volume of (\\(C2 \\times H \\times W\\)) with \\(C2 * (C + 1)\\). This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (\\(C2=3\\)) for image generation models. Figure 26 shows an example.\n\n\n\n\n\n\nFigure 26: Source: Johnson (2019)\n\n\n\n\n\nDepthwise Separable Convolutions\nDepthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (\\(1 \\times K \\times K\\)). Figure 27 shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See Figure 28 for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.\n\n\n\n\n\n\nFigure 27: Source: https://paperswithcode.com/method/depthwise-convolution\n\n\n\n\n\n\n\n\n\nFigure 28: Source: Yu and Koltun (2016)"
  },
  {
    "objectID": "pages/cnns.html#pooling-layers",
    "href": "pages/cnns.html#pooling-layers",
    "title": "4 - Convolutional Neural Networks",
    "section": "Pooling Layers",
    "text": "Pooling Layers\nOften, the spatial dimensionality of the activation maps needs to be successively reduced in a CNN. This reduces the number of computations and memory required. Also, information is condensed and aggregated layer by layer: high spatial resolution is often no longer necessary. We have already seen that convolutional layers with a stride &gt; 1 can achieve this goal. However, it is also possible to use pooling layers, which do not have (learnable) parameters.\nA frequently used variant is the max-pooling layer. This layer operates independently on each slice along the depth dimension and returns the maximum value. The kernel size and stride must also be defined. A stride of \\(2 \\times 2\\) with a kernel of \\(2 \\times 2\\) halves the dimensionality of the activation maps along height and width.\nFor any \\(i,k,s\\) and without padding:\n\\[\\begin{equation}\no = \\lfloor\\frac{i - k}{s}\\rfloor + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 29: Source: Li (2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30: Input\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Max Pooling\n\n\n\n\n\n\n\n\n\n\n\nFigure 32: Average Pooling\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Global Average Pooling\n\n\n\n\n\n\nFigure 33 shows the result of max-pooling, average-pooling and global average-pooling. In average-pooling, instead of choosing the maximum, the average activation is calculated. Otherwise, average-pooling works the same way as max-pooling. A crucial pooling variant is global average pooling. The average of the activations is calculated along the depth dimension (i.e., no kernel size or stride needs to be defined). Activation maps with (\\(C \\times H \\times W\\)) are reduced to (\\(C \\times 1 \\times 1\\)). This is useful, for example, to directly model logits in a classification problem with \\(C\\) classes. This allows architectures to be created that do not use fully connected layers at all."
  },
  {
    "objectID": "pages/cnns.html#pytorch-examples",
    "href": "pages/cnns.html#pytorch-examples",
    "title": "4 - Convolutional Neural Networks",
    "section": "PyTorch Examples",
    "text": "PyTorch Examples\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nimport torchshow as ts\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n\n#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')\nimage_path = \"../assets/images/cnns/cat.jpg\"\nimg = Image.open(image_path)\nimg\n\n\n\n\n\n\n\n\n\nfilter = torch.tensor(\n    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B\n    ]).unsqueeze(0).float()\nts.show(filter, show_axis=False)\n\n\n\n\n\n\n\n\n\ninput = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)\ninput /= 255.0\ninput -= 1.0\nresult = F.conv2d(input, filter, stride=1, padding=0, dilation=1, groups=1)\n\n\nts.show(result)\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/torchshow/visualization.py:388: UserWarning:\n\nOriginal input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.\n\n\n\n\n\n\n\n\n\n\n2D-Convolution:\n\nresult = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)\nts.show(result)\n\n\n\n\n\n\n\n\nTransposed convolution:\n\nresult = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)\nresult = F.conv_transpose2d(result, weight=torch.ones_like(filter))\nts.show(result)\n\n\n\n\n\n\n\n\nMax-Pooling:\n\nresult = F.max_pool2d(input, kernel_size=8, stride=8)\nts.show(result)"
  },
  {
    "objectID": "pages/recent_advances.html",
    "href": "pages/recent_advances.html",
    "title": "8 - Foundation Models",
    "section": "",
    "text": "Foundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields."
  },
  {
    "objectID": "pages/recent_advances.html#how-clip-works",
    "href": "pages/recent_advances.html#how-clip-works",
    "title": "8 - Foundation Models",
    "section": "How CLIP Works",
    "text": "How CLIP Works\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs."
  },
  {
    "objectID": "pages/recent_advances.html#applications-of-clip",
    "href": "pages/recent_advances.html#applications-of-clip",
    "title": "8 - Foundation Models",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\nContent Moderation: CLIP can assist in identifying inappropriate content in images based on textual cues."
  },
  {
    "objectID": "pages/recent_advances.html#example",
    "href": "pages/recent_advances.html#example",
    "title": "8 - Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport torch\nimport clip\nfrom PIL import Image\n\n# Load the model and the preprocess function\nmodel, preprocess = clip.load(\"ViT-B/32\")\n\n# Load an image\nimage = preprocess(Image.open(\"path/to/your/image.jpg\")).unsqueeze(0)\n\n# Define a set of labels\nlabels = [\"a dog\", \"a cat\", \"a car\", \"a tree\"]\n\n# Tokenize the labels\ntext = clip.tokenize(labels)\n\n# Compute the image and text features\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\n# Compute the similarity between the image and each label\nsimilarities = (image_features @ text_features.T).softmax(dim=-1)\n\n# Print the most similar label\nprint(\"Label:\", labels[similarities.argmax().item()])"
  },
  {
    "objectID": "pages/recent_advances.html#how-vqa-works",
    "href": "pages/recent_advances.html#how-vqa-works",
    "title": "8 - Foundation Models",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers."
  },
  {
    "objectID": "pages/recent_advances.html#applications-of-vqa",
    "href": "pages/recent_advances.html#applications-of-vqa",
    "title": "8 - Foundation Models",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nEducational Tools: VQA systems can be used in educational applications to assist students in learning by providing answers to questions about visual content.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services."
  },
  {
    "objectID": "pages/recent_advances.html#example-1",
    "href": "pages/recent_advances.html#example-1",
    "title": "8 - Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of a VQA system using a hypothetical multi-modal model:\n\n# Hypothetical code for a Visual Question Answering system\nimport torch\nfrom PIL import Image\nfrom transformers import VQAModel, VQATokenizer\n\n# Load the model and the tokenizer\nmodel = VQAModel.from_pretrained(\"hypothetical-vqa-model\")\ntokenizer = VQATokenizer.from_pretrained(\"hypothetical-vqa-model\")\n\n# Load an image\nimage = Image.open(\"path/to/your/image.jpg\")\n\n# Define a question\nquestion = \"What is in the image?\"\n\n# Preprocess the image and the question\ninputs = tokenizer(image, question, return_tensors=\"pt\")\n\n# Get the model's answer\nwith torch.no_grad():\n    outputs = model(**inputs)\n    answer = outputs.logits.argmax(-1)\n\n# Print the answer\nprint(\"Answer:\", tokenizer.decode(answer))"
  },
  {
    "objectID": "pages/recent_advances.html#conclusion",
    "href": "pages/recent_advances.html#conclusion",
    "title": "8 - Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nFoundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape."
  },
  {
    "objectID": "slides_cas/practical.html#leaky-abstraction",
    "href": "slides_cas/practical.html#leaky-abstraction",
    "title": "Practical Considerations",
    "section": "Leaky Abstraction",
    "text": "Leaky Abstraction\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#silent-failure",
    "href": "slides_cas/practical.html#silent-failure",
    "title": "Practical Considerations",
    "section": "Silent Failure",
    "text": "Silent Failure\nTraining neural networks fails silently!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#get-to-know-the-data",
    "href": "slides_cas/practical.html#get-to-know-the-data",
    "title": "Practical Considerations",
    "section": "1) Get to Know the Data",
    "text": "1) Get to Know the Data\nThoroughly inspect the data!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#camera-traps-errors",
    "href": "slides_cas/practical.html#camera-traps-errors",
    "title": "Practical Considerations",
    "section": "Camera Traps: Errors",
    "text": "Camera Traps: Errors\n\n\n\nExamples of images from camera traps.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#camera-traps-difficulties",
    "href": "slides_cas/practical.html#camera-traps-difficulties",
    "title": "Practical Considerations",
    "section": "Camera Traps: Difficulties",
    "text": "Camera Traps: Difficulties\n\n\n\nExamples of images from camera traps. Source: Beery, Van Horn, and Perona (2018)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#rare-classes",
    "href": "slides_cas/practical.html#rare-classes",
    "title": "Practical Considerations",
    "section": "Rare Classes",
    "text": "Rare Classes\n\n\n\nAn image of a serval. Below are the model confidences.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#multiple-classes",
    "href": "slides_cas/practical.html#multiple-classes",
    "title": "Practical Considerations",
    "section": "Multiple Classes",
    "text": "Multiple Classes\n\nExamples of an image from a camera trap with different species.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-1",
    "href": "slides_cas/practical.html#baselines-1",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEvaluation pipeline, metrics, experiment tracking, and baseline model.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#ml-process",
    "href": "slides_cas/practical.html#ml-process",
    "title": "Practical Considerations",
    "section": "ML Process",
    "text": "ML Process\n\n\n\nThe components of a typical machine learning process. Source: Raschka and Mirjalili (2020)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#experiment-tracking",
    "href": "slides_cas/practical.html#experiment-tracking",
    "title": "Practical Considerations",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\n\n\n\nWeights and Biases experiment tracking.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-2",
    "href": "slides_cas/practical.html#baselines-2",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEnsure reproducibility.\nimport torch\ntorch.manual_seed(0)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-3",
    "href": "slides_cas/practical.html#baselines-3",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nAvoid unnecessary techniques and complexities. Reduce error susceptibility.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-4",
    "href": "slides_cas/practical.html#baselines-4",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nIf possible, use a human baseline. How good can the model be?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#difficult-cases",
    "href": "slides_cas/practical.html#difficult-cases",
    "title": "Practical Considerations",
    "section": "Difficult Cases",
    "text": "Difficult Cases\n\nAn image from a camera trap that is difficult to classify. Here, annotators had a 96.6% agreement with experts.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-5",
    "href": "slides_cas/practical.html#baselines-5",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nTrain an input-independent baseline. Is the model learning anything at all?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-6",
    "href": "slides_cas/practical.html#baselines-6",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nOverfit the model on a batch of data. Does the optimization work?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#baselines-7",
    "href": "slides_cas/practical.html#baselines-7",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nVisualize what goes into the model. Is my preprocessing working?\ny_hat = model(x)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#fixed-sample-segmentation-example",
    "href": "slides_cas/practical.html#fixed-sample-segmentation-example",
    "title": "Practical Considerations",
    "section": "Fixed Sample: Segmentation Example",
    "text": "Fixed Sample: Segmentation Example\n\n\n\nExample of a segmentation problem: input on the left and output on the right.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#overfit-1",
    "href": "slides_cas/practical.html#overfit-1",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nAt this point, you should have a good understanding of the dataset, high confidence in the evaluation pipeline, and initial baselines from simple models. Now, look for a model that performs well on the training set.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#overfit-2",
    "href": "slides_cas/practical.html#overfit-2",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nLook for a good model architecture. Follow the principle “Don’t be a hero”. Prefer already implemented/established architectures.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-1",
    "href": "slides_cas/practical.html#regularization-1",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAt this point, you should have achieved good performance on the training set. Now, focus on the validation set.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-2",
    "href": "slides_cas/practical.html#regularization-2",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nThe simplest measure to achieve better performance (and also reduce overfitting) is to collect more training data. However, this is often expensive!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#learning-curve",
    "href": "slides_cas/practical.html#learning-curve",
    "title": "Practical Considerations",
    "section": "Learning Curve",
    "text": "Learning Curve\nIs it worth collecting more data?\n\nExample of a learning curve. X-axis: Performance, Y-axis: Number of training samples. Left panel with Gaussian Naive Bayes and right panel with Support Vector Classifier.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-3",
    "href": "slides_cas/practical.html#regularization-3",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAnother possibility is data augmentation. New data points are generated from existing ones by making random changes to the data. Typically, data points are augmented on-the-fly.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#data-augmentation-augly",
    "href": "slides_cas/practical.html#data-augmentation-augly",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Augly",
    "text": "Data Augmentation: Augly\n\n\n\nAugLy",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#data-augmentation-albumentations",
    "href": "slides_cas/practical.html#data-augmentation-albumentations",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Albumentations",
    "text": "Data Augmentation: Albumentations\n\nAlbumentations",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#data-augmentation-kornia",
    "href": "slides_cas/practical.html#data-augmentation-kornia",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Kornia",
    "text": "Data Augmentation: Kornia\n\n\n\nKornia",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#data-augmentation-example",
    "href": "slides_cas/practical.html#data-augmentation-example",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Example",
    "text": "Data Augmentation: Example\n\n\n\nData augmentation example.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#data-augmentation-synthetic-data",
    "href": "slides_cas/practical.html#data-augmentation-synthetic-data",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Synthetic Data",
    "text": "Data Augmentation: Synthetic Data\n\n\n\nFrom Beery et al. (2020). Synthetic and semi-synthetic data.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-4",
    "href": "slides_cas/practical.html#regularization-4",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith early stopping, a model is trained and periodically evaluated on a validation set, e.g., after each epoch. Training is stopped if no significant improvement is achieved after x evaluation cycles.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#early-stopping",
    "href": "slides_cas/practical.html#early-stopping",
    "title": "Practical Considerations",
    "section": "Early Stopping",
    "text": "Early Stopping\n\n\n\nSource: Link",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-5",
    "href": "slides_cas/practical.html#regularization-5",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nEarly stopping in PyTorch.\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-6",
    "href": "slides_cas/practical.html#regularization-6",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith weight decay, a model can be regularized. The update step in gradient descent is modified.\n\\[\\begin{equation}\n\\theta_{t+1} = \\theta_t (1 - \\lambda) - \\eta \\nabla J(\\theta)\n\\end{equation}\\]\nWhere \\(t\\) is the iteration, \\(\\theta\\) the model parameters, \\(\\eta\\) the learning rate, and \\(\\lambda\\) the decay parameter.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-7",
    "href": "slides_cas/practical.html#regularization-7",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nTransfer learning involves adapting a pre-trained model on a large dataset (e.g., ImageNet) to a new task. The last layer is removed and replaced according to the new task. The network is then further trained. Layers can be frozen (weights not updated) or fine-tuned (weights further trained).",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#transfer-learning",
    "href": "slides_cas/practical.html#transfer-learning",
    "title": "Practical Considerations",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nSource: Johnson and Fouhey (2021)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#regularization-8",
    "href": "slides_cas/practical.html#regularization-8",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nIn PyTorch, you can freeze the parameters:\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#hyper-parameter-tuning",
    "href": "slides_cas/practical.html#hyper-parameter-tuning",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nIn this step, different hyperparameters and architectures are systematically evaluated. Techniques such as grid search or random search can be used, with random search being preferred.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#hyper-parameter-tuning-1",
    "href": "slides_cas/practical.html#hyper-parameter-tuning-1",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nParameterized architecture:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#squeeze-out-the-juice",
    "href": "slides_cas/practical.html#squeeze-out-the-juice",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nAfter finding the best architectures and hyperparameters, there are further ways to squeeze out more performance.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#squeeze-out-the-juice-1",
    "href": "slides_cas/practical.html#squeeze-out-the-juice-1",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nModel ensembling.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#squeeze-out-the-juice-2",
    "href": "slides_cas/practical.html#squeeze-out-the-juice-2",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nTrain longer.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#double-descent",
    "href": "slides_cas/practical.html#double-descent",
    "title": "Practical Considerations",
    "section": "Double Descent",
    "text": "Double Descent\n\nSource: Nakkiran et al. (2019)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#squeeze-out-the-juice-3",
    "href": "slides_cas/practical.html#squeeze-out-the-juice-3",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nOther training techniques:\n\nSpecial optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#huggingface",
    "href": "slides_cas/practical.html#huggingface",
    "title": "Practical Considerations",
    "section": "HuggingFace",
    "text": "HuggingFace\nHuggingFace",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#timm",
    "href": "slides_cas/practical.html#timm",
    "title": "Practical Considerations",
    "section": "timm",
    "text": "timm\nPyTorch Image Models (timm)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides_cas/practical.html#links",
    "href": "slides_cas/practical.html#links",
    "title": "Practical Considerations",
    "section": "Links",
    "text": "Links\n\nDS-cookie cutter\nPyTorch Lightning\nHydra\nWeights & Biases\nNeptune AI\nVersion Control Systems for ML Projects",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/demos.html",
    "href": "pages/demos.html",
    "title": "Demos",
    "section": "",
    "text": "CNN Filters Visualization\nVisualize filters and their effect of a pre-trained ResNet-18.\n\n  \n\n\n\nCLIP Demo\nThis model can calculate similarities between and among images and texts. It can be used for zero-shot (no labels) image classification.\n\n  \n\n\n\nVisual Question Answering\nTest a model that can answer questions given an image. The notebook contains a small model which can be run on cpu and a much larger model which ideally is run on GPU."
  },
  {
    "objectID": "pages/intro.html",
    "href": "pages/intro.html",
    "title": "1 - Introduction",
    "section": "",
    "text": "Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.\nThe integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services. Examples in the field of image processing with Deep Learning are shown in Figure 1, Figure 2, and Figure 3.\n\n\n\n\n\n\nFigure 1: Example from Link. Left is the original image, right is the version enhanced with Deep Learning.\n\n\n\n\n\n\n\n\n\nFigure 2: Example from Link. Left is the original image, right is the manipulated version.\n\n\n\n\n\n\n\n\n\nFigure 3: Example from Link. Left is the original image, right is the manipulated version.\n\n\n\n\n\n\n\n\n\nQuestion\nWhat steps do you think the model in Figure 3 performs?\n\n\n\nThe increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. Figure 4 shows that special chips have been developed to process images with Deep Learning models quickly.\n\n\n\n\n\n\nFigure 4: From Link.\n\n\n\n\n\nWe will now explore some challenges that must be overcome when analyzing images with machine learning models.\n\n\nThe semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.\n\n\n\n\n\n\nFigure 5: Illustration of the semantic gap.\n\n\n\n\n\n\nThe meaning of the image does not change with the viewpoint, but the pixels do.\n\n\n\n\n\n\nFigure 6: Source\n\n\n\n\n\n\nObjects are often flexible and appear in different shapes and poses.\n\n\n\n\n\n\nFigure 7: Source\n\n\n\n\n\n\nChanges in illumination affect pixel values and the visibility of objects.\n\n\n\n\n\n\nFigure 8: Source\n\n\n\n\n\n\nBackground pixels can resemble objects and make their exact delineation or visibility more difficult.\n\n\n\n\n\n\nFigure 9: Source\n\n\n\n\n\n\nObjects are not always fully visible, which can make their detection more difficult.\n\n\n\n\n\n\nFigure 10: Source\n\n\n\n\n\n\nObjects of the same class can exhibit large intra-class variability.\n\n\n\n\n\n\nFigure 11: Source\n\n\n\n\n\n\nFigure 12 shows that context information can be important to correctly classify an object.\n\n\n\n\n\n\nFigure 12: Context Source\n\n\n\n\n\n\n\nThere are various problem statements in image processing that can be modeled with Deep Learning. Below, we will see some of them.\n\n\nIn image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. Figure 13 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012) (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.\n\n\n\n\n\n\nFigure 13: Multi-Class Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\n\n\n\nObject detection involves locating and recognizing (multiple) objects in an image. Figure 14 shows an example from the paper by Redmon et al. (2016). Each object is localized with a bounding box and assigned an object class.\n\n\n\n\n\n\nFigure 14: Object Detection example (from Redmon et al. (2016)). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.\n\n\n\n\n\n\nIn segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). Figure 15 shows an example of object segmentation (instance segmentation) from the paper by He et al. (2018), where individual objects are detected and precisely localized (segmented) at the pixel level.\n\n\n\n\n\n\nFigure 15: Object Segmentation example (from He et al. (2018)).\n\n\n\nThe following video shows an example of semantic segmentation:\n\n\n\n\n\nIn keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. Figure 16 shows an example of keypoint detection from the paper by He et al. (2018), where\nindividual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).\n\n\n\n\n\n\nFigure 16: Keypoint Detection example (from He et al. (2018)).\n\n\n\n\n\n\nThere are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below are some examples.\n\n\nFigure 17 shows an example of image generation from the paper by Isola et al. (2018), where images are generated conditioned on image inputs (translated).\n\n\n\n\n\n\nFigure 17: Image Generation example (from Isola et al. (2018)).\n\n\n\n\n\n\nIn the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in Figure 18. This allows for higher frame rates.\n\n\n\n\n\n\nFigure 18: Nvidia DLSS: Link\n\n\n\n\n\n\nImage colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. Figure 19 shows an example.\n\n\n\n\n\n\nFigure 19: Norwegian Bride (est late 1890s) from DeOldify: Link\n\n\n\n\n\n\nIn view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. Figure 20 shows the data on which such a model is trained and what can be generated with it.\n\n\n\n\n\n\nFigure 20: Neural Radiance Fields - example (from Mildenhall et al. (2020)).\n\n\n\n\n\n\nIn unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model’s output. However, you can often make changes to generated images or interpolate between data points. Figure 21 shows generated images from a model trained on portrait images of people.\n\n\n\n\n\n\nFigure 21: StyleGan3 (from Karras et al. (2021)).\n\n\n\nFigure 22 shows how generated images can be further adjusted with manually set reference points.\n\n\n\n\n\n\nFigure 22: DragGAN (from Pan et al. (2023)).\n\n\n\n\n\n\nFigure 23 shows an example of image generation from the paper by Rombach et al. (2022), where images are generated conditioned on text inputs.\n\n\n\n\n\n\nFigure 23: Image Generation example (from Rombach et al. (2022).)\n\n\n\nOn civitai, there are numerous examples and models to admire or download.\n\n\n\n\nVarious models can also be combined into pipelines. One example is Grounded-Segment-Anything, shown in Figure 24. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.\n\n\n\n\n\n\nFigure 24: Example from Grounded-Segment-Anything Link\n\n\n\n\n\n\nThere are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. Figure 25 shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.\n\n\n\n\n\n\nFigure 25: Example from Facebook’s Detectron Library Link\n\n\n\nHugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification.\n\n\n\n\nWe follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:\n\nCollecting a dataset of images and their labels.\nUsing a machine learning algorithm to train a model that learns to associate images with labels.\nEvaluating/applying the model on new data.\n\n\ndef train(images, labels):\n \"\"\" Train a Model \"\"\"\n # Fit Model here\n return model\n\ndef predict(test_images, model):\n \"\"\" Predict \"\"\"\n predictions = model(test_images)\n return predictions\n\n\n\n\n\n\n\nQuestion\nHow would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.\n\n\n\n\n\nWhen modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. Figure 26 illustrates this process graphically.\n\n\n\n\n\n\nFigure 26: Machine Learning Pipeline (Source: Raschka and Mirjalili (2020))\n\n\n\nAt the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.\n\n\n\nA model is typically described as a function of a data point, generating an output \\(\\hat{y}\\):\n\\[\\begin{align*}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{align*}\\]\nMost models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by \\(\\theta\\).\n\\[\\begin{align*}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{align*}\\]\nFor simplicity, we often omit \\(\\theta\\): \\(f(\\mathbf{x}^{(i)})\\)\n\n\n\nThe coefficients are fitted to a training dataset through an optimization procedure.\nThe optimization procedure can often be influenced by additional factors, called hyperparameters (\\(\\alpha, \\lambda, \\dots\\)). These cannot be directly optimized.\nThe function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use \\(J(\\cdot)\\) to denote the cost function. Often, the cost function is also referred to as the loss function \\(L(\\cdot)\\). We use \\(l(\\cdot)\\) for the per-sample loss, i.e., the computation of the cost function on a single sample.\nOur goal is to find a model (and its parameters) that minimizes the cost function:\n\\[\\begin{equation*}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation*}\\]\nUsually, preprocessing of variables precedes the learning of\nthe coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.\n\n\n\nModel selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.\nIf we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see Figure 27) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.\n\n\n\n\n\n\nFigure 27: Train-Test Split to select and evaluate models.\n\n\n\n\n\n\nImages typically have very high dimensionality. For example, an RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\). Classical machine learning algorithms often struggle with such high dimensionalities:\n\nThey are very slow or require a lot of memory.\nThey cannot exploit the 2-D structure of images.\nThey are very sensitive to slight changes in images (e.g., rotations).\nThey can easily overfit, as the number of features is close to the number of observations (training set).\n\nWhen modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. Figure 28, Figure 29, and Figure 30 show various feature extraction methods.\nFigure 28 shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.\n\n\n\n\n\n\nFigure 28: Color Histograms as Features (Source: Johnson (2022))\n\n\n\nFigure 29 shows that techniques like Histogram of Oriented Gradients (HOG) Dalal and Triggs (2005) can be used to extract structures from images. Such features were successfully used for pedestrian detection Dalal and Triggs (2005).\n\n\n\n\n\n\nFigure 29: HOG as Features (Source: Johnson (2022))\n\n\n\nFigure 30 shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.\n\n\n\n\n\n\nFigure 30: Bag of (visual) words Features (Source: Johnson (2022))\n\n\n\nFinally, all features can be combined, often more is better, as shown in Figure 31.\n\n\n\n\n\n\nFigure 31: Image Features (Source: Johnson (2022))\n\n\n\nDepending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from Figure 31 can still be reduced in dimensionality, e.g., with Principal Component Analysis.\nFigure 32 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.\n\n\n\n\n\n\nFigure 32: CIFAR10 Dataset Source\n\n\n\n\n\n\n\nTo accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced Everingham et al. (2007). These involved various tasks, such as detecting objects in photographs (Figure 33).\n\n\n\n\n\n\nFigure 33: Images/illustrations from Link and Johnson (2022). On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.\n\n\n\nThe easy availability of images on the internet has made it possible to collect increasingly larger datasets. ImageNet is such a very large, hierarchically annotated image dataset Deng et al. (2009) with over 1.4 million images, categorized into 1,000 object classes. Figure 34 illustrates the dataset.\n\n\n\n\n\n\nFigure 34: ImageNet, Image Source, details in Deng et al. (2009)\n\n\n\nSince 2010, challenges have been regularly conducted on the ImageNet dataset Russakovsky et al. (2015), such as image classification and object detection. Figure 35 shows the development of the error rate over time.\n\n\n\n\n\n\nFigure 35: Source: Johnson (2022)\n\n\n\nIn 2011, a team won Perronnin et al. (2010) by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.\nIn 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. Krizhevsky, Sutskever, and Hinton (2012) impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in Figure 36.\n\n\n\n\n\n\nFigure 36: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nWhile classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms Figure 37, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning Figure 38.\n\n\n\n\n\n\nFigure 37: Illustration from Johnson (2022)\n\n\n\n\n\n\n\n\n\nFigure 38: Illustration from Johnson (2022)\n\n\n\nDeep learning-based approaches have several advantages over classical machine learning methods:\n\nAutomatic feature extraction: no manual feature extraction procedures are needed.\nHierarchical features: these are particularly valuable for processing and understanding visual data.\nGeneralization: with more training data, deep learning methods generalize better.\nEnd-to-end learning: this approach allows many problems to be modeled similarly.\nRobustness to variability: certain models are naturally invariant to\n\ntransformations like translations, scalings, etc. - Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.\n\n\nWe will now explore the most important milestones in deep learning for image analysis.\n\n\n\nHubel and Wiesel (1959) showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.\n\n\n\n\n\n\nFigure 39: Illustration Source\n\n\n\n\n\n\nFukushima (1980) defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 40: The Neocognitron Fukushima (1980).\n\n\n\n\n\n\nRumelhart, Hinton, and Williams (1986) introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.\n\n\n\n\n\n\nFigure 41: Backpropagation in neural networks Rumelhart, Hinton, and Williams (1986).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good video on backpropagation: 3Blue1Brown Backpropagation Calculus\n\n\n\n\n\nLecun et al. (1998) implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in Figure 42.\n\n\n\n\n\n\nFigure 42: Modern CNN Lecun et al. (1998).\n\n\n\nCNNs became extremely popular after winning the ImageNet competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in Figure 43.\n\n\n\n\n\n\nFigure 43: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the COCO Challenges, with various tasks.\n\n\n\nCNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models Dosovitskiy et al. (2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures Liu et al. (2021). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures Woo et al. (2023). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily Smith et al. (2023)."
  },
  {
    "objectID": "pages/intro.html#challenges",
    "href": "pages/intro.html#challenges",
    "title": "1 - Introduction",
    "section": "",
    "text": "We will now explore some challenges that must be overcome when analyzing images with machine learning models.\n\n\nThe semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.\n\n\n\n\n\n\nFigure 5: Illustration of the semantic gap.\n\n\n\n\n\n\nThe meaning of the image does not change with the viewpoint, but the pixels do.\n\n\n\n\n\n\nFigure 6: Source\n\n\n\n\n\n\nObjects are often flexible and appear in different shapes and poses.\n\n\n\n\n\n\nFigure 7: Source\n\n\n\n\n\n\nChanges in illumination affect pixel values and the visibility of objects.\n\n\n\n\n\n\nFigure 8: Source\n\n\n\n\n\n\nBackground pixels can resemble objects and make their exact delineation or visibility more difficult.\n\n\n\n\n\n\nFigure 9: Source\n\n\n\n\n\n\nObjects are not always fully visible, which can make their detection more difficult.\n\n\n\n\n\n\nFigure 10: Source\n\n\n\n\n\n\nObjects of the same class can exhibit large intra-class variability.\n\n\n\n\n\n\nFigure 11: Source\n\n\n\n\n\n\nFigure 12 shows that context information can be important to correctly classify an object.\n\n\n\n\n\n\nFigure 12: Context Source"
  },
  {
    "objectID": "pages/intro.html#problem-statements",
    "href": "pages/intro.html#problem-statements",
    "title": "1 - Introduction",
    "section": "",
    "text": "There are various problem statements in image processing that can be modeled with Deep Learning. Below, we will see some of them.\n\n\nIn image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. Figure 13 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012) (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.\n\n\n\n\n\n\nFigure 13: Multi-Class Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\n\n\n\nObject detection involves locating and recognizing (multiple) objects in an image. Figure 14 shows an example from the paper by Redmon et al. (2016). Each object is localized with a bounding box and assigned an object class.\n\n\n\n\n\n\nFigure 14: Object Detection example (from Redmon et al. (2016)). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.\n\n\n\n\n\n\nIn segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). Figure 15 shows an example of object segmentation (instance segmentation) from the paper by He et al. (2018), where individual objects are detected and precisely localized (segmented) at the pixel level.\n\n\n\n\n\n\nFigure 15: Object Segmentation example (from He et al. (2018)).\n\n\n\nThe following video shows an example of semantic segmentation:\n\n\n\n\n\nIn keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. Figure 16 shows an example of keypoint detection from the paper by He et al. (2018), where\nindividual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).\n\n\n\n\n\n\nFigure 16: Keypoint Detection example (from He et al. (2018)).\n\n\n\n\n\n\nThere are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below are some examples.\n\n\nFigure 17 shows an example of image generation from the paper by Isola et al. (2018), where images are generated conditioned on image inputs (translated).\n\n\n\n\n\n\nFigure 17: Image Generation example (from Isola et al. (2018)).\n\n\n\n\n\n\nIn the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in Figure 18. This allows for higher frame rates.\n\n\n\n\n\n\nFigure 18: Nvidia DLSS: Link\n\n\n\n\n\n\nImage colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. Figure 19 shows an example.\n\n\n\n\n\n\nFigure 19: Norwegian Bride (est late 1890s) from DeOldify: Link\n\n\n\n\n\n\nIn view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. Figure 20 shows the data on which such a model is trained and what can be generated with it.\n\n\n\n\n\n\nFigure 20: Neural Radiance Fields - example (from Mildenhall et al. (2020)).\n\n\n\n\n\n\nIn unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model’s output. However, you can often make changes to generated images or interpolate between data points. Figure 21 shows generated images from a model trained on portrait images of people.\n\n\n\n\n\n\nFigure 21: StyleGan3 (from Karras et al. (2021)).\n\n\n\nFigure 22 shows how generated images can be further adjusted with manually set reference points.\n\n\n\n\n\n\nFigure 22: DragGAN (from Pan et al. (2023)).\n\n\n\n\n\n\nFigure 23 shows an example of image generation from the paper by Rombach et al. (2022), where images are generated conditioned on text inputs.\n\n\n\n\n\n\nFigure 23: Image Generation example (from Rombach et al. (2022).)\n\n\n\nOn civitai, there are numerous examples and models to admire or download.\n\n\n\n\nVarious models can also be combined into pipelines. One example is Grounded-Segment-Anything, shown in Figure 24. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.\n\n\n\n\n\n\nFigure 24: Example from Grounded-Segment-Anything Link\n\n\n\n\n\n\nThere are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. Figure 25 shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.\n\n\n\n\n\n\nFigure 25: Example from Facebook’s Detectron Library Link\n\n\n\nHugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification."
  },
  {
    "objectID": "pages/intro.html#machine-learning",
    "href": "pages/intro.html#machine-learning",
    "title": "1 - Introduction",
    "section": "",
    "text": "We follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:\n\nCollecting a dataset of images and their labels.\nUsing a machine learning algorithm to train a model that learns to associate images with labels.\nEvaluating/applying the model on new data.\n\n\ndef train(images, labels):\n \"\"\" Train a Model \"\"\"\n # Fit Model here\n return model\n\ndef predict(test_images, model):\n \"\"\" Predict \"\"\"\n predictions = model(test_images)\n return predictions\n\n\n\n\n\n\n\nQuestion\nHow would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.\n\n\n\n\n\nWhen modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. Figure 26 illustrates this process graphically.\n\n\n\n\n\n\nFigure 26: Machine Learning Pipeline (Source: Raschka and Mirjalili (2020))\n\n\n\nAt the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.\n\n\n\nA model is typically described as a function of a data point, generating an output \\(\\hat{y}\\):\n\\[\\begin{align*}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{align*}\\]\nMost models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by \\(\\theta\\).\n\\[\\begin{align*}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{align*}\\]\nFor simplicity, we often omit \\(\\theta\\): \\(f(\\mathbf{x}^{(i)})\\)\n\n\n\nThe coefficients are fitted to a training dataset through an optimization procedure.\nThe optimization procedure can often be influenced by additional factors, called hyperparameters (\\(\\alpha, \\lambda, \\dots\\)). These cannot be directly optimized.\nThe function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use \\(J(\\cdot)\\) to denote the cost function. Often, the cost function is also referred to as the loss function \\(L(\\cdot)\\). We use \\(l(\\cdot)\\) for the per-sample loss, i.e., the computation of the cost function on a single sample.\nOur goal is to find a model (and its parameters) that minimizes the cost function:\n\\[\\begin{equation*}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation*}\\]\nUsually, preprocessing of variables precedes the learning of\nthe coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.\n\n\n\nModel selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.\nIf we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see Figure 27) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.\n\n\n\n\n\n\nFigure 27: Train-Test Split to select and evaluate models.\n\n\n\n\n\n\nImages typically have very high dimensionality. For example, an RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\). Classical machine learning algorithms often struggle with such high dimensionalities:\n\nThey are very slow or require a lot of memory.\nThey cannot exploit the 2-D structure of images.\nThey are very sensitive to slight changes in images (e.g., rotations).\nThey can easily overfit, as the number of features is close to the number of observations (training set).\n\nWhen modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. Figure 28, Figure 29, and Figure 30 show various feature extraction methods.\nFigure 28 shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.\n\n\n\n\n\n\nFigure 28: Color Histograms as Features (Source: Johnson (2022))\n\n\n\nFigure 29 shows that techniques like Histogram of Oriented Gradients (HOG) Dalal and Triggs (2005) can be used to extract structures from images. Such features were successfully used for pedestrian detection Dalal and Triggs (2005).\n\n\n\n\n\n\nFigure 29: HOG as Features (Source: Johnson (2022))\n\n\n\nFigure 30 shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.\n\n\n\n\n\n\nFigure 30: Bag of (visual) words Features (Source: Johnson (2022))\n\n\n\nFinally, all features can be combined, often more is better, as shown in Figure 31.\n\n\n\n\n\n\nFigure 31: Image Features (Source: Johnson (2022))\n\n\n\nDepending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from Figure 31 can still be reduced in dimensionality, e.g., with Principal Component Analysis.\nFigure 32 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.\n\n\n\n\n\n\nFigure 32: CIFAR10 Dataset Source"
  },
  {
    "objectID": "pages/intro.html#deep-learning",
    "href": "pages/intro.html#deep-learning",
    "title": "1 - Introduction",
    "section": "",
    "text": "To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced Everingham et al. (2007). These involved various tasks, such as detecting objects in photographs (Figure 33).\n\n\n\n\n\n\nFigure 33: Images/illustrations from Link and Johnson (2022). On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.\n\n\n\nThe easy availability of images on the internet has made it possible to collect increasingly larger datasets. ImageNet is such a very large, hierarchically annotated image dataset Deng et al. (2009) with over 1.4 million images, categorized into 1,000 object classes. Figure 34 illustrates the dataset.\n\n\n\n\n\n\nFigure 34: ImageNet, Image Source, details in Deng et al. (2009)\n\n\n\nSince 2010, challenges have been regularly conducted on the ImageNet dataset Russakovsky et al. (2015), such as image classification and object detection. Figure 35 shows the development of the error rate over time.\n\n\n\n\n\n\nFigure 35: Source: Johnson (2022)\n\n\n\nIn 2011, a team won Perronnin et al. (2010) by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.\nIn 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. Krizhevsky, Sutskever, and Hinton (2012) impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in Figure 36.\n\n\n\n\n\n\nFigure 36: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nWhile classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms Figure 37, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning Figure 38.\n\n\n\n\n\n\nFigure 37: Illustration from Johnson (2022)\n\n\n\n\n\n\n\n\n\nFigure 38: Illustration from Johnson (2022)\n\n\n\nDeep learning-based approaches have several advantages over classical machine learning methods:\n\nAutomatic feature extraction: no manual feature extraction procedures are needed.\nHierarchical features: these are particularly valuable for processing and understanding visual data.\nGeneralization: with more training data, deep learning methods generalize better.\nEnd-to-end learning: this approach allows many problems to be modeled similarly.\nRobustness to variability: certain models are naturally invariant to\n\ntransformations like translations, scalings, etc. - Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.\n\n\nWe will now explore the most important milestones in deep learning for image analysis.\n\n\n\nHubel and Wiesel (1959) showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.\n\n\n\n\n\n\nFigure 39: Illustration Source\n\n\n\n\n\n\nFukushima (1980) defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 40: The Neocognitron Fukushima (1980).\n\n\n\n\n\n\nRumelhart, Hinton, and Williams (1986) introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.\n\n\n\n\n\n\nFigure 41: Backpropagation in neural networks Rumelhart, Hinton, and Williams (1986).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good video on backpropagation: 3Blue1Brown Backpropagation Calculus\n\n\n\n\n\nLecun et al. (1998) implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in Figure 42.\n\n\n\n\n\n\nFigure 42: Modern CNN Lecun et al. (1998).\n\n\n\nCNNs became extremely popular after winning the ImageNet competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in Figure 43.\n\n\n\n\n\n\nFigure 43: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the COCO Challenges, with various tasks.\n\n\n\nCNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models Dosovitskiy et al. (2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures Liu et al. (2021). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures Woo et al. (2023). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily Smith et al. (2023)."
  },
  {
    "objectID": "pages/quiz.html",
    "href": "pages/quiz.html",
    "title": "Quiz",
    "section": "",
    "text": "The following questions could be exam questions.",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#convolutions",
    "href": "pages/quiz.html#convolutions",
    "title": "Quiz",
    "section": "Convolutions",
    "text": "Convolutions\n\nA convolutional layer has 64 activations (\\(C_{in} = 64\\) and \\(H=16\\), \\(W=16\\)). You want to reduce its spatial dimensionality by half, while increasing the number of channels. How do you parameterize your next convolutional layer?\nIn th example above: How many weights do you need to learn?\nYou have very large images (\\(8000 \\times 8000\\) pixels). Your model always crashes with out-of-memory-errors. What options do you have when parameterizing your convolutions?",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#cnns",
    "href": "pages/quiz.html#cnns",
    "title": "Quiz",
    "section": "CNNs",
    "text": "CNNs\n\nCan CNNs be used to count objects? Justify your answer.\n\n\n\n\nCan CNNs count objects\n\n\n\nCan CNNs be used to model inputs, e.g. satellite data that are not RGB images, i.e. have more than 3 input channels? Justify your answer.\nYou want to model images which are not square. They have a spatial resolution of 800x400. What are the consequences?",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#image-classification",
    "href": "pages/quiz.html#image-classification",
    "title": "Quiz",
    "section": "Image Classification",
    "text": "Image Classification\n\nYou trained a model to classify images into synthetic and real. The model is quite good but not perfect. You have the option to verify some images manually. Which do you choose? Justify your answer.",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#foundation-models",
    "href": "pages/quiz.html#foundation-models",
    "title": "Quiz",
    "section": "Foundation Models",
    "text": "Foundation Models\n\nYou applied CLIP on a ataset to identify synthetic / fake images and used the following prompts:\n\n\n“A synthetic image”.\n“A real image”.\n\nHowever it does not perform well. What can you do?",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/slides_cas.html",
    "href": "pages/slides_cas.html",
    "title": "Slides",
    "section": "",
    "text": "Recap Quiz slides.\nIntro slides.\nFrameworks slides.\nNeural Networks slides.\nCNNs slides.\nImage Classification slides.\nPractical slides."
  },
  {
    "objectID": "pages/literature.html",
    "href": "pages/literature.html",
    "title": "Books",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#pytorch",
    "href": "pages/literature.html#pytorch",
    "title": "Books",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#deep-learning",
    "href": "pages/literature.html#deep-learning",
    "title": "Books",
    "section": "Deep Learning",
    "text": "Deep Learning\nSimon J.D. Prince, Understanding Deep Learning, MIT Press, Prince (2023)\n\nBrandaktuelles Buch über Deep Learning\nUmfassende Einführung ins Thema mit sehr guten Illustrationen\nOnline verfügbar: Link\n\nGoodfellow, Ian and Bengio, Yoshua and Courville, Aaron, Deep Learning, MIT Press, Goodfellow, Bengio, and Courville (2016)\n\nSehr gute und umfassende Einführung in Deep Learning\nEtwas älter aber immer noch in weiten Teilen aktuell\nOnline verfügbar: Link\n\nChollet, François, Deep Learning with Python, Second Edition, Manning Publications, Chollet (2021)\n\nchapters 8-9 are about computer vision\nfree access with FHNW-Emailadresse in O’Reilly online Mediathek\n\nStevens et al, Deep Learning with PyTorch, Manning Publications, Stevens, Antiga, and Viehmann (2020)",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#machine-learning",
    "href": "pages/literature.html#machine-learning",
    "title": "Books",
    "section": "Machine Learning",
    "text": "Machine Learning\nGéron A, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, O’Reilly 2019\n\nJupyter Notebooks sind öffentlich verfügbar: Link\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nRaschka S, Python Machine Learning, 3rd Edition, PACKT 2019\n\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nKevin P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press 2022\n\nVorabversion gratis verfügbar: Link\nUmfassende Einführung in Machine Learning mit ausführlichen theoretischen Hintergründen\n\nChollet F, Deep Learning with Python, 2nd Edition, MEAP 2020\n\nEin Klassiker für eine Einführung in Deep Learning (und Keras)\n\nHastie T et al., Elements of Statistical Learning, Springer 2009.\n\nKann als pdf gratis runtergeladen werden: Link\nEnthält Machine Learning Grundlagen und viele Methoden (wenig über Neuronale Netzwerke)\n\nVanderPlas J, Python Data Science Handbook, O’Reilly 2017.\n\nWurde mit Jupyter Notebooks geschrieben.\nDer gesamte Inhalt finden sie auf einer website: Link\nDas Repository kann von github runtergelanden werden: Link",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/exercises.html",
    "href": "pages/exercises.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nExercises\nExercises can be found here: Link",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Herzlich willkommen zum Modul Computer Vision mit Deep Learning (1. Teil)!\nHier finden Sie Unterlagen und aktuelle Informationen zum Modul.\nModul Page\nCAS Page",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lernziele",
    "href": "index.html#lernziele",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "Lernziele",
    "text": "Lernziele\n\nConvolutional Neural Networks verstehen, implementieren und trainieren können\nBildklassifikation vertieft verstehen und systematisch umsetzen können\nMit Deep Learning Frameworks umgehen und Libraries verwenden können\nPraktische Anwendung vorstellen und erklären können",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#inhalte",
    "href": "index.html#inhalte",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "Inhalte",
    "text": "Inhalte\n\nTag 1 - Grundlagen Convolutional Neural Networks\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nEinführung Computer Vision mit Deep Learning\n\n\n9:30 - 10:30\nÜbung: Deep Learning mit PyTorch und Bilder\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nConvolutional Neural Networks\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 16:30\nÜbung: CNNs für Bildklassifikation\n\n\n\n\n\nTag 2 - Bildklassifikation und Anwendungen\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nTheorie: Bildklassifikation & Recap\n\n\n9:30 - 10:30\nTheorie: Praktische Überlegungen\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nÜbung: Bildklassifikation\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 14:30\nÜbung: Bildklassifikation\n\n\n14:30 - 14:45\nPause\n\n\n14:45 - 15:15\nTheorie: Foundation Models\n\n\n15:15 - 16:30\nÜbung: Foundation Models",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/links.html",
    "href": "pages/links.html",
    "title": "Helpful Links & Resources",
    "section": "",
    "text": "Links and ressources to different topics related to Machine Learning, Deep Learning, and Images.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#theory",
    "href": "pages/links.html#theory",
    "title": "Helpful Links & Resources",
    "section": "Theory",
    "text": "Theory\n\nPyTorch\nPyTorch internals - Blog Post\n\n\nDeep Learning and Computer Vision\nUniversity of Michigan - Deep Learning for Computer Vision\n\nSehr gute Vorlesung zum Thema\n\nUniversity of California, Berkeley - Modern Computer Vision and Deep Learning\n\nSehr gute Vorlesung zum Thema\n\n\n\nNeuronale Netzwerke - Basics\nPerceptron Learning Rule S. Raschka\nCS229 Stanford MLP Backpropagation\nNotes on Backpropagation\n3Blue1Brown Gradient Descent\n3Blue1Brown Backpropagation Calculus\nAndrew Ng Backprop\nAndrej Karpathy - Backpropagation from the ground up\n\n\nModel Selection\nPaper von S.Raschka: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#practical",
    "href": "pages/links.html#practical",
    "title": "Helpful Links & Resources",
    "section": "Practical",
    "text": "Practical\nAndrej Karpathy - A Recipe for Training Neural Networks\n\nML Best Practices Videos\nMartin Zinkevich - Best Practices for ML Engineering\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Try Next\nAndrew Ng - Advice For Applying Machine Learning | Learning Curves\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)\nAndrew Ng - Machine Learning System Design | Prioritizing What To Work On\nAndrew Ng - Machine Learning System Design | Error Analysis\nAndrew Ng - Machine Learning System Design | Data For Machine Learning",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#tools",
    "href": "pages/links.html#tools",
    "title": "Helpful Links & Resources",
    "section": "Tools",
    "text": "Tools\n\nData Science Repository\nBuild a Reproducible and Maintainable Data Science Project\n\ngreat jupyter book to learen about how to structure a repository and more\n\nLightning-Hydra-Template\n\ntemplate to strcuture a repository based on experiment configuration with Hydra and Pytorch-Lightning\n\n\n\nData Handling\ndatasets\n\nGreat package to create and manage (large) image datasets\n\nimg2dataset\n\nPackage to download large image datasets from urls\n\nDVC\n\nPackage for data version control\n\n\n\nPyTorch\nLightning\n\nboilerplate code to easily train models and use gpu, etc.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/slides.html",
    "href": "pages/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Recap Quiz slides.\nIntro slides.\nFrameworks slides.\nNeural Networks slides.\nCNNs slides.\nImage Classification slides.\nPractical slides."
  },
  {
    "objectID": "pages/notation.html",
    "href": "pages/notation.html",
    "title": "Mathematical Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#numbers-and-arrays",
    "href": "pages/notation.html#numbers-and-arrays",
    "title": "Mathematical Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#indexing",
    "href": "pages/notation.html#indexing",
    "title": "Mathematical Notation",
    "section": "Indexing",
    "text": "Indexing\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(a_i\\)\nElement \\(i\\) of vector \\(\\mathbf{a}\\), with indexing starting at 1\n\n\n\\(A_{i,j}\\)\nElement \\(i, j\\) of matrix \\(\\mathbf{A}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#datasets-and-distributions",
    "href": "pages/notation.html#datasets-and-distributions",
    "title": "Mathematical Notation",
    "section": "Datasets and Distributions",
    "text": "Datasets and Distributions\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\mathbf{X}\\)\nThe design matrix with dimensionality \\(nxp\\) with \\(n\\) samples with \\(p\\) features.\n\n\n\\(\\mathbf{x}^{(i)}\\)\nThe i-th training example.\n\n\n\\(\\mathbf{y}^{(i)}\\)\nThe label-vector for the i-th training example.\n\n\n\\(y^{(i)}\\)\nThe label for the i-th training example.",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#probability-theory",
    "href": "pages/notation.html#probability-theory",
    "title": "Mathematical Notation",
    "section": "Probability Theory",
    "text": "Probability Theory\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(P(x)\\)\nA probability distribution over a discrete variable.\n\n\n\\(p(x)\\)\nA probability distribution over a contiuous variable or over a variable whose type has not been specified.\n\n\n\\(\\mathbb{E}_{x \\sim P} [ f(x) ]\\text{ or } \\mathbb{E} f(x)\\)\nExpectation of \\(f(x)\\) with respect to \\(P(x)\\)\n\n\n\\(\\mathcal{N} ( \\mathbf{x} ; \\mu , \\Sigma)\\)\nGaussian distribution over \\(\\mathbf{x}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\)\n\n\n\\(x \\sim \\mathcal{N} (\\mu , \\sigma)\\)\nGaussian distribution over \\(x\\) with mean \\(\\mu\\) and variance \\(\\sigma\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#calculus",
    "href": "pages/notation.html#calculus",
    "title": "Mathematical Notation",
    "section": "Calculus",
    "text": "Calculus\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\nabla_{\\mathbf{w}} J\\)\nGradient of \\(J\\) with respect to \\(\\mathbf{w}\\)\n\n\n\\(\\frac{\\partial J}{\\partial w}\\)\nPartial derivative of \\(J\\) with respect to \\(w\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#functions",
    "href": "pages/notation.html#functions",
    "title": "Mathematical Notation",
    "section": "Functions",
    "text": "Functions\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\log x\\)\nThe natural logarithm of \\(x\\).\n\n\n\\(\\lVert \\mathbf{x} \\rVert_p\\)\n\\(L^p\\) norm of \\(\\mathbf{x}\\)\n\n\n\\(\\lVert \\mathbf{x} \\rVert\\)\n\\(L^2\\) norm of \\(\\mathbf{x}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#deep-learning",
    "href": "pages/notation.html#deep-learning",
    "title": "Mathematical Notation",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\nNCHW\nThe input format of images and activations in PyTorch. N: number of images (batch size), C: number of channels, H: height, W: width",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#overview",
    "href": "slides_cas/image_classification.html#overview",
    "title": "Image Classification",
    "section": "Overview",
    "text": "Overview\n\nIntroduction\nModeling\nLoss Function\nArchitectures",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#adversarial-panda",
    "href": "slides_cas/image_classification.html#adversarial-panda",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#adversarial-panda-1",
    "href": "slides_cas/image_classification.html#adversarial-panda-1",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#image-classification",
    "href": "slides_cas/image_classification.html#image-classification",
    "title": "Image Classification",
    "section": "Image Classification",
    "text": "Image Classification\n\n\n\nExample of Image Classification.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#image-classification-example",
    "href": "slides_cas/image_classification.html#image-classification-example",
    "title": "Image Classification",
    "section": "Image Classification: Example",
    "text": "Image Classification: Example\n\n\n\nExample of Image Classification (from Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#image-classification-camera-traps",
    "href": "slides_cas/image_classification.html#image-classification-camera-traps",
    "title": "Image Classification",
    "section": "Image Classification: Camera Traps",
    "text": "Image Classification: Camera Traps\n\n\n\nExample images from camera traps.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#parametric-approach",
    "href": "slides_cas/image_classification.html#parametric-approach",
    "title": "Image Classification",
    "section": "Parametric Approach",
    "text": "Parametric Approach\nWith a parametric approach, we seek a model of the following form:\n\\[\\begin{equation}\n    \\hat{y}^{(i)} = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThe model parameters \\(\\theta\\) define our model and must be learned with an algorithm.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#softmax-classifier",
    "href": "slides_cas/image_classification.html#softmax-classifier",
    "title": "Image Classification",
    "section": "Softmax Classifier",
    "text": "Softmax Classifier\nWe want to model the following probability:\n\\[\\begin{equation}\n    P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the Softmax function \\(\\sigma(\\mathbf{z})\\).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#softmax-transformation",
    "href": "slides_cas/image_classification.html#softmax-transformation",
    "title": "Image Classification",
    "section": "Softmax Transformation",
    "text": "Softmax Transformation\n\\[\\begin{equation}\n    P(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#logits-to-probabilities",
    "href": "slides_cas/image_classification.html#logits-to-probabilities",
    "title": "Image Classification",
    "section": "Logits to Probabilities",
    "text": "Logits to Probabilities\n\n\n\nLogits (left) to probabilities with the Softmax function.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#probabilities",
    "href": "slides_cas/image_classification.html#probabilities",
    "title": "Image Classification",
    "section": "Probabilities",
    "text": "Probabilities\n\nImage classifier with confidences.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#likelihood",
    "href": "slides_cas/image_classification.html#likelihood",
    "title": "Image Classification",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood of a data point:\n\\[\\begin{equation}\n    P(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThis is the modeled probability for the actually observed class \\(y^{(i)}\\).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#likelihood-for-multi-class-classification",
    "href": "slides_cas/image_classification.html#likelihood-for-multi-class-classification",
    "title": "Image Classification",
    "section": "Likelihood for Multi-Class Classification",
    "text": "Likelihood for Multi-Class Classification\nThe likelihood of a data point for multi-class classification:\n\\[\\begin{equation}\n    \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nWhere \\(y^{(i)} \\in \\mathbb{R}^{K}\\) is a one-hot encoded vector, with the \\(1\\) at the true class.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#maximum-likelihood",
    "href": "slides_cas/image_classification.html#maximum-likelihood",
    "title": "Image Classification",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe likelihood of an entire dataset:\n\\[\\begin{equation}\n    \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the maximum likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#negative-log-likelihood",
    "href": "slides_cas/image_classification.html#negative-log-likelihood",
    "title": "Image Classification",
    "section": "Negative Log-Likelihood",
    "text": "Negative Log-Likelihood\nEquivalently, we can minimize the negative log likelihood:\n\\[\\begin{align}\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& - \\log \\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#cross-entropy",
    "href": "slides_cas/image_classification.html#cross-entropy",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nThe loss function derived with maximum likelihood can also be viewed from the perspective of cross-entropy between two discrete probability distributions.\n\\[\\begin{align}\n    CE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\n    CE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#cross-entropy-1",
    "href": "slides_cas/image_classification.html#cross-entropy-1",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\n\nTrue distribution (left) and predicted distribution (right).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#alexnet",
    "href": "slides_cas/image_classification.html#alexnet",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#alexnet-1",
    "href": "slides_cas/image_classification.html#alexnet-1",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#alexnet-table",
    "href": "slides_cas/image_classification.html#alexnet-table",
    "title": "Image Classification",
    "section": "AlexNet: Table",
    "text": "AlexNet: Table\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#vgg",
    "href": "slides_cas/image_classification.html#vgg",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\nVGG Simonyan and Zisserman (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#vgg-1",
    "href": "slides_cas/image_classification.html#vgg-1",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\nVGG Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#vgg-2",
    "href": "slides_cas/image_classification.html#vgg-2",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#resnet",
    "href": "slides_cas/image_classification.html#resnet",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\nSource: He et al. (2016)\nTest error for deeper model is larger. Overfitting?",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#resnet-1",
    "href": "slides_cas/image_classification.html#resnet-1",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\nSource: He et al. (2016)\nTraining error for deeper model is also larger? What is going on?!",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#resnet-2",
    "href": "slides_cas/image_classification.html#resnet-2",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nResNet He et al. (2016) (Image from Johnson (2019))",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#resnet-3",
    "href": "slides_cas/image_classification.html#resnet-3",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom He et al. (2016)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#resnet-4",
    "href": "slides_cas/image_classification.html#resnet-4",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom Li et al. (2018)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#convnext",
    "href": "slides_cas/image_classification.html#convnext",
    "title": "Image Classification",
    "section": "ConvNext",
    "text": "ConvNext\n\n\n\n\n\n\n\n\n\n\nFigures from Liu et al. (2022).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#imagenet-performance",
    "href": "slides_cas/image_classification.html#imagenet-performance",
    "title": "Image Classification",
    "section": "ImageNet Performance",
    "text": "ImageNet Performance\n\nImage from Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#choosing-the-architecture",
    "href": "slides_cas/image_classification.html#choosing-the-architecture",
    "title": "Image Classification",
    "section": "Choosing the Architecture",
    "text": "Choosing the Architecture\n\nDon’t be a hero!\n\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#pre-processing",
    "href": "slides_cas/image_classification.html#pre-processing",
    "title": "Image Classification",
    "section": "Pre-Processing",
    "text": "Pre-Processing\n\nResizing / Cropping to a fixed size\nScaling: from the range [0, 255] to the range [0, 1]\nNormalization: Often normalized along the color channels\n\nPyTorch Examples",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/image_classification.html#transfer-learning",
    "href": "slides_cas/image_classification.html#transfer-learning",
    "title": "Image Classification",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer learning refers to the process of adapting a trained model that models Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#species-identification",
    "href": "slides_cas/intro.html#species-identification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Species Identification",
    "text": "Species Identification\n\nSource: Breitenmoser-Würsten et al. (2024)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#synthetic-image-detection",
    "href": "slides_cas/intro.html#synthetic-image-detection",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Synthetic Image Detection",
    "text": "Synthetic Image Detection\n\n\n\n\n\n\n\n\nSource\n\n\n\n\n\n\n\nSource\n\n\n\n\n\n\nFigure 2: Examples of synthetic images.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#object-identification-and-translation",
    "href": "slides_cas/intro.html#object-identification-and-translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Object Identification and Translation",
    "text": "Object Identification and Translation\n\n\n\n\n\n\n\n\n\n\n\n(a) Identification & Search\n\n\n\n\n\n\n\n\n\n\n\n(b) Translation\n\n\n\n\n\n\n\nFigure 3: Google Lens",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#self-driving",
    "href": "slides_cas/intro.html#self-driving",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Self-Driving",
    "text": "Self-Driving\n\n\n\n\n\n\nFigure 4: Example from Waymo.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#biometric-id",
    "href": "slides_cas/intro.html#biometric-id",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Biometric ID",
    "text": "Biometric ID\n\nExample from Apple Face ID",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#precision-agriculture",
    "href": "slides_cas/intro.html#precision-agriculture",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Precision Agriculture",
    "text": "Precision Agriculture\n\n\nExample from Häni, Roy, and Isler (2020)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#medical-segmentation",
    "href": "slides_cas/intro.html#medical-segmentation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Medical Segmentation",
    "text": "Medical Segmentation\n\nExample from Ma et al. (2024).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#photo-enhancement",
    "href": "slides_cas/intro.html#photo-enhancement",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Photo Enhancement",
    "text": "Photo Enhancement\n\nExample from Google Magic Editor",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#ai-chips",
    "href": "slides_cas/intro.html#ai-chips",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "AI Chips",
    "text": "AI Chips\n\nFrom Link.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-classification",
    "href": "slides_cas/intro.html#image-classification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#object-detection",
    "href": "slides_cas/intro.html#object-detection",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Object Detection",
    "text": "Object Detection\n\nObject Detection Beispiel (aus Redmon et al. (2016)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#segmentation",
    "href": "slides_cas/intro.html#segmentation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentation",
    "text": "Segmentation\n\n\n\n\n\nObject Segmentation Beispiel (aus He et al. (2018)).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---manipulation",
    "href": "slides_cas/intro.html#image-generation---manipulation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Manipulation",
    "text": "Image Generation - Manipulation\n\nSource: Link, DragGAN by Pan et al. (2023)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---translation",
    "href": "slides_cas/intro.html#image-generation---translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Translation",
    "text": "Image Generation - Translation\n\nImage Generation Beispiel (aus Isola et al. (2018)).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---super-resolution",
    "href": "slides_cas/intro.html#image-generation---super-resolution",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Super Resolution",
    "text": "Image Generation - Super Resolution\n\nNvidia dlss: Link",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---colorization",
    "href": "slides_cas/intro.html#image-generation---colorization",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Colorization",
    "text": "Image Generation - Colorization\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#many-tasks",
    "href": "slides_cas/intro.html#many-tasks",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Many tasks",
    "text": "Many tasks\n\n\n\nImage Classification\nObject Detection (and Tracking)\nImage Segmentation\n\nSemantic Segmentation\nInstance Segmentation\n\nOptical Character Recognition (OCR)\nPose Estimation\nFacial Recognition\nAction Recognition\n\n\n\nImage Generation\n\nStyle Transfer\nImage Inpainting\nSuper-Resolution\nText-to-Image (and more)\n\nImage Captioning\n3D Reconstruction\nImage Retrieval\n\n\nList is not exhaustive!",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#semantic-gap",
    "href": "slides_cas/intro.html#semantic-gap",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\nIllustration des semantic gap.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#point-of-view",
    "href": "slides_cas/intro.html#point-of-view",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Point of View",
    "text": "Point of View\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#deformation",
    "href": "slides_cas/intro.html#deformation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Deformation",
    "text": "Deformation\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#lighting",
    "href": "slides_cas/intro.html#lighting",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Lighting",
    "text": "Lighting\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#background",
    "href": "slides_cas/intro.html#background",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Background",
    "text": "Background\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#occlusion",
    "href": "slides_cas/intro.html#occlusion",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Occlusion",
    "text": "Occlusion\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#intraclass-variation",
    "href": "slides_cas/intro.html#intraclass-variation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Intraclass Variation",
    "text": "Intraclass Variation\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#context-relevance",
    "href": "slides_cas/intro.html#context-relevance",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Context Relevance",
    "text": "Context Relevance\n\n\n\n\n\n\n\n\n\n\nKontext Source",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#machine-learning-approach",
    "href": "slides_cas/intro.html#machine-learning-approach",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Approach",
    "text": "Machine Learning Approach\nWith Machine Learning, we follow a data-driven approach to solve various tasks:\n\nCollect a dataset of images and their labels.\nUse a machine learning algorithm to train a model (e.g., a classifier).\nEvaluate and apply the model to new data.\n\n\ndef train(images, labels):\n    \"\"\" Train a Model \"\"\"\n    # Fit Model here\n    return model\n\ndef predict(test_images, model):\n    \"\"\" Predict \"\"\"\n    predictions = model(test_images)\n    return predictions",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#question",
    "href": "slides_cas/intro.html#question",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Question",
    "text": "Question\nImage Super Resolution\nHow would you train a model for image super resolution?\nThe task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#machine-learning-pipeline",
    "href": "slides_cas/intro.html#machine-learning-pipeline",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Pipeline",
    "text": "Machine Learning Pipeline",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#pytorch",
    "href": "slides_cas/intro.html#pytorch",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "PyTorch",
    "text": "PyTorch\nIn this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources.\nFundamental Concepts:\n\nTensor: N-dimensional array, like numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks\n\nLet’s check it out! (on images)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#what-are-they",
    "href": "slides_cas/foundation_models.html#what-are-they",
    "title": "Foundation Models",
    "section": "What are they?",
    "text": "What are they?\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#characteristics-of-foundation-models",
    "href": "slides_cas/foundation_models.html#characteristics-of-foundation-models",
    "title": "Foundation Models",
    "section": "Characteristics of Foundation Models",
    "text": "Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#clip-a-foundation-model-example",
    "href": "slides_cas/foundation_models.html#clip-a-foundation-model-example",
    "title": "Foundation Models",
    "section": "CLIP: A Foundation Model Example",
    "text": "CLIP: A Foundation Model Example\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#how-clip-works",
    "href": "slides_cas/foundation_models.html#how-clip-works",
    "title": "Foundation Models",
    "section": "How CLIP Works",
    "text": "How CLIP Works\n\nSource: Radford et al. (2021)CLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#how-clip-can-be-applied",
    "href": "slides_cas/foundation_models.html#how-clip-can-be-applied",
    "title": "Foundation Models",
    "section": "How CLIP can be applied",
    "text": "How CLIP can be applied\n\nSource: Radford et al. (2021)",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#applications-of-clip",
    "href": "slides_cas/foundation_models.html#applications-of-clip",
    "title": "Foundation Models",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#example",
    "href": "slides_cas/foundation_models.html#example",
    "title": "Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#fine-tuning-clip",
    "href": "slides_cas/foundation_models.html#fine-tuning-clip",
    "title": "Foundation Models",
    "section": "Fine-Tuning CLIP",
    "text": "Fine-Tuning CLIP\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#visual-question-anwering",
    "href": "slides_cas/foundation_models.html#visual-question-anwering",
    "title": "Foundation Models",
    "section": "Visual Question Anwering",
    "text": "Visual Question Anwering\nMulti-modal models extend the capabilities of foundation models by integrating and processing multiple types of data simultaneously. One notable example of a multi-modal model is a Visual Question Answering (VQA) system, which can understand and answer questions about images. Figure 1 shows an example.\n\nSource: Li et al. (2023)",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#how-vqa-works",
    "href": "slides_cas/foundation_models.html#how-vqa-works",
    "title": "Foundation Models",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers. Figure 2 shows training data examples that were used in the Flamingo model Alayrac et al. (2022).\n\nSource: Alayrac et al. (2022)",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#applications-of-vqa",
    "href": "slides_cas/foundation_models.html#applications-of-vqa",
    "title": "Foundation Models",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#conclusion",
    "href": "slides_cas/foundation_models.html#conclusion",
    "title": "Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nFoundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape.",
    "crumbs": [
      "Slides",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#overview",
    "href": "slides_cas/cnns.html#overview",
    "title": "Convolutional Neural Networks",
    "section": "Overview",
    "text": "Overview\n\nIntroduction & Motivation\nConvolutional Layers\nProperties\nVariants and Layers\nVisualizations and Architectures",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#properties-of-image-data",
    "href": "slides_cas/cnns.html#properties-of-image-data",
    "title": "Convolutional Neural Networks",
    "section": "Properties of Image Data",
    "text": "Properties of Image Data\n\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) (height, width) has = \\(150'528\\) values.\nLocality: Nearby pixels are statistically related\nStability under transformations: Interpretation of an image does not change under many geomoetric transformations.\n\n\nImage Source",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptron-and-images",
    "href": "slides_cas/cnns.html#multilayer-perceptron-and-images",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptron and Images",
    "text": "Multilayer-Perceptron and Images\n\nMLP on example image.\nMLPs deal with flat inputs: The spatial structure gets lost.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-high-dimensional-inputs",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-high-dimensional-inputs",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and High-Dimensional Inputs",
    "text": "Multilayer-Perceptrons and High-Dimensional Inputs\n\nMLP on example image.Dimensionality of weight matrix \\(\\mathbf{W}\\) scales with input size. \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\), while \\(d\\) the dimensionality of the inputs, and \\(k\\) the number of neurons in the first hidden layer. \\(k\\) must be sufficiently large to learn the necessary patterns. This can lead to practical problems regarding memory and compute.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.The columns of the weight matrix \\(\\mathbf{W}\\) (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned by reshaping them to the dimensionality of the input images \\(\\mathbf{x} \\in \\mathbb{R}^{h \\times w}\\).",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-1",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-1",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-2",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-2",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.\nCan an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e. \\(f(P(\\mathbf{x}))\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-3",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-3",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.Yes! An MLP has no notion of distance and treats every connection between every input equally.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-translations",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-translations",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Translations",
    "text": "Multilayer-Perceptrons and Translations\n\nMLP and pattern shiftsOften the patterns we want to learn are not spatially constraint. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation.\n\nHow do MLPs deal with this?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#mlp-and-images",
    "href": "slides_cas/cnns.html#mlp-and-images",
    "title": "Convolutional Neural Networks",
    "section": "MLP and Images",
    "text": "MLP and Images\n\n\nFigure 8\n\nMLPs need to (re-) learn the same pattern at all positions, which is incredibly inefficient. This also means the training data must contain all patterns at all feasible positions, otherwise the MLP won’t be able to learn how to recognize them.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#mlps-and-images",
    "href": "slides_cas/cnns.html#mlps-and-images",
    "title": "Convolutional Neural Networks",
    "section": "MLPs and Images",
    "text": "MLPs and Images\n\nHigh-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.\nLocality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.\nStability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.\n\n\nDemonstration: https://adamharley.com/nn_vis/mlp/2d.html\n\n\nWhat do we need?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#invariance-and-equivariance",
    "href": "slides_cas/cnns.html#invariance-and-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Invariance and Equivariance",
    "text": "Invariance and Equivariance\nFor many tasks small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\n\nA function \\(f(\\mathbf{x})\\) is invariant to a transformation \\(g(\\mathbf{x})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = f(\\mathbf{x})\n\\end{align}\\]\n\n\nA function \\(f(\\mathbf{x})\\) (such as a layer in a neural network) of an image \\(\\mathbf{x}\\) is equivariant with respect to a transformation \\(g(\\mathbf{\\mathbf{x}})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = g(f(\\mathbf{x}))\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#example-invariance",
    "href": "slides_cas/cnns.html#example-invariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Invariance",
    "text": "Example Invariance\nExample where invariance is required:\n\nWhen objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#example-equivariance",
    "href": "slides_cas/cnns.html#example-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Equivariance",
    "text": "Example Equivariance\nExample where equivariance is required:\n\nWhen objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#history-experiments-on-cats",
    "href": "slides_cas/cnns.html#history-experiments-on-cats",
    "title": "Convolutional Neural Networks",
    "section": "History: Experiments on Cats",
    "text": "History: Experiments on Cats\n\nIllustration Source",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#visual-cortex",
    "href": "slides_cas/cnns.html#visual-cortex",
    "title": "Convolutional Neural Networks",
    "section": "Visual Cortex",
    "text": "Visual Cortex\n\nRepresentation of transformations in the visual cortex. Source: Kubilius (2017)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-neural-networks",
    "href": "slides_cas/cnns.html#convolutional-neural-networks",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\nThe activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: Li (2022)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolution-operation",
    "href": "slides_cas/cnns.html#convolution-operation",
    "title": "Convolutional Neural Networks",
    "section": "Convolution Operation",
    "text": "Convolution Operation\nConvolution in Deep Learning is typically implemented as cross-correlation.\n\\[\\begin{equation}\nS(i, j) = (K * I)(i, j) = b + \\sum_m \\sum_n I(i + m, j + n) K(m, n)\n\\end{equation}\\]\n\\(I\\) is the input (for example an image), \\(K\\) is the kernel (typically smaller than \\(I\\)) and \\(b\\) is a bias term which is being added to the weighted sum.\nIf \\(I\\) is an RGB image (in the first layer of a CNN for example), the kernel \\(K\\) would have dimensionality \\(3 \\times K \\times K\\) (assuming a square kernel). More generally we learn kernels of the dimensionality \\(C_{in} \\times K \\times K\\).\nMultiple kernels, let’s say \\(C_o\\) kernels, can be grouped together: \\(C_o \\times C_{in} \\times K \\times K\\).\nWe often refer to such tensors as filters or filter banks.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolution-on-rgb-images",
    "href": "slides_cas/cnns.html#convolution-on-rgb-images",
    "title": "Convolutional Neural Networks",
    "section": "Convolution on RGB Images",
    "text": "Convolution on RGB Images\n\nSource: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-1",
    "href": "slides_cas/cnns.html#convolutional-layers-1",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-2",
    "href": "slides_cas/cnns.html#convolutional-layers-2",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-3",
    "href": "slides_cas/cnns.html#convolutional-layers-3",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-4",
    "href": "slides_cas/cnns.html#convolutional-layers-4",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-5",
    "href": "slides_cas/cnns.html#convolutional-layers-5",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding-stride-kernel-size-dilation",
    "href": "slides_cas/cnns.html#padding-stride-kernel-size-dilation",
    "title": "Convolutional Neural Networks",
    "section": "Padding, Stride, Kernel Size, Dilation",
    "text": "Padding, Stride, Kernel Size, Dilation\nThere are several options to parameterise a convolution.\n\n\nPadding: How much padding is added to the input?\nStride: What is the step size (stride) of the kernel?\nKernel-Size: What is the kernel size?\nDilation: What is the dilation rate?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding",
    "href": "slides_cas/cnns.html#padding",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\nWhat is the output dimensionality when convolving a 3x3 kernel?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding-1",
    "href": "slides_cas/cnns.html#padding-1",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\nLeft: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#stride",
    "href": "slides_cas/cnns.html#stride",
    "title": "Convolutional Neural Networks",
    "section": "Stride",
    "text": "Stride\n\nConvolution with stride (1, 1)The kernel is shifted by 1 pixel before each new computation.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#stride-1",
    "href": "slides_cas/cnns.html#stride-1",
    "title": "Convolutional Neural Networks",
    "section": "Stride",
    "text": "Stride\n\nConvolution with stride (1, 1)The kernel is shifted by 2 pixel before each new computation.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#kernel-size",
    "href": "slides_cas/cnns.html#kernel-size",
    "title": "Convolutional Neural Networks",
    "section": "Kernel-Size",
    "text": "Kernel-Size",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#kernel-size-1",
    "href": "slides_cas/cnns.html#kernel-size-1",
    "title": "Convolutional Neural Networks",
    "section": "Kernel-Size",
    "text": "Kernel-Size\n\n\nWhat is the difference between using 3x3 vs 5x5 kernels? What is the same?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#dilation",
    "href": "slides_cas/cnns.html#dilation",
    "title": "Convolutional Neural Networks",
    "section": "Dilation",
    "text": "Dilation\n\n\nWhy would we use dilation \\(d \\gt 1\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#calculations-spatial-dimensionality-of-activation-maps",
    "href": "slides_cas/cnns.html#calculations-spatial-dimensionality-of-activation-maps",
    "title": "Convolutional Neural Networks",
    "section": "Calculations: Spatial Dimensionality of Activation Maps",
    "text": "Calculations: Spatial Dimensionality of Activation Maps\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)\n\\(d\\): Dilation rate.\n\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k-(k-1)(d-1)}{s} \\right\\rfloor + 1\n\\end{equation}\\]\nNote: If dilation is 1 \\((k-1)(d-1)\\) equals 0.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#calculations-number-of-weights-in-a-convolutional-layer",
    "href": "slides_cas/cnns.html#calculations-number-of-weights-in-a-convolutional-layer",
    "title": "Convolutional Neural Networks",
    "section": "Calculations: Number of Weights in a Convolutional Layer",
    "text": "Calculations: Number of Weights in a Convolutional Layer\nYou can calculate the total number of weights in a convolutional layer with the following formula:\n\n\\(k\\): Kernel size (assumption: square kernel, so kernel has dimensions \\(k \\times k\\))\n\\(C_{in}\\): Number of input channels\n\\(C_{out}\\): Number of output channels\n\n\n\\[\\begin{equation}\n\\text{Total Weights} = (k \\times k) \\times C_{in} \\times C_{out}\n\\end{equation}\\]\n\n\nEach filter has a size of \\(k \\times k \\times C_{in}\\), and there are \\(C_{out}\\) filters in total, resulting in \\((k \\times k \\times C_{in}) \\times C_{out}\\) weights.\n\n\nBiases: If each output channel has a bias term, add an additional \\(C_{out}\\) weights for the biases.\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#quiz",
    "href": "slides_cas/cnns.html#quiz",
    "title": "Convolutional Neural Networks",
    "section": "Quiz",
    "text": "Quiz\nScenario:\n\nInput activations: \\(3 \\times 32 \\times 32\\) (C, H, W)\nConvolution: 10 filters with \\(5  \\times 5\\) kernel size, stride=1, pad=2\n\nQuestions:\n\nWhat is the size of the activation map?\nHow many weights / parameters defines this operation?\n\nFormulas:\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k-(k-1)(d-1)}{s} \\right\\rfloor + 1\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#sparse-connectivity-and-parameter-sharing",
    "href": "slides_cas/cnns.html#sparse-connectivity-and-parameter-sharing",
    "title": "Convolutional Neural Networks",
    "section": "Sparse Connectivity and Parameter Sharing",
    "text": "Sparse Connectivity and Parameter Sharing\nLocal (Sparse) Connectivity: Neurons are only locally connected.\nParameter Sharing: Weights of a neuron are applied locally but are the same across the entire input.\n\n\nCan we recognize cats with a one-layer CNN?\nCan we recognize cats anywhere in an image with a CNN?\nIs parameter sharing always useful?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#translation-invariance-equivariance",
    "href": "slides_cas/cnns.html#translation-invariance-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Translation Invariance / Equivariance",
    "text": "Translation Invariance / Equivariance\nGiven a translation \\(g()\\), which spatially shifts inputs:\n\nTranslation invariance: \\(f(g(x))=f(x)\\)\nTranslation equivariance: \\(f(g(x))=g(f(x))\\)\n\n\nAre convolutions translation invariant, equivariant, or both?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#stacking-convolutions",
    "href": "slides_cas/cnns.html#stacking-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Stacking Convolutions",
    "text": "Stacking Convolutions\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field",
    "href": "slides_cas/cnns.html#receptive-field",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\nUsing a 5x5 kernel",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field-1",
    "href": "slides_cas/cnns.html#receptive-field-1",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\nUsing a 5x5 kernel and two layers",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field-2",
    "href": "slides_cas/cnns.html#receptive-field-2",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field-3",
    "href": "slides_cas/cnns.html#receptive-field-3",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field-4",
    "href": "slides_cas/cnns.html#receptive-field-4",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\nThe receptive field \\(r_0\\) of a convolutional neural network can be calculated as follows:\n\nDefine the recurrence relation:\n\n\\[\\begin{equation}\nr_{l-1} = s_l \\cdot r_l + (k_l - s_l)\n\\end{equation}\\]\nwhere: - \\(r_l\\): Receptive field at layer $l $. - \\(s_l\\): Stride of layer \\(l\\). - \\(k_l\\): Kernel size of layer \\(l\\).\n\nSolving this recurrence relation, we get the closed-form solution for the receptive field at the input layer \\(r_0\\):\n\n\\[\\begin{equation}\nr_0 = \\sum_{l=1}^L \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\n\\end{equation}\\]\nwhere: - $L $: Total number of layers. - $k_l $: Kernel size at layer $l $. - $s_i $: Stride at layer $i $, from layer 1 up to $l-1 $.\nThere is a nice article about receptive fields and how to calculate them: Link",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#architecture",
    "href": "slides_cas/cnns.html#architecture",
    "title": "Convolutional Neural Networks",
    "section": "Architecture",
    "text": "Architecture\nSequence of layers and their hyper-parameters defines an architecture.\n\nVGG - Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#learned-filters",
    "href": "slides_cas/cnns.html#learned-filters",
    "title": "Convolutional Neural Networks",
    "section": "Learned Filters",
    "text": "Learned Filters\n\nSource: Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#lets-test-it",
    "href": "slides_cas/cnns.html#lets-test-it",
    "title": "Convolutional Neural Networks",
    "section": "Let’s test it!",
    "text": "Let’s test it!\nLet’s train a CNN.\nHere is an interesting demo: https://poloclub.github.io/cnn-explainer/",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#downsampling",
    "href": "slides_cas/cnns.html#downsampling",
    "title": "Convolutional Neural Networks",
    "section": "Downsampling",
    "text": "Downsampling\nSpatial downsampling is often a desired property of convolutional layers.\nWe can achieve that by choosing a stride \\(\\gt 1\\).\nHowever, there are other options, such as pooling layers.\n\na) Sub-Sampling, b) Max Pooling, c) Average Pooling . Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#upsampling",
    "href": "slides_cas/cnns.html#upsampling",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling",
    "text": "Upsampling\nSometimes we need to increase the spatial dimensionality of our activation maps.\n\nWhat would be an example when we want to do that?\n\n\n\n\n\na) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#upsampling-with-transposed-convolutions",
    "href": "slides_cas/cnns.html#upsampling-with-transposed-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling with Transposed Convolutions",
    "text": "Upsampling with Transposed Convolutions",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#changing-the-number-of-channels",
    "href": "slides_cas/cnns.html#changing-the-number-of-channels",
    "title": "Convolutional Neural Networks",
    "section": "Changing the number of channels",
    "text": "Changing the number of channels\nSometimes one needs to just change the number of channels without spatial mixing. This can be achieved with 1x1 convolutions. \n\nSource: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#global-average-pooling",
    "href": "slides_cas/cnns.html#global-average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Global Average Pooling",
    "text": "Global Average Pooling\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.\n\nGlobal Average pooling, input (left) and output (right).",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#other-layers",
    "href": "slides_cas/cnns.html#other-layers",
    "title": "Convolutional Neural Networks",
    "section": "Other Layers",
    "text": "Other Layers\nThere are many other types of layers, also there are more flavours of convolutional layers.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/classification.html",
    "href": "pages/classification.html",
    "title": "5 - Image Classification",
    "section": "",
    "text": "Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. Figure 1 illustrates the problem in multi-class classification.\n\n\n\n\n\n\nFigure 1: Image Classification example.\n\n\n\nFigure 2 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012), which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.\n\n\n\n\n\n\nFigure 2: Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\nFigure 3 illustrates the challenge with images taken by camera traps, which need to be classified along animal species.\n\n\n\n\n\n\nFigure 3: Example images from camera traps."
  },
  {
    "objectID": "pages/classification.html#softmax-classifier",
    "href": "pages/classification.html#softmax-classifier",
    "title": "5 - Image Classification",
    "section": "Softmax Classifier",
    "text": "Softmax Classifier\nWith a Softmax Classifier, we interpret model predictions/scores as probabilities of class memberships: \\(P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\\). We interpret the output as a Categorical Distribution over all possible classes.\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the softmax function \\(\\sigma(\\mathbf{z})\\):\n\\[\nP(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\]\nFigure 4 shows an example of the effect of the softmax transformation.\n\n\n\n\n\n\n\n\nFigure 4: Logits (left) to probabilities with the Softmax function (right)."
  },
  {
    "objectID": "pages/classification.html#likelihood",
    "href": "pages/classification.html#likelihood",
    "title": "5 - Image Classification",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood of a data point \\((\\mathbf{x}^{(i)}, y^{(i)})\\) is the probability of observing/realizing a data point, given a model with certain parameters:\n\\[\nP(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\]\nThis means we formulate a model with a probabilistic interpretation of predictions: \\(f(\\theta, \\mathbf{x}^{(i)}): \\mathbb{R}^{n} \\mapsto [0, 1]\\)\nFor a multi-class classification, the label vector is one-hot encoded \\(\\mathbf{y}^{(i)} \\in \\{0, 1\\}^K\\), where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:\n\\[\n\\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\]\nSince only one entry in \\(\\mathbf{y}^{(i)}\\) is 1, the likelihood is simply the prediction for the true class \\(P(Y = y^{(i)}| X = \\mathbf{x}^{(i)})\\).\nMore Info\nGiven \\(\\mathbf{y} = [0, 1, 0, 1, 1]\\) and the following \\(\\hat{\\mathbf{y}}\\), calculate the likelihood.\nFor \\(\\hat{\\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9]\\):\n\n\n\n\n\n\nClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.362880\n\n\n\n\n\nDoes it get larger for \\(\\hat{\\mathbf{y}} = [0.1, 0.9, 0.2, 0.7, 0.9]\\)?\n\n\n\n\n\n\nClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.9, 0.2, 0.7, 0.9])\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.408240\n\n\n\n\n\nWhat happens if we increase the dataset by copying the vector 10 times? \\(\\hat{\\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9, 0.1, 0.8, ...]\\)?\n\n\n\n\n\n\nClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])\n\ny_true = np.repeat(y_true, 10, axis=0)\ny_pred = np.repeat(y_pred, 10, axis=0)\n\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.000040"
  },
  {
    "objectID": "pages/classification.html#maximum-likelihood",
    "href": "pages/classification.html#maximum-likelihood",
    "title": "5 - Image Classification",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe likelihood \\(P(\\mathbf{y} | \\theta, \\mathbf{X})\\) of observing our entire dataset \\((\\mathbf{X}, \\mathbf{y})\\), given the parameters \\(\\theta\\) and assuming that the data points \\((\\mathbf{x}^{(i)}, y^{(i)})\\) are independent and identically distributed, can be calculated as:\n\\[\\begin{equation}\n\\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the Maximum Likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset."
  },
  {
    "objectID": "pages/classification.html#negative-log-likelihood",
    "href": "pages/classification.html#negative-log-likelihood",
    "title": "5 - Image Classification",
    "section": "Negative Log-Likelihood",
    "text": "Negative Log-Likelihood\nWith Maximum Likelihood, we aim to choose the parameters \\(\\theta\\) such that \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\) is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\). Finally, we take the negative of the function, allowing us to minimize it.\n\\[\\begin{equation}\nL(\\mathbf{X}, \\mathbf{y}, \\theta) = - \\log \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\nL(\\mathbf{X}, \\mathbf{y}, \\theta) = -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]"
  },
  {
    "objectID": "pages/classification.html#cross-entropy",
    "href": "pages/classification.html#cross-entropy",
    "title": "5 - Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nThe loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution \\(\\mathbf{y}^{(i)}\\) and the predicted \\(\\mathbf{\\hat{y}}^{(i)}\\). Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution \\(p(x)\\) when using the approximation \\(q(x)\\).\n\\[\\begin{equation}\nCE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\nCE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{equation}\\]\nIt is evident that cross-entropy is identical to the negative log-likelihood.\n\n\n\n\n\n\n\n\nFigure 5: True Distribution (left) and Predicted Distribution (right).\n\n\n\n\n\nFigure 5 shows an example with a cross-entropy value of: 0.266."
  },
  {
    "objectID": "pages/classification.html#alexnet",
    "href": "pages/classification.html#alexnet",
    "title": "5 - Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\nCNNs became extremely popular after winning the ImageNet Competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in Figure 6. ImageNet is a large, hierarchical image dataset Deng et al. (2009), which enabled efficient training of CNNs for the first time.\nAlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.\nThe model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.\nFigure 6 shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.\n\n\n\n\n\n\nFigure 6: AlexNet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in Figure 7.\n\n\n\n\n\n\nFigure 7: AlexNet Llamas et al. (2017).\n\n\n\nFigure 8 presents the operations in AlexNet in tabular form.\n\n\n\n\n\n\nFigure 8: Source: Johnson (2019).\n\n\n\nWe can also easily load AlexNet via torchvision.\n\nimport torch\nimport torchvision.models as models\nimport torchinfo\n\nalexnet = models.alexnet()\nx = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\nyhat = alexnet(x)\n\nprint(torchinfo.summary(alexnet, input_size=(1, 3, 224, 224)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 1000]                 --\n├─Sequential: 1-1                        [1, 256, 6, 6]            --\n│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n├─Sequential: 1-3                        [1, 1000]                 --\n│    └─Dropout: 2-14                     [1, 9216]                 --\n│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n│    └─ReLU: 2-16                        [1, 4096]                 --\n│    └─Dropout: 2-17                     [1, 4096]                 --\n│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n│    └─ReLU: 2-19                        [1, 4096]                 --\n│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n==========================================================================================\nTotal params: 61,100,840\nTrainable params: 61,100,840\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 714.68\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 3.95\nParams size (MB): 244.40\nEstimated Total Size (MB): 248.96\n=========================================================================================="
  },
  {
    "objectID": "pages/classification.html#vgg",
    "href": "pages/classification.html#vgg",
    "title": "5 - Image Classification",
    "section": "VGG",
    "text": "VGG\nSimonyan and Zisserman (2015) won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. Figure 9 shows the architecture as presented in the original paper. Figure 10 visualizes the architecture.\n\n\n\n\n\n\nFigure 9: VGG Simonyan and Zisserman (2015).\n\n\n\n\n\n\n\n\n\nFigure 10: Source: Link\n\n\n\nVGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled. This was done to maintain the time complexity of the layers. VGG does not use normalization layers.\nFigure 11 compares VGG with AlexNet.\n\n\n\n\n\n\nFigure 11: Source: Johnson (2019)."
  },
  {
    "objectID": "pages/classification.html#resnet",
    "href": "pages/classification.html#resnet",
    "title": "5 - Image Classification",
    "section": "ResNet",
    "text": "ResNet\nHe et al. (2016) wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see Figure 12). They noticed that the performance did not degrade due to overfitting (see Figure 13, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. He et al. (2016) hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.\n\n\n\n\n\n\nFigure 12: Source: He et al. (2016)\n\n\n\n\n\n\n\n\n\nFigure 13: Source: He et al. (2016)\n\n\n\nThey then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. Figure 14 shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer \\(i\\) to layer \\(i+1\\), the residue.\n\n\n\n\n\n\nFigure 14: ResNet He et al. (2016) (Graphic from Johnson (2019).)\n\n\n\nResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. Figure 15 shows the architecture.\n\n\n\n\n\n\nFigure 15: Source: He et al. (2016).\n\n\n\nResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in Figure 16.\n\n\n\n\n\n\nFigure 16: Source: Li et al. (2018)."
  },
  {
    "objectID": "pages/classification.html#convnext",
    "href": "pages/classification.html#convnext",
    "title": "5 - Image Classification",
    "section": "ConvNext",
    "text": "ConvNext\nOne of the most modern CNN architectures was described in Liu et al. (2022). This architecture uses tricks and implementation ideas accumulated over decades from various architectures. Figure 17 shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.\n\n\n\n\n\n\nFigure 17: Convnext Liu et al. (2022).\n\n\n\nThere is already a new version of this architecture Woo et al. (2023)."
  },
  {
    "objectID": "pages/classification.html#imagenet-performance",
    "href": "pages/classification.html#imagenet-performance",
    "title": "5 - Image Classification",
    "section": "ImageNet Performance",
    "text": "ImageNet Performance\nFigure 18 shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see Link.\n\n\n\n\n\n\nFigure 18: Graphic from Johnson (2019)"
  },
  {
    "objectID": "pages/classification.html#which-architecture",
    "href": "pages/classification.html#which-architecture",
    "title": "5 - Image Classification",
    "section": "Which Architecture?",
    "text": "Which Architecture?\nWhich architecture should be chosen for a specific problem? A common tip is: Don’t be a hero.\nOne should rely on off-the-shelf architectures and not implement their own without a good reason.\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.\nImportant considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory)."
  },
  {
    "objectID": "pages/classification.html#squeezeexcite-networks",
    "href": "pages/classification.html#squeezeexcite-networks",
    "title": "5 - Image Classification",
    "section": "Squeeze/Excite Networks",
    "text": "Squeeze/Excite Networks\nSqueeze-and-Excite Networks (SE-Networks) were introduced in 2019 Hu et al. (2019). These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. Figure 19 shows an illustration.\n\n\n\n\n\n\nFigure 19: Source: Hu et al. (2019).\n\n\n\nThese SE blocks can be easily applied to arbitrary activation maps. Figure 19 shows an input feature map \\(\\mathbf{\\mathsf{X}} \\in \\mathbb{R}^{H' \\times W' \\times C'}\\) that is transformed with \\(F_{tr}\\) (e.g., with a convolutional layer). This results in the activation maps \\(\\mathbf{\\mathsf{U}} \\in \\mathbb{R}^{H \\times W \\times C}\\).\nThe transformation \\(F_{sq}(\\cdot)\\) applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map \\(\\mathbf{\\mathsf{U}}\\). This results in a vector \\(z \\in \\mathbb{R}^{1 \\times 1 \\times C}\\).\nThe excitation operation \\(F_{ex}(\\cdot, W)\\) uses a gating mechanism with parameters \\(W\\), implemented with two fully-connected layers and activation functions. The result is \\(s \\in \\mathbb{R}^{1 \\times 1 \\times C}\\), the channel weights. It is called gating because the weights range from \\([0, 1]\\) and thus control how much information of a channel flows through (gatekeeping).\nFinally, \\(F_{scale}(\\cdot)\\) scales the activation maps \\(\\mathbf{\\mathsf{U}}\\) with the channel weights.\nThe operation can be described as follows:\n\\[\\begin{equation}\nU = F_{tr}(X) \\\\\nz = F_{sq}(U) = \\text{GlobalAvgPool}(U) \\\\\ns = F_{ex}(z, W) = \\sigma(\\mathbf{W}_2 g(\\mathbf{W}_1 z)) \\\\\n\\hat{X} = F_{scale}(U, s) = U \\odot s\n\\end{equation}\\]\nwhere \\(g()\\) represents the ReLU function, and \\(\\sigma\\) represents the sigmoid function."
  },
  {
    "objectID": "pages/classification.html#normalization-layers",
    "href": "pages/classification.html#normalization-layers",
    "title": "5 - Image Classification",
    "section": "Normalization Layers",
    "text": "Normalization Layers\nNormalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in Figure 20. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see Ba, Kiros, and Hinton (2016)). The general form of normalization is given in equation Equation 1. The parameters \\(\\gamma\\) and \\(\\beta\\) are learned, while the means \\(E[x]\\) and variances \\(\\sigma^2[x]\\) are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better Santurkar et al. (2019).\n\\[\ny = \\frac{x - E[x]}{\\sqrt{\\sigma^2[x] + \\epsilon}} * \\gamma + \\beta\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 20: Source: Qiao et al. (2020)."
  },
  {
    "objectID": "pages/classification.html#architecture",
    "href": "pages/classification.html#architecture",
    "title": "5 - Image Classification",
    "section": "Architecture",
    "text": "Architecture\nExample of a CNN architecture.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()"
  },
  {
    "objectID": "pages/classification.html#loss-function-1",
    "href": "pages/classification.html#loss-function-1",
    "title": "5 - Image Classification",
    "section": "Loss Function",
    "text": "Loss Function\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
  }
]