[
  {
    "objectID": "pages/frameworks.html",
    "href": "pages/frameworks.html",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "",
    "text": "There are a variety of Deep Learning frameworks. These frameworks allow for easy configuration, training, and deploying of neural networks. They are often developed via Python API. Figure 1 shows some frameworks.\n\n\n\n\n\n\nFigure 1: Frameworks (from Li (2022)).\n\n\n\nKey features of such frameworks are:\n\nFast development and testing of neural networks\nAutomatic differentiation of operations\nEfficient execution on diverse hardware\n\n\n\nAt the core of neural networks is the Computational Graph. It automatically embeds dependent operations in a directed acyclic graph (DAG). Gradients are tracked as needed, allowing variables to be efficiently updated/trained.\nThe following shows an example in Numpy where we define computations and manually calculate derivatives. The graph is shown in Figure 2.\n\\[\\begin{equation}\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) =  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij}\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 2: Computational Graph.\n\n\n\n\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0\nde_dd = 1.0\nde_dc = c\ndd_da = b\n\ndf_da = df_de * de_dd * dd_da\n\nprint(df_da)\n\n[[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\n\n\nHere’s the same example in PyTorch. Using x.backward(), gradients with respect to x are computed for variables connected to x.\n\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n\n\nHere are the nodes of the computational graph.\n\nfrom torchviz import make_dot\nmake_dot(f, params={'a': a, 'b': b, 'c': c, 'f':f , 'd': d, 'e':e })\n\n\n\n\n\n\n\n\nTo perform the computation on a GPU, a simple instruction is enough:\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\na = a.to(device=device)\nb = b.to(device=device)\nc = c.to(device=device)\n\nUsing cpu device"
  },
  {
    "objectID": "pages/frameworks.html#computational-graph-autograd",
    "href": "pages/frameworks.html#computational-graph-autograd",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "",
    "text": "At the core of neural networks is the Computational Graph. It automatically embeds dependent operations in a directed acyclic graph (DAG). Gradients are tracked as needed, allowing variables to be efficiently updated/trained.\nThe following shows an example in Numpy where we define computations and manually calculate derivatives. The graph is shown in Figure 2.\n\\[\\begin{equation}\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) =  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij}\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 2: Computational Graph.\n\n\n\n\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0\nde_dd = 1.0\nde_dc = c\ndd_da = b\n\ndf_da = df_de * de_dd * dd_da\n\nprint(df_da)\n\n[[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\n\n\nHere’s the same example in PyTorch. Using x.backward(), gradients with respect to x are computed for variables connected to x.\n\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n\n\nHere are the nodes of the computational graph.\n\nfrom torchviz import make_dot\nmake_dot(f, params={'a': a, 'b': b, 'c': c, 'f':f , 'd': d, 'e':e })\n\n\n\n\n\n\n\n\nTo perform the computation on a GPU, a simple instruction is enough:\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\na = a.to(device=device)\nb = b.to(device=device)\nc = c.to(device=device)\n\nUsing cpu device"
  },
  {
    "objectID": "pages/frameworks.html#fundamental-concepts",
    "href": "pages/frameworks.html#fundamental-concepts",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Fundamental Concepts",
    "text": "Fundamental Concepts\n\nTensor: N-dimensional array, similar to numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks"
  },
  {
    "objectID": "pages/frameworks.html#tensors",
    "href": "pages/frameworks.html#tensors",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Tensors",
    "text": "Tensors\ntorch.Tensor is the central data structure in PyTorch. Essentially very similar to numpy.array, it can be easily loaded onto GPUs.\nTensors can be created in various ways. For example, from lists:\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\nprint(x_data)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\nOr from numpy.ndarray:\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nprint(x_np)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\nOr from other tensors:\n\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\nOnes Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.0205, 0.3983],\n        [0.4603, 0.1273]]) \n\n\n\nOr with randomly generated numbers or constants:\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.0878, 0.2615, 0.0568],\n        [0.3083, 0.5013, 0.5037]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nTensor attributes:\n\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\nThere are over 100 operations that can be performed on a tensor. The full list is available here.\nIndexing and Slicing:\n\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[:, -1]}\")\ntensor[:,1] = 0\nprint(tensor)\n\nFirst row: tensor([1., 1., 1., 1.])\nFirst column: tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\nJoining tensors:\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n\n\nArithmetic operations:\n\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])"
  },
  {
    "objectID": "pages/frameworks.html#autograd",
    "href": "pages/frameworks.html#autograd",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Autograd",
    "text": "Autograd\nTo train neural networks, backpropagation is typically used. This calculates the gradient of the loss function with respect to the model parameters. To compute these gradients, PyTorch provides an auto-diff functionality: torch.autograd. This can automatically compute gradients for a computational graph.\nThe following is an example using a 1-layer neural network (see Figure 3 ):\n\n\n\n\n\n\nFigure 3: Source: PyTorch\n\n\n\nHere is the definition of the network in PyTorch:\n\nimport torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5\n\n, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\nWe can now use Autograd to compute the gradient:\n\nloss.backward()\nprint(w.grad)\nprint(b.grad)\n\ntensor([[0.2009, 0.3211, 0.2620],\n        [0.2009, 0.3211, 0.2620],\n        [0.2009, 0.3211, 0.2620],\n        [0.2009, 0.3211, 0.2620],\n        [0.2009, 0.3211, 0.2620]])\ntensor([0.2009, 0.3211, 0.2620])"
  },
  {
    "objectID": "pages/frameworks.html#torch.nn",
    "href": "pages/frameworks.html#torch.nn",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "torch.nn",
    "text": "torch.nn\nPyTorch provides various building blocks for creating neural networks. These are available in torch.nn. Additionally, you can define any compositions of such building blocks that inherit from torch.nn.Module. A neural network is typically a torch.nn.Module. Each module implements the forward() method to define how data is processed.\nHere is an example:\n\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nYou can also visualize the model:\n\nmodel = NeuralNetwork()\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nTo use the model, you can pass input data. This will execute the forward() method, along with background operations.\n\nX = torch.rand(1, 28, 28)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\nPredicted class: tensor([1])\n\n\nThe executed operations will look like this:\n\nfrom torchviz import make_dot\nmake_dot(logits)"
  },
  {
    "objectID": "pages/frameworks.html#torch.optim",
    "href": "pages/frameworks.html#torch.optim",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "torch.optim",
    "text": "torch.optim\nTo optimize the parameters of a model, you need an optimization algorithm. torch.optim implements various algorithms, such as Stochastic Gradient Descent or the often used Adam Optimizer.\n\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nYou can then use the optimizer to adjust the parameters, you just need to define a loss function:\n\nloss_fn = torch.nn.CrossEntropyLoss()\nfor i in range(0, 3):\n    input, target = torch.rand(1, 28, 28), torch.randint(low=0, high=10, size=(1, ))\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n\n\n\n\n\n\nNote\n\n\n\nNote optimizer.zero_grad() which resets the accumulated gradients of the variables to 0."
  },
  {
    "objectID": "pages/frameworks.html#training-loops",
    "href": "pages/frameworks.html#training-loops",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Training Loops",
    "text": "Training Loops\nTypically, you put together a training loop to train a model. A training loop iterates over batches of data and optimizes the model parameters with each iteration.\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\n\n\n\n\n\n\nNote\n\n\n\nPyTorch-Lightning provides many functionalities to simplify managing training loops. It simplifies using PyTorch similar to how Keras does for TensorFlow."
  },
  {
    "objectID": "pages/frameworks.html#pre-trained-models",
    "href": "pages/frameworks.html#pre-trained-models",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Pre-trained models",
    "text": "Pre-trained models\nSince training models can be time-consuming and expensive, pre-trained models are often used. They allow models to be adapted to a specific task more quickly and cost-effectively. In many areas, particularly NLP and computer vision, using pre-trained models is standard. PyTorch provides torchvision for computer vision applications. torchvision provides functionalities useful for modeling image data. Pre-trained models can also be easily integrated, as shown in the following example:\n\nfrom torchvision.models import resnet50, ResNet50_Weights\n\nweights = ResNet50_Weights.IMAGENET1K_V2\nmodel = resnet50(weights=weights)"
  },
  {
    "objectID": "pages/frameworks.html#tensorflow",
    "href": "pages/frameworks.html#tensorflow",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "TensorFlow",
    "text": "TensorFlow\nFor a long time, PyTorch and TensorFlow have been the biggest deep learning frameworks. TensorFlow stands out with a clean high-level API with Keras, which allows for easy implementation of complex models. Traditionally, TensorFlow is well established in the industry, while PyTorch is widely used in academia."
  },
  {
    "objectID": "pages/frameworks.html#scikit-learn",
    "href": "pages/frameworks.html#scikit-learn",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Scikit-Learn",
    "text": "Scikit-Learn\nScikit-Learn is THE machine learning framework in Python. However, Scikit-Learn never covered the area of neural networks and lacks auto-diff functionality. Therefore, Scikit-Learn is irrelevant when training neural networks. However, Scikit-Learn functionalities are often used to carry out the machine learning process, such as splitting datasets into train, validation, and test sets. Also, visualizations, such as the confusion matrix or calculating metrics, can be done via Scikit-Learn."
  },
  {
    "objectID": "pages/frameworks.html#onnx",
    "href": "pages/frameworks.html#onnx",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "ONNX",
    "text": "ONNX\nONNX is an open format to represent machine learning models. It allows models trained in one framework to be transferred to another. Trained models can also be deployed on various platforms."
  },
  {
    "objectID": "pages/frameworks.html#monitoring",
    "href": "pages/frameworks.html#monitoring",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Monitoring",
    "text": "Monitoring\nWhen training models, monitoring the training process, debugging, and logging hyperparameters, metrics, etc., is very important. Various tools enable these functionalities. Well-known examples are TensorBoard and Weights & Biases."
  },
  {
    "objectID": "pages/frameworks.html#tensor-operations",
    "href": "pages/frameworks.html#tensor-operations",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Tensor Operations",
    "text": "Tensor Operations\nIn neural networks, there are many tensor operations. Tensors are essentially multi-dimensional arrays, such as a scalar \\(x\\), a vector \\(\\mathbf{x}\\), or a matrix \\(\\mathbf{X}\\).\nFigure 4 illustrates a matrix multiplication, a typical representative of a tensor operation. As you can see, the calculations (entries of the matrix \\(\\mathbf{A}\\mathbf{C}\\)) are independent of each other and can be fully parallelized.\n\n\n\n\n\n\nFigure 4: Matrix Multiplication (from Li (2022))."
  },
  {
    "objectID": "pages/frameworks.html#graphics-processing-units-gpus",
    "href": "pages/frameworks.html#graphics-processing-units-gpus",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Graphics Processing Units (GPUs)",
    "text": "Graphics Processing Units (GPUs)\nGPUs have made deep learning possible in the first place. With their parallel structure, they can efficiently compute parallelizable tasks such as tensor operations.\nCPUs have far fewer cores than GPUs, but they are faster and can handle more complex tasks. CPUs are therefore ideal for sequential tasks. GPUs have many more cores, which are less complex and slower. Therefore, GPUs are excellent for parallel tasks. Figure 5 illustrates the differences.\n\n\n\n\n\n\nFigure 5: CPU vs GPU example (from Li (2022))."
  },
  {
    "objectID": "pages/frameworks.html#cuda-cudnn",
    "href": "pages/frameworks.html#cuda-cudnn",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "CUDA & cuDNN",
    "text": "CUDA & cuDNN\nCUDA is an API by Nvidia to perform computations on the GPU. It allows parallelizable tasks to be implemented efficiently. cuDNN is a library that efficiently executes certain operations, such as convolutions, in neural networks on the GPU. cuDNN is based on CUDA and significantly accelerates the training of neural networks. Figure 6 illustrates speed differences when training various neural networks with CPU, GPU, and optimized cuDNN.\n\n\n\n\n\n\nFigure 6: Speed comparison (from Li (2022), data from Link)"
  },
  {
    "objectID": "pages/frameworks.html#data-loading",
    "href": "pages/frameworks.html#data-loading",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Data Loading",
    "text": "Data Loading\nA crucial bottleneck in practice is the transfer of data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is referred to as GPU starvation. There are several approaches to solve this problem:\n\nRead the data into RAM (not feasible for larger datasets)\nUse fast disks, such as SSDs\nUtilize multiple CPU threads to read data in parallel and keep it in RAM (pre-fetching)\n\nFigure 7 shows the various components.\n\n\n\n\n\n\nFigure 7: Source: Li (2022)\n\n\n\nDeep learning frameworks like PyTorch implement special classes that allow data to be prepared in multiple threads. Sometimes a certain number of CPU cores is needed to supply a GPU with enough data. Figure 8 shows a starved GPU: You can clearly see that the utilization repeatedly drops to 0 because the GPU has to wait for data.\n\n\n\n\n\n\nFigure 8: The Y-axis shows the GPU utilization in percentage, while the X-axis represents time. Source"
  },
  {
    "objectID": "pages/frameworks.html#gpu-parallelism",
    "href": "pages/frameworks.html#gpu-parallelism",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "GPU Parallelism",
    "text": "GPU Parallelism\nModels can also be trained on multiple GPUs. There are two main paradigms: data parallelism and model parallelism (see Figure 9 ). With data parallelism, each GPU has a copy of the model, and each GPU is trained on different data batches. With model parallelism, the model is split across multiple GPUs. Models can be trained on a server with multiple GPUs or even over the network (distributed). ML frameworks provide functionalities to handle these.\n\n\n\n\n\n\nFigure 9: Data and Model Parallelism (from Li (2022))."
  },
  {
    "objectID": "pages/classification.html",
    "href": "pages/classification.html",
    "title": "5 - Image Classification",
    "section": "",
    "text": "Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. Figure 1 illustrates the problem in multi-class classification.\n\n\n\n\n\n\nFigure 1: Image Classification example.\n\n\n\nFigure 2 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012), which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.\n\n\n\n\n\n\nFigure 2: Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\nFigure 3 illustrates the challenge with images taken by camera traps, which need to be classified along animal species.\n\n\n\n\n\n\nFigure 3: Example images from camera traps."
  },
  {
    "objectID": "pages/classification.html#softmax-classifier",
    "href": "pages/classification.html#softmax-classifier",
    "title": "5 - Image Classification",
    "section": "Softmax Classifier",
    "text": "Softmax Classifier\nWith a Softmax Classifier, we interpret model predictions/scores as probabilities of class memberships: \\(P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\\). We interpret the output as a Categorical Distribution over all possible classes.\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the softmax function \\(\\sigma(\\mathbf{z})\\):\n\\[\nP(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\]\nFigure 4 shows an example of the effect of the softmax transformation.\n\n\n\n\n\n\n\n\nFigure 4: Logits (left) to probabilities with the Softmax function (right)."
  },
  {
    "objectID": "pages/classification.html#likelihood",
    "href": "pages/classification.html#likelihood",
    "title": "5 - Image Classification",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood of a data point \\((\\mathbf{x}^{(i)}, y^{(i)})\\) is the probability of observing/realizing a data point, given a model with certain parameters:\n\\[\nP(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\]\nThis means we formulate a model with a probabilistic interpretation of predictions: \\(f(\\theta, \\mathbf{x}^{(i)}): \\mathbb{R}^{n} \\mapsto [0, 1]\\)\nFor a multi-class classification, the label vector is one-hot encoded \\(\\mathbf{y}^{(i)} \\in \\{0, 1\\}^K\\), where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:\n\\[\n\\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\]\nSince only one entry in \\(\\mathbf{y}^{(i)}\\) is 1, the likelihood is simply the prediction for the true class \\(P(Y = y^{(i)}| X = \\mathbf{x}^{(i)})\\).\nMore Info\nGiven \\(\\mathbf{y} = [0, 1, 0, 1, 1]\\) and the following \\(\\hat{\\mathbf{y}}\\), calculate the likelihood.\nFor \\(\\hat{\\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9]\\):\n\n\n\n\n\n\nClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.362880\n\n\n\n\n\nDoes it get larger for \\(\\hat{\\mathbf{y}} = [0.1, 0.9, 0.2, 0.7, 0.9]\\)?\n\n\n\n\n\n\nClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.9, 0.2, 0.7, 0.9])\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.408240\n\n\n\n\n\nWhat happens if we increase the dataset by copying the vector 10 times? \\(\\hat{\\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9, 0.1, 0.8, ...]\\)?\n\n\n\n\n\n\nClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])\n\ny_true = np.repeat(y_true, 10, axis=0)\ny_pred = np.repeat(y_pred, 10, axis=0)\n\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.000040"
  },
  {
    "objectID": "pages/classification.html#maximum-likelihood",
    "href": "pages/classification.html#maximum-likelihood",
    "title": "5 - Image Classification",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe likelihood \\(P(\\mathbf{y} | \\theta, \\mathbf{X})\\) of observing our entire dataset \\((\\mathbf{X}, \\mathbf{y})\\), given the parameters \\(\\theta\\) and assuming that the data points \\((\\mathbf{x}^{(i)}, y^{(i)})\\) are independent and identically distributed, can be calculated as:\n\\[\\begin{equation}\n\\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the Maximum Likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset."
  },
  {
    "objectID": "pages/classification.html#negative-log-likelihood",
    "href": "pages/classification.html#negative-log-likelihood",
    "title": "5 - Image Classification",
    "section": "Negative Log-Likelihood",
    "text": "Negative Log-Likelihood\nWith Maximum Likelihood, we aim to choose the parameters \\(\\theta\\) such that \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\) is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\). Finally, we take the negative of the function, allowing us to minimize it.\n\\[\\begin{equation}\nL(\\mathbf{X}, \\mathbf{y}, \\theta) = - \\log \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\nL(\\mathbf{X}, \\mathbf{y}, \\theta) = -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]"
  },
  {
    "objectID": "pages/classification.html#cross-entropy",
    "href": "pages/classification.html#cross-entropy",
    "title": "5 - Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nThe loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution \\(\\mathbf{y}^{(i)}\\) and the predicted \\(\\mathbf{\\hat{y}}^{(i)}\\). Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution \\(p(x)\\) when using the approximation \\(q(x)\\).\n\\[\\begin{equation}\nCE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\nCE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{equation}\\]\nIt is evident that cross-entropy is identical to the negative log-likelihood.\n\n\n\n\n\n\n\n\nFigure 5: True Distribution (left) and Predicted Distribution (right).\n\n\n\n\n\nFigure 5 shows an example with a cross-entropy value of: 0.266."
  },
  {
    "objectID": "pages/classification.html#alexnet",
    "href": "pages/classification.html#alexnet",
    "title": "5 - Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\nCNNs became extremely popular after winning the ImageNet Competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in Figure 6. ImageNet is a large, hierarchical image dataset Deng et al. (2009), which enabled efficient training of CNNs for the first time.\nAlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.\nThe model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.\nFigure 6 shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.\n\n\n\n\n\n\nFigure 6: AlexNet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in Figure 7.\n\n\n\n\n\n\nFigure 7: AlexNet Llamas et al. (2017).\n\n\n\nFigure 8 presents the operations in AlexNet in tabular form.\n\n\n\n\n\n\nFigure 8: Source: Johnson (2019).\n\n\n\nWe can also easily load AlexNet via torchvision.\n\nimport torch\nimport torchvision.models as models\nimport torchinfo\n\nalexnet = models.alexnet()\nx = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\nyhat = alexnet(x)\n\nprint(torchinfo.summary(alexnet, input_size=(1, 3, 224, 224)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 1000]                 --\n├─Sequential: 1-1                        [1, 256, 6, 6]            --\n│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n├─Sequential: 1-3                        [1, 1000]                 --\n│    └─Dropout: 2-14                     [1, 9216]                 --\n│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n│    └─ReLU: 2-16                        [1, 4096]                 --\n│    └─Dropout: 2-17                     [1, 4096]                 --\n│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n│    └─ReLU: 2-19                        [1, 4096]                 --\n│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n==========================================================================================\nTotal params: 61,100,840\nTrainable params: 61,100,840\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 714.68\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 3.95\nParams size (MB): 244.40\nEstimated Total Size (MB): 248.96\n=========================================================================================="
  },
  {
    "objectID": "pages/classification.html#vgg",
    "href": "pages/classification.html#vgg",
    "title": "5 - Image Classification",
    "section": "VGG",
    "text": "VGG\nSimonyan and Zisserman (2015) won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. Figure 9 shows the architecture as presented in the original paper. Figure 10 visualizes the architecture.\n\n\n\n\n\n\nFigure 9: VGG Simonyan and Zisserman (2015).\n\n\n\n\n\n\n\n\n\nFigure 10: Source: Link\n\n\n\nVGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled. This was done to maintain the time complexity of the layers. VGG does not use normalization layers.\nFigure 11 compares VGG with AlexNet.\n\n\n\n\n\n\nFigure 11: Source: Johnson (2019)."
  },
  {
    "objectID": "pages/classification.html#resnet",
    "href": "pages/classification.html#resnet",
    "title": "5 - Image Classification",
    "section": "ResNet",
    "text": "ResNet\nHe et al. (2016) wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see Figure 12). They noticed that the performance did not degrade due to overfitting (see Figure 13, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. He et al. (2016) hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.\n\n\n\n\n\n\nFigure 12: Source: He et al. (2016)\n\n\n\n\n\n\n\n\n\nFigure 13: Source: He et al. (2016)\n\n\n\nThey then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. Figure 14 shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer \\(i\\) to layer \\(i+1\\), the residue.\n\n\n\n\n\n\nFigure 14: ResNet He et al. (2016) (Graphic from Johnson (2019).)\n\n\n\nResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. Figure 15 shows the architecture.\n\n\n\n\n\n\nFigure 15: Source: He et al. (2016).\n\n\n\nResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in Figure 16.\n\n\n\n\n\n\nFigure 16: Source: Li et al. (2018)."
  },
  {
    "objectID": "pages/classification.html#convnext",
    "href": "pages/classification.html#convnext",
    "title": "5 - Image Classification",
    "section": "ConvNext",
    "text": "ConvNext\nOne of the most modern CNN architectures was described in Liu et al. (2022). This architecture uses tricks and implementation ideas accumulated over decades from various architectures. Figure 17 shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.\n\n\n\n\n\n\nFigure 17: Convnext Liu et al. (2022).\n\n\n\nThere is already a new version of this architecture Woo et al. (2023)."
  },
  {
    "objectID": "pages/classification.html#imagenet-performance",
    "href": "pages/classification.html#imagenet-performance",
    "title": "5 - Image Classification",
    "section": "ImageNet Performance",
    "text": "ImageNet Performance\nFigure 18 shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see Link.\n\n\n\n\n\n\nFigure 18: Graphic from Johnson (2019)"
  },
  {
    "objectID": "pages/classification.html#which-architecture",
    "href": "pages/classification.html#which-architecture",
    "title": "5 - Image Classification",
    "section": "Which Architecture?",
    "text": "Which Architecture?\nWhich architecture should be chosen for a specific problem? A common tip is: Don’t be a hero.\nOne should rely on off-the-shelf architectures and not implement their own without a good reason.\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.\nImportant considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory)."
  },
  {
    "objectID": "pages/classification.html#squeezeexcite-networks",
    "href": "pages/classification.html#squeezeexcite-networks",
    "title": "5 - Image Classification",
    "section": "Squeeze/Excite Networks",
    "text": "Squeeze/Excite Networks\nSqueeze-and-Excite Networks (SE-Networks) were introduced in 2019 Hu et al. (2019). These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. Figure 19 shows an illustration.\n\n\n\n\n\n\nFigure 19: Source: Hu et al. (2019).\n\n\n\nThese SE blocks can be easily applied to arbitrary activation maps. Figure 19 shows an input feature map \\(\\mathbf{\\mathsf{X}} \\in \\mathbb{R}^{H' \\times W' \\times C'}\\) that is transformed with \\(F_{tr}\\) (e.g., with a convolutional layer). This results in the activation maps \\(\\mathbf{\\mathsf{U}} \\in \\mathbb{R}^{H \\times W \\times C}\\).\nThe transformation \\(F_{sq}(\\cdot)\\) applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map \\(\\mathbf{\\mathsf{U}}\\). This results in a vector \\(z \\in \\mathbb{R}^{1 \\times 1 \\times C}\\).\nThe excitation operation \\(F_{ex}(\\cdot, W)\\) uses a gating mechanism with parameters \\(W\\), implemented with two fully-connected layers and activation functions. The result is \\(s \\in \\mathbb{R}^{1 \\times 1 \\times C}\\), the channel weights. It is called gating because the weights range from \\([0, 1]\\) and thus control how much information of a channel flows through (gatekeeping).\nFinally, \\(F_{scale}(\\cdot)\\) scales the activation maps \\(\\mathbf{\\mathsf{U}}\\) with the channel weights.\nThe operation can be described as follows:\n\\[\\begin{equation}\nU = F_{tr}(X) \\\\\nz = F_{sq}(U) = \\text{GlobalAvgPool}(U) \\\\\ns = F_{ex}(z, W) = \\sigma(\\mathbf{W}_2 g(\\mathbf{W}_1 z)) \\\\\n\\hat{X} = F_{scale}(U, s) = U \\odot s\n\\end{equation}\\]\nwhere \\(g()\\) represents the ReLU function, and \\(\\sigma\\) represents the sigmoid function."
  },
  {
    "objectID": "pages/classification.html#normalization-layers",
    "href": "pages/classification.html#normalization-layers",
    "title": "5 - Image Classification",
    "section": "Normalization Layers",
    "text": "Normalization Layers\nNormalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in Figure 20. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see Ba, Kiros, and Hinton (2016)). The general form of normalization is given in equation Equation 1. The parameters \\(\\gamma\\) and \\(\\beta\\) are learned, while the means \\(E[x]\\) and variances \\(\\sigma^2[x]\\) are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better Santurkar et al. (2019).\n\\[\ny = \\frac{x - E[x]}{\\sqrt{\\sigma^2[x] + \\epsilon}} * \\gamma + \\beta\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 20: Source: Qiao et al. (2020)."
  },
  {
    "objectID": "pages/classification.html#architecture",
    "href": "pages/classification.html#architecture",
    "title": "5 - Image Classification",
    "section": "Architecture",
    "text": "Architecture\nExample of a CNN architecture.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()"
  },
  {
    "objectID": "pages/classification.html#loss-function-1",
    "href": "pages/classification.html#loss-function-1",
    "title": "5 - Image Classification",
    "section": "Loss Function",
    "text": "Loss Function\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "pages/neural_networks.html",
    "href": "pages/neural_networks.html",
    "title": "3 - Neural Networks",
    "section": "",
    "text": "A biological neural network is a part of the nervous system and consists of interconnected neurons. A neuron is connected to other neurons via dendrites (these are “weighted” input signals) and via the axon (output signal) (see Figure 1). If the input signals exceed a certain threshold, the neuron “fires” and sends a signal through the axon, which then serves as an input signal for other neurons. Humans have about 86 billion neurons, each connected to about 1000 others on average (source).\n\n\n\n\n\n\nFigure 1: Schematic representation of connected neurons. Phillips (2015)\n\n\n\nOptical signals are processed, among other things, in the visual cortex (Figure 2). Signals are processed hierarchically, with the first layers recognizing simple patterns and later layers recognizing shapes and objects. See also the work of Hubel and Wiesel Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 2: Representation of transformations in the visual cortex. Kubilius (2017)\n\n\n\nThe question that arises is:\nCan we create artificial neural networks and reproduce the performance of biological neural networks?"
  },
  {
    "objectID": "pages/neural_networks.html#activation-functions",
    "href": "pages/neural_networks.html#activation-functions",
    "title": "3 - Neural Networks",
    "section": "Activation Functions",
    "text": "Activation Functions\nThe following code shows how activation functions are important for modeling non-linear relationships. The model has a hidden layer with several neurons but (left) no activation function and (right) with ReLU activation.\n\n\n\n\n\n\n\n\nFigure 4: Linear (left) vs non-linear (right) activation function.\n\n\n\n\n\nFigure 5 now shows a neural network including activation functions (ReLU). Sometimes the biases are also shown as nodes that feed into the next layer.\n\n\n\n\n\n\nFigure 5: A neural network with a hidden layer. The lines show connections between neurons and their weights \\(w_{i,j}\\)."
  },
  {
    "objectID": "pages/neural_networks.html#universal-approximation-theorem",
    "href": "pages/neural_networks.html#universal-approximation-theorem",
    "title": "3 - Neural Networks",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\nWith a shallow neural network, any continuous function can be modeled with arbitrary accuracy (Universal Approximation Theorem). The following graphic illustrates that as the number of linear functions (and thus piecewise linear regions) increases, the approximation of the underlying function becomes more accurate.\n\n\n\n\n\n\nFigure 6: Approximation of a 1-D function with piecewise linear regions. The more regions, the more accurate the approximation. Source: Prince (2023)\n\n\n\nNeural networks are therefore a particularly powerful class of models!"
  },
  {
    "objectID": "pages/recent_advances.html",
    "href": "pages/recent_advances.html",
    "title": "8 - Foundation Models",
    "section": "",
    "text": "Foundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields."
  },
  {
    "objectID": "pages/recent_advances.html#how-clip-works",
    "href": "pages/recent_advances.html#how-clip-works",
    "title": "8 - Foundation Models",
    "section": "How CLIP Works",
    "text": "How CLIP Works\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs."
  },
  {
    "objectID": "pages/recent_advances.html#applications-of-clip",
    "href": "pages/recent_advances.html#applications-of-clip",
    "title": "8 - Foundation Models",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\nContent Moderation: CLIP can assist in identifying inappropriate content in images based on textual cues."
  },
  {
    "objectID": "pages/recent_advances.html#example",
    "href": "pages/recent_advances.html#example",
    "title": "8 - Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport torch\nimport clip\nfrom PIL import Image\n\n# Load the model and the preprocess function\nmodel, preprocess = clip.load(\"ViT-B/32\")\n\n# Load an image\nimage = preprocess(Image.open(\"path/to/your/image.jpg\")).unsqueeze(0)\n\n# Define a set of labels\nlabels = [\"a dog\", \"a cat\", \"a car\", \"a tree\"]\n\n# Tokenize the labels\ntext = clip.tokenize(labels)\n\n# Compute the image and text features\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\n# Compute the similarity between the image and each label\nsimilarities = (image_features @ text_features.T).softmax(dim=-1)\n\n# Print the most similar label\nprint(\"Label:\", labels[similarities.argmax().item()])"
  },
  {
    "objectID": "pages/recent_advances.html#how-vqa-works",
    "href": "pages/recent_advances.html#how-vqa-works",
    "title": "8 - Foundation Models",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers."
  },
  {
    "objectID": "pages/recent_advances.html#applications-of-vqa",
    "href": "pages/recent_advances.html#applications-of-vqa",
    "title": "8 - Foundation Models",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nEducational Tools: VQA systems can be used in educational applications to assist students in learning by providing answers to questions about visual content.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services."
  },
  {
    "objectID": "pages/recent_advances.html#example-1",
    "href": "pages/recent_advances.html#example-1",
    "title": "8 - Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of a VQA system using a hypothetical multi-modal model:\n\n# Hypothetical code for a Visual Question Answering system\nimport torch\nfrom PIL import Image\nfrom transformers import VQAModel, VQATokenizer\n\n# Load the model and the tokenizer\nmodel = VQAModel.from_pretrained(\"hypothetical-vqa-model\")\ntokenizer = VQATokenizer.from_pretrained(\"hypothetical-vqa-model\")\n\n# Load an image\nimage = Image.open(\"path/to/your/image.jpg\")\n\n# Define a question\nquestion = \"What is in the image?\"\n\n# Preprocess the image and the question\ninputs = tokenizer(image, question, return_tensors=\"pt\")\n\n# Get the model's answer\nwith torch.no_grad():\n    outputs = model(**inputs)\n    answer = outputs.logits.argmax(-1)\n\n# Print the answer\nprint(\"Answer:\", tokenizer.decode(answer))"
  },
  {
    "objectID": "pages/recent_advances.html#conclusion",
    "href": "pages/recent_advances.html#conclusion",
    "title": "8 - Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nFoundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape."
  },
  {
    "objectID": "pages/segmentation.html",
    "href": "pages/segmentation.html",
    "title": "7 - Segmentation",
    "section": "",
    "text": "In image segmentation, individual pixels in the input image are assigned to a known set of classes (semantic segmentation) or objects (instance segmentation). Figure 1 illustrates the differences between image classification, object detection, and segmentation.\n\n\n\n\n\n\nFigure 1: Source: Johnson (2019).\n\n\n\nSemantic segmentation can be viewed as a classification problem where each pixel is individually classified. Thus, semantic segmentation is similar to image classification but more complex. Figure 2 shows an example from a dataset with segmented street scenes, for training models for self-driving cars.\n\n\n\n\n\n\nFigure 2: Top: Photo, bottom: annotated segmentation map. Source: Cordts et al. (2016).\n\n\n\nFigure 3 shows a medical example where a model was trained to segment chest X-rays.\n\n\n\n\n\n\nFigure 3: Source: Novikov et al. (2018).\n\n\n\nInstance segmentation is comparable to object detection but more complex because entire pixel masks must be predicted, defining the spatial extent of individual objects. Figure 4 shows an example.\n\n\n\n\n\n\nFigure 4: Instance segmentation. Source: He et al. (2018).\n\n\n\nWe will now look at methods for semantic segmentation and instance segmentation."
  },
  {
    "objectID": "pages/segmentation.html#sliding-window",
    "href": "pages/segmentation.html#sliding-window",
    "title": "7 - Segmentation",
    "section": "Sliding-Window",
    "text": "Sliding-Window\nOne method for semantic segmentation is to classify each pixel by classifying the pixel in the center using a sliding window approach. The sliding window would provide context information, allowing more accurate classification. Figure 5 illustrates the process.\n\n\n\n\n\n\nFigure 5: Source: Johnson (2019).\n\n\n\nHowever, this approach is very inefficient as a forward pass through the CNN would have to be performed for each pixel, and features extracted from overlapping sliding windows would not be reused."
  },
  {
    "objectID": "pages/segmentation.html#fully-convolutional-networks",
    "href": "pages/segmentation.html#fully-convolutional-networks",
    "title": "7 - Segmentation",
    "section": "Fully Convolutional Networks",
    "text": "Fully Convolutional Networks\nShelhamer, Long, and Darrell (2016) proposed one of the first fully convolutional networks (FCNs). An FCN consists solely of convolutional layers (specifically, it has no fully connected/linear layers) and can thus process images of any spatial dimension and produce a segmentation map of the same dimension. By replacing fully connected/linear layers with convolutional layers, the dependency on a fixed input size can be eliminated.\nFigure 6 illustrates an FCN. The FCN has an output of dimension \\(H \\times W \\times K\\) (height, width, depth), where \\(K\\) is the number of classes. The class-specific activation maps model the probability that a pixel belongs to the corresponding class. With the argmax function, each pixel could then be assigned to the class with the highest probability.\n\n\n\n\n\n\nFigure 6: Source: Johnson (2019).\n\n\n\nThe problem with this approach is that it requires a lot of compute (FLOPs) because the spatial dimensions of the deeper layers still correspond to the input dimension. Therefore, many operations must be performed as the filters must be convolved over a larger area.\nThe first layers in a CNN learn local structures (as the receptive field is very small, they cannot learn anything else), which are successively aggregated in further layers. The number of channels is typically increased to allow the CNN to recognize different variations of patterns, increasing the model’s memory requirements. Additionally, sufficient layers are needed to ensure the receptive field (see ?@sec-cnn-receptive-field) is large enough for accurate segmentation.\nIn image classification, the global label of the image is modeled. Thus, this problem does not exist in image classification, as the spatial dimension of the activation maps can be gradually reduced, keeping the compute approximately constant across the network.\nShelhamer, Long, and Darrell (2016) solved the problem by gradually down-sampling the activation maps using convolutions with stride &gt;2 or pooling layers (just like in image classification architectures) but then up-sampling the activation maps from various layers using an up-sampling method (see Section 3). They concatenate information from various layers to obtain activation maps containing rich features with local and global context. These are then reduced to the desired number of classes with \\(1 \\times 1\\) convolutions as needed. See Figure 7 for an illustration.\n\n\n\n\n\n\nFigure 7: Source: Tai et al. (2017). Architecture as applied in the FCN paper Shelhamer, Long, and Darrell (2016).\n\n\n\nBy using skip connections, which directly connect activation maps in the middle of the architecture with deeper layers, the segmentation map results were significantly improved. Figure 8 shows examples.\n\n\n\n\n\n\nFigure 8: From left to right, showing the results of models with skip connections to increasingly earlier layers. The far right is the ground truth. Source: Shelhamer, Long, and Darrell (2016)."
  },
  {
    "objectID": "pages/segmentation.html#encoder-decoder-networks",
    "href": "pages/segmentation.html#encoder-decoder-networks",
    "title": "7 - Segmentation",
    "section": "Encoder-Decoder Networks",
    "text": "Encoder-Decoder Networks\nWith the encoder-decoder architecture, the input (the image) is gradually reduced spatially (encoded) until a dense representation (encoding) is obtained. This encoding is then gradually expanded spatially with a decoder until the original dimension is reached. Figure 9 illustrates the process. This architecture is very compute-efficient and, due to the symmetry of the encoder and decoder, produces segmentation maps that correspond to the input resolution.\n\n\n\n\n\n\nFigure 9: Source: Johnson (2019).\n\n\n\nAn extreme compression (encoding) was applied, for example, by Noh, Hong, and Han (2015), see Figure 10. This makes the model significantly more efficient as the activation maps are relatively small.\n\n\n\n\n\n\nFigure 10: Source: Noh, Hong, and Han (2015)."
  },
  {
    "objectID": "pages/segmentation.html#unet",
    "href": "pages/segmentation.html#unet",
    "title": "7 - Segmentation",
    "section": "UNet",
    "text": "UNet\nA well-known architecture is U-Net Ronneberger, Fischer, and Brox (2015). It has been successfully used to segment images in medicine/biology. U-Net inspired architectures are also used in numerous other applications (e.g., image generation Rombach et al. (2022)). Figure 19 shows examples of such segmentation.\n\n\n\n\n\n\nFigure 19: Source: Ronneberger, Fischer, and Brox (2015).\n\n\n\nThe unique aspect of U-Net is that it uses an encoder/decoder architecture while simultaneously employing shortcut/skip connections to connect various layers directly. Figure 20 shows the U-Net architecture (U-shaped, hence the name), including the copy and crop operations that connect the layers. These connections directly copy detailed low-level information to the output without passing through the bottleneck in the encoder, where there may not be enough capacity to preserve it. The bottleneck encodes global information relevant to all positions, making the segmentation more accurate in detail.\n\n\n\n\n\n\nFigure 20: Source: Ronneberger, Fischer, and Brox (2015).\n\n\n\nAdditionally, when training the models, the individual pixels were weighted differently. The closer a pixel is to the edge of an object, the higher its loss was weighted. This allows U-Net to learn especially sharp separations between objects, which can be important in medicine when, for example, segmenting cells that are very close to each other."
  },
  {
    "objectID": "pages/segmentation.html#loss",
    "href": "pages/segmentation.html#loss",
    "title": "7 - Segmentation",
    "section": "Loss",
    "text": "Loss\nSince semantic segmentation essentially performs classification at the pixel level, the same loss function used in image classification can be applied at the pixel level. Figure 21 shows that the softmax function is applied individually to all pixel positions to obtain probability distributions per pixel.\n\n\n\n\n\n\nFigure 21: Pixel-level softmax for a single pixel illustrated. Output is \\(H \\times W \\times K\\).\n\n\n\nOften, per-pixel cross-entropy is used as the loss function, where \\(N\\) refers to the total number of pixels:\n\\[\\begin{align}\nCE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{align}\\]"
  },
  {
    "objectID": "pages/segmentation.html#mask-r-cnn",
    "href": "pages/segmentation.html#mask-r-cnn",
    "title": "7 - Segmentation",
    "section": "Mask R-CNN",
    "text": "Mask R-CNN\nOne of the most well-known models is an extension of Faster R-CNN: Mask R-CNN. Figure 22 illustrates the additional output head responsible for mask prediction.\n\n\n\n\n\n\nFigure 22: Source: He et al. (2018).\n\n\n\nMask R-CNN models the masks with an output size of \\(NxNxK\\), where \\(NxN\\) is the spatial dimension of the RoI pooling of the individual objects. \\(K\\) is the number of classes. Masks are always generated for all classes. When training the models, the mask of the ground truth class \\(k\\) is evaluated, and the binary pixel-wise cross-entropy loss is calculated accordingly.\n\\[\\begin{align}\n\\text{binary CE} = - \\sum_{i=1}^{N^2}  \\Big( (\\log \\hat{y}_k^{(i)})^{y_k^{(i)}} + (\\log (1-\\hat{y}_k^{(i)}))^{(1 - y_k^{(i)})} \\Big)\n\\end{align}\\]\nMask R-CNN also uses an improved version of RoI pooling, called RoI align, to more precisely align the masks with the object in the input (since the spatial resolution of the RoI is much smaller than the input object).\n\n\n\n\n\n\nFigure 23: Source: He et al. (2018).\n\n\n\nFigure 24 shows examples of training data. Note that the ground truth masks are each cropped relative to the predicted bounding box.\n\n\n\n\n\n\nFigure 24: Source: Johnson (2019).\n\n\n\nMask R-CNN works remarkably well, as results from He et al. (2018) show, see Figure 25.\n\n\n\n\n\n\nFigure 25: Source: He et al. (2018)."
  },
  {
    "objectID": "pages/segmentation.html#pixel-accuracy-pa",
    "href": "pages/segmentation.html#pixel-accuracy-pa",
    "title": "7 - Segmentation",
    "section": "Pixel Accuracy (PA)",
    "text": "Pixel Accuracy (PA)\nPixel accuracy is the ratio of correctly classified pixels to the total number of pixels. For \\(K + 1\\) classes (including the background class),\npixel accuracy is defined as:\n\\[\\begin{equation}\n\\text{PA} = \\frac{\\sum_{i=0}^Kp_{ii}}{\\sum_{i=0}^K\\sum_{j=0}^K p_{ij}}\n\\end{equation}\\]\nwhere \\(p_{ij}\\) is the number of pixels of class \\(i\\) predicted as class \\(j\\)."
  },
  {
    "objectID": "pages/segmentation.html#mean-pixel-accuracy-mpa",
    "href": "pages/segmentation.html#mean-pixel-accuracy-mpa",
    "title": "7 - Segmentation",
    "section": "Mean Pixel Accuracy (MPA)",
    "text": "Mean Pixel Accuracy (MPA)\nMean pixel accuracy is an extension of pixel accuracy. The ratio of correct pixels to all pixels is calculated for each class and then averaged over the number of classes.\n\\[\\begin{equation}\n\\text{MPA} = \\frac{1}{K+1} \\sum_{i=0}^K \\frac{p_{ii}}{\\sum_{j=0}^K p_{ij}}\n\\end{equation}\\]"
  },
  {
    "objectID": "pages/segmentation.html#intersection-over-union-iou",
    "href": "pages/segmentation.html#intersection-over-union-iou",
    "title": "7 - Segmentation",
    "section": "Intersection over Union (IoU)",
    "text": "Intersection over Union (IoU)\nThis metric is often used in semantic segmentation. It is the area of the intersection of the prediction and ground truth, divided by the union of the prediction and ground truth.\n\\[\\begin{equation}\n\\text{IoU} = \\frac{\\lvert A \\cap B \\rvert}{\\lvert A \\cup B \\rvert}\n\\end{equation}\\]"
  },
  {
    "objectID": "pages/segmentation.html#mean-intersection-over-union-m-iou",
    "href": "pages/segmentation.html#mean-intersection-over-union-m-iou",
    "title": "7 - Segmentation",
    "section": "Mean Intersection over Union (M-IoU)",
    "text": "Mean Intersection over Union (M-IoU)\nM-IoU is the average IoU over all classes."
  },
  {
    "objectID": "pages/segmentation.html#precision-recall-f1",
    "href": "pages/segmentation.html#precision-recall-f1",
    "title": "7 - Segmentation",
    "section": "Precision / Recall / F1",
    "text": "Precision / Recall / F1\nPrecision is the proportion of samples classified as positive that are actually positive:\n\\(\\text{Precision} = \\frac{TP}{TP + FP}\\)\nRecall is the proportion of positive samples that are correctly identified:\n\\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\nF1 is the harmonic mean of precision and recall:\n\\(\\text{F1} = \\frac{2 \\text{Precision Recall}}{\\text{Precision} + \\text{Recall}}\\)"
  },
  {
    "objectID": "pages/segmentation.html#dice-coefficient",
    "href": "pages/segmentation.html#dice-coefficient",
    "title": "7 - Segmentation",
    "section": "Dice Coefficient",
    "text": "Dice Coefficient\nThe Dice coefficient is twice the intersection of the prediction and ground truth, divided by the total number of pixels. The Dice coefficient is thus similar to the IoU.\n\\[\\begin{equation}\n\\text{Dice} = \\frac{2 \\lvert A \\cap B \\rvert}{\\lvert A \\rvert + \\lvert  B \\rvert}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides_cas/intro.html#species-identification",
    "href": "slides_cas/intro.html#species-identification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Species Identification",
    "text": "Species Identification\n\n\nSource: Breitenmoser-Würsten et al. (2024)",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#object-identification-and-translation",
    "href": "slides_cas/intro.html#object-identification-and-translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Object Identification and Translation",
    "text": "Object Identification and Translation\n\n\n\n\n\n\n\n\n\n\n\n(a) Identification & Search\n\n\n\n\n\n\n\n\n\n\n\n(b) Translation\n\n\n\n\n\n\n\nFigure 2: Google Lens",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#self-driving",
    "href": "slides_cas/intro.html#self-driving",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Self-Driving",
    "text": "Self-Driving\n\n\n\n\n\n\nFigure 3: Example from Waymo.",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#biometric-id",
    "href": "slides_cas/intro.html#biometric-id",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Biometric ID",
    "text": "Biometric ID\n\nExample from Apple Face ID",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#precision-agriculture",
    "href": "slides_cas/intro.html#precision-agriculture",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Precision Agriculture",
    "text": "Precision Agriculture\n\n\nExample from Häni, Roy, and Isler (2020)",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#medical-segmentation",
    "href": "slides_cas/intro.html#medical-segmentation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Medical Segmentation",
    "text": "Medical Segmentation\n\nExample from Ma et al. (2024).",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#photo-enhancement",
    "href": "slides_cas/intro.html#photo-enhancement",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Photo Enhancement",
    "text": "Photo Enhancement\n\nExample from Google Magic Editor",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#ai-chips",
    "href": "slides_cas/intro.html#ai-chips",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "AI Chips",
    "text": "AI Chips\n\nFrom Link.",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-classification",
    "href": "slides_cas/intro.html#image-classification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#object-detection",
    "href": "slides_cas/intro.html#object-detection",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Object Detection",
    "text": "Object Detection\n\nObject Detection Beispiel (aus Redmon et al. (2016)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist.",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#segmentation",
    "href": "slides_cas/intro.html#segmentation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentation",
    "text": "Segmentation\n\n\n\n\n\nObject Segmentation Beispiel (aus He et al. (2018)).",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---manipulation",
    "href": "slides_cas/intro.html#image-generation---manipulation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Manipulation",
    "text": "Image Generation - Manipulation\n\nSource: Link, DragGAN by Pan et al. (2023)",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---translation",
    "href": "slides_cas/intro.html#image-generation---translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Translation",
    "text": "Image Generation - Translation\n\nImage Generation Beispiel (aus Isola et al. (2018)).",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---super-resolution",
    "href": "slides_cas/intro.html#image-generation---super-resolution",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Super Resolution",
    "text": "Image Generation - Super Resolution\n\nNvidia dlss: Link",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#image-generation---colorization",
    "href": "slides_cas/intro.html#image-generation---colorization",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Colorization",
    "text": "Image Generation - Colorization\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#many-tasks",
    "href": "slides_cas/intro.html#many-tasks",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Many tasks",
    "text": "Many tasks\n\n\n\nImage Classification\nObject Detection (and Tracking)\nImage Segmentation\n\nSemantic Segmentation\nInstance Segmentation\n\nOptical Character Recognition (OCR)\nPose Estimation\nFacial Recognition\nAction Recognition\n\n\n\nImage Generation\n\nStyle Transfer\nImage Inpainting\nSuper-Resolution\nText-to-Image (and more)\n\nImage Captioning\n3D Reconstruction\nImage Retrieval\n\n\nList is not exhaustive!",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#semantic-gap",
    "href": "slides_cas/intro.html#semantic-gap",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\nIllustration des semantic gap.",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#point-of-view",
    "href": "slides_cas/intro.html#point-of-view",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Point of View",
    "text": "Point of View\n\nSource",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#deformation",
    "href": "slides_cas/intro.html#deformation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Deformation",
    "text": "Deformation\n\nSource",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#lighting",
    "href": "slides_cas/intro.html#lighting",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Lighting",
    "text": "Lighting\n\nSource",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#background",
    "href": "slides_cas/intro.html#background",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Background",
    "text": "Background\n\nSource",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#occlusion",
    "href": "slides_cas/intro.html#occlusion",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Occlusion",
    "text": "Occlusion\n\nSource",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#intraclass-variation",
    "href": "slides_cas/intro.html#intraclass-variation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Intraclass Variation",
    "text": "Intraclass Variation\n\nSource",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#context-relevance",
    "href": "slides_cas/intro.html#context-relevance",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Context Relevance",
    "text": "Context Relevance\n\n\n\n\n\n\n\n\n\n\nKontext Source",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#machine-learning-approach",
    "href": "slides_cas/intro.html#machine-learning-approach",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Approach",
    "text": "Machine Learning Approach\nWith Machine Learning, we follow a data-driven approach to solve various tasks:\n\nCollect a dataset of images and their labels.\nUse a machine learning algorithm to train a model (e.g., a classifier).\nEvaluate and apply the model to new data.\n\n\ndef train(images, labels):\n    \"\"\" Train a Model \"\"\"\n    # Fit Model here\n    return model\n\ndef predict(test_images, model):\n    \"\"\" Predict \"\"\"\n    predictions = model(test_images)\n    return predictions",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#question",
    "href": "slides_cas/intro.html#question",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Question",
    "text": "Question\nImage Super Resolution\nHow would you train a model for image super resolution?\nThe task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#machine-learning-pipeline",
    "href": "slides_cas/intro.html#machine-learning-pipeline",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Pipeline",
    "text": "Machine Learning Pipeline",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides_cas/intro.html#pytorch",
    "href": "slides_cas/intro.html#pytorch",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "PyTorch",
    "text": "PyTorch\nIn this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources.\nFundamental Concepts:\n\nTensor: N-dimensional array, like numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks\n\nLet’s check it out! (on images)",
    "crumbs": [
      "Slides CAS",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/neural_networks.html#neuronen",
    "href": "slides/neural_networks.html#neuronen",
    "title": "Neural Networks",
    "section": "Neuronen",
    "text": "Neuronen\n\nSchematische Darstellung von verbundenen Neuronen. Source: Phillips (2015)"
  },
  {
    "objectID": "slides/neural_networks.html#visueller-cortex",
    "href": "slides/neural_networks.html#visueller-cortex",
    "title": "Neural Networks",
    "section": "Visueller Cortex",
    "text": "Visueller Cortex\n\nRepresentation von Transformationen im visuellen Cortex. Source: Kubilius (2017)"
  },
  {
    "objectID": "slides/neural_networks.html#multilayer-perceptron",
    "href": "slides/neural_networks.html#multilayer-perceptron",
    "title": "Neural Networks",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\n\nEin neuronales Netzwerk mit zwei Hidden Layer. Die Linien zeigen Verbindungen zwischen den Neuronen. Source: Li (2022)"
  },
  {
    "objectID": "slides/neural_networks.html#lineares-modell",
    "href": "slides/neural_networks.html#lineares-modell",
    "title": "Neural Networks",
    "section": "Lineares Modell",
    "text": "Lineares Modell\nEin lineares Modell hat folgende Form:\n\\[\\begin{equation}\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W} \\mathbf{x}^{(i)}  +  \\mathbf{b}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/neural_networks.html#neuronales-netzwerk",
    "href": "slides/neural_networks.html#neuronales-netzwerk",
    "title": "Neural Networks",
    "section": "Neuronales Netzwerk",
    "text": "Neuronales Netzwerk\n\\[\\begin{equation*}\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W}^{(2)} g\\big(\\mathbf{W}^{(1)} \\mathbf{x}^{(i)}  +  \\mathbf{b}^{(1)} \\big)  +  \\mathbf{b}^{(2)}\n\\end{equation*}\\]"
  },
  {
    "objectID": "slides/neural_networks.html#activation-function",
    "href": "slides/neural_networks.html#activation-function",
    "title": "Neural Networks",
    "section": "Activation Function",
    "text": "Activation Function\n\\[\\begin{equation}\n\\text{ReLU}(x) = \\begin{cases}\nx, & \\text{if } x \\geq 0 \\\\\n0, & \\text{if } x &lt; 0\n\\end{cases}\n\\end{equation}\\]\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.neural_network import MLPClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\n\ndef plot_non_linear_vs_linear():\n    fig, ax = plt.subplots(figsize=(12, 6), ncols=2)\n\n    X, y = make_blobs(\n        n_samples=100,\n        n_features=2,\n        random_state=0,\n        cluster_std=0.5,\n        centers=[(-1, -1), (1, 1), (-1, 1), (1, -1)],\n    )\n    y = np.where(y &lt; 2, 1, 0)\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=[10],\n        activation=\"identity\",\n        max_iter=200,\n        random_state=123,\n        learning_rate_init=1.0,\n    ).fit(X, y)\n    _ = plot_decision_regions(X, y, clf, ax=ax[0])\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=[10],\n        activation=\"relu\",\n        max_iter=400,\n        random_state=123,\n        learning_rate_init=0.1,\n    ).fit(X, y)\n    _ = plot_decision_regions(X, y, clf, ax=ax[1])\n\n    _ = ax[0].set_title(\"Linear: $g(x) = x$\")\n    _ = ax[1].set_title(\"Non-Linear: $g(x) = ReLU(x)$\")\n\n    return fig\n\nfig = plot_non_linear_vs_linear()\nfig.show()\n\n\n\n\n\n\n\nFigure 1: Linear (left) vs non-linear (right) activation function."
  },
  {
    "objectID": "pages/intro.html",
    "href": "pages/intro.html",
    "title": "1 - Introduction",
    "section": "",
    "text": "Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.\nThe integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services. Examples in the field of image processing with Deep Learning are shown in Figure 1, Figure 2, and Figure 3.\n\n\n\n\n\n\nFigure 1: Example from Link. Left is the original image, right is the version enhanced with Deep Learning.\n\n\n\n\n\n\n\n\n\nFigure 2: Example from Link. Left is the original image, right is the manipulated version.\n\n\n\n\n\n\n\n\n\nFigure 3: Example from Link. Left is the original image, right is the manipulated version.\n\n\n\n\n\n\n\n\n\nQuestion\nWhat steps do you think the model in Figure 3 performs?\n\n\n\nThe increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. Figure 4 shows that special chips have been developed to process images with Deep Learning models quickly.\n\n\n\n\n\n\nFigure 4: From Link.\n\n\n\n\n\nWe will now explore some challenges that must be overcome when analyzing images with machine learning models.\n\n\nThe semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.\n\n\n\n\n\n\nFigure 5: Illustration of the semantic gap.\n\n\n\n\n\n\nThe meaning of the image does not change with the viewpoint, but the pixels do.\n\n\n\n\n\n\nFigure 6: Source\n\n\n\n\n\n\nObjects are often flexible and appear in different shapes and poses.\n\n\n\n\n\n\nFigure 7: Source\n\n\n\n\n\n\nChanges in illumination affect pixel values and the visibility of objects.\n\n\n\n\n\n\nFigure 8: Source\n\n\n\n\n\n\nBackground pixels can resemble objects and make their exact delineation or visibility more difficult.\n\n\n\n\n\n\nFigure 9: Source\n\n\n\n\n\n\nObjects are not always fully visible, which can make their detection more difficult.\n\n\n\n\n\n\nFigure 10: Source\n\n\n\n\n\n\nObjects of the same class can exhibit large intra-class variability.\n\n\n\n\n\n\nFigure 11: Source\n\n\n\n\n\n\nFigure 12 shows that context information can be important to correctly classify an object.\n\n\n\n\n\n\nFigure 12: Context Source\n\n\n\n\n\n\n\nThere are various problem statements in image processing that can be modeled with Deep Learning. Below, we will see some of them.\n\n\nIn image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. Figure 13 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012) (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.\n\n\n\n\n\n\nFigure 13: Multi-Class Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\n\n\n\nObject detection involves locating and recognizing (multiple) objects in an image. Figure 14 shows an example from the paper by Redmon et al. (2016). Each object is localized with a bounding box and assigned an object class.\n\n\n\n\n\n\nFigure 14: Object Detection example (from Redmon et al. (2016)). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.\n\n\n\n\n\n\nIn segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). Figure 15 shows an example of object segmentation (instance segmentation) from the paper by He et al. (2018), where individual objects are detected and precisely localized (segmented) at the pixel level.\n\n\n\n\n\n\nFigure 15: Object Segmentation example (from He et al. (2018)).\n\n\n\nThe following video shows an example of semantic segmentation:\n\n\n\n\n\nIn keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. Figure 16 shows an example of keypoint detection from the paper by He et al. (2018), where\nindividual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).\n\n\n\n\n\n\nFigure 16: Keypoint Detection example (from He et al. (2018)).\n\n\n\n\n\n\nThere are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below are some examples.\n\n\nFigure 17 shows an example of image generation from the paper by Isola et al. (2018), where images are generated conditioned on image inputs (translated).\n\n\n\n\n\n\nFigure 17: Image Generation example (from Isola et al. (2018)).\n\n\n\n\n\n\nIn the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in Figure 18. This allows for higher frame rates.\n\n\n\n\n\n\nFigure 18: Nvidia DLSS: Link\n\n\n\n\n\n\nImage colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. Figure 19 shows an example.\n\n\n\n\n\n\nFigure 19: Norwegian Bride (est late 1890s) from DeOldify: Link\n\n\n\n\n\n\nIn view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. Figure 20 shows the data on which such a model is trained and what can be generated with it.\n\n\n\n\n\n\nFigure 20: Neural Radiance Fields - example (from Mildenhall et al. (2020)).\n\n\n\n\n\n\nIn unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model’s output. However, you can often make changes to generated images or interpolate between data points. Figure 21 shows generated images from a model trained on portrait images of people.\n\n\n\n\n\n\nFigure 21: StyleGan3 (from Karras et al. (2021)).\n\n\n\nFigure 22 shows how generated images can be further adjusted with manually set reference points.\n\n\n\n\n\n\nFigure 22: DragGAN (from Pan et al. (2023)).\n\n\n\n\n\n\nFigure 23 shows an example of image generation from the paper by Rombach et al. (2022), where images are generated conditioned on text inputs.\n\n\n\n\n\n\nFigure 23: Image Generation example (from Rombach et al. (2022).)\n\n\n\nOn civitai, there are numerous examples and models to admire or download.\n\n\n\n\nVarious models can also be combined into pipelines. One example is Grounded-Segment-Anything, shown in Figure 24. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.\n\n\n\n\n\n\nFigure 24: Example from Grounded-Segment-Anything Link\n\n\n\n\n\n\nThere are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. Figure 25 shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.\n\n\n\n\n\n\nFigure 25: Example from Facebook’s Detectron Library Link\n\n\n\nHugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification.\n\n\n\n\nWe follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:\n\nCollecting a dataset of images and their labels.\nUsing a machine learning algorithm to train a model that learns to associate images with labels.\nEvaluating/applying the model on new data.\n\n\ndef train(images, labels):\n \"\"\" Train a Model \"\"\"\n # Fit Model here\n return model\n\ndef predict(test_images, model):\n \"\"\" Predict \"\"\"\n predictions = model(test_images)\n return predictions\n\n\n\n\n\n\n\nQuestion\nHow would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.\n\n\n\n\n\nWhen modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. Figure 26 illustrates this process graphically.\n\n\n\n\n\n\nFigure 26: Machine Learning Pipeline (Source: Raschka and Mirjalili (2020))\n\n\n\nAt the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.\n\n\n\nA model is typically described as a function of a data point, generating an output \\(\\hat{y}\\):\n\\[\\begin{align*}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{align*}\\]\nMost models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by \\(\\theta\\).\n\\[\\begin{align*}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{align*}\\]\nFor simplicity, we often omit \\(\\theta\\): \\(f(\\mathbf{x}^{(i)})\\)\n\n\n\nThe coefficients are fitted to a training dataset through an optimization procedure.\nThe optimization procedure can often be influenced by additional factors, called hyperparameters (\\(\\alpha, \\lambda, \\dots\\)). These cannot be directly optimized.\nThe function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use \\(J(\\cdot)\\) to denote the cost function. Often, the cost function is also referred to as the loss function \\(L(\\cdot)\\). We use \\(l(\\cdot)\\) for the per-sample loss, i.e., the computation of the cost function on a single sample.\nOur goal is to find a model (and its parameters) that minimizes the cost function:\n\\[\\begin{equation*}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation*}\\]\nUsually, preprocessing of variables precedes the learning of\nthe coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.\n\n\n\nModel selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.\nIf we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see Figure 27) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.\n\n\n\n\n\n\nFigure 27: Train-Test Split to select and evaluate models.\n\n\n\n\n\n\nImages typically have very high dimensionality. For example, an RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\). Classical machine learning algorithms often struggle with such high dimensionalities:\n\nThey are very slow or require a lot of memory.\nThey cannot exploit the 2-D structure of images.\nThey are very sensitive to slight changes in images (e.g., rotations).\nThey can easily overfit, as the number of features is close to the number of observations (training set).\n\nWhen modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. Figure 28, Figure 29, and Figure 30 show various feature extraction methods.\nFigure 28 shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.\n\n\n\n\n\n\nFigure 28: Color Histograms as Features (Source: Johnson (2022))\n\n\n\nFigure 29 shows that techniques like Histogram of Oriented Gradients (HOG) Dalal and Triggs (2005) can be used to extract structures from images. Such features were successfully used for pedestrian detection Dalal and Triggs (2005).\n\n\n\n\n\n\nFigure 29: HOG as Features (Source: Johnson (2022))\n\n\n\nFigure 30 shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.\n\n\n\n\n\n\nFigure 30: Bag of (visual) words Features (Source: Johnson (2022))\n\n\n\nFinally, all features can be combined, often more is better, as shown in Figure 31.\n\n\n\n\n\n\nFigure 31: Image Features (Source: Johnson (2022))\n\n\n\nDepending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from Figure 31 can still be reduced in dimensionality, e.g., with Principal Component Analysis.\nFigure 32 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.\n\n\n\n\n\n\nFigure 32: CIFAR10 Dataset Source\n\n\n\n\n\n\n\nTo accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced Everingham et al. (2007). These involved various tasks, such as detecting objects in photographs (Figure 33).\n\n\n\n\n\n\nFigure 33: Images/illustrations from Link and Johnson (2022). On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.\n\n\n\nThe easy availability of images on the internet has made it possible to collect increasingly larger datasets. ImageNet is such a very large, hierarchically annotated image dataset Deng et al. (2009) with over 1.4 million images, categorized into 1,000 object classes. Figure 34 illustrates the dataset.\n\n\n\n\n\n\nFigure 34: ImageNet, Image Source, details in Deng et al. (2009)\n\n\n\nSince 2010, challenges have been regularly conducted on the ImageNet dataset Russakovsky et al. (2015), such as image classification and object detection. Figure 35 shows the development of the error rate over time.\n\n\n\n\n\n\nFigure 35: Source: Johnson (2022)\n\n\n\nIn 2011, a team won Perronnin et al. (2010) by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.\nIn 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. Krizhevsky, Sutskever, and Hinton (2012) impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in Figure 36.\n\n\n\n\n\n\nFigure 36: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nWhile classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms Figure 37, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning Figure 38.\n\n\n\n\n\n\nFigure 37: Illustration from Johnson (2022)\n\n\n\n\n\n\n\n\n\nFigure 38: Illustration from Johnson (2022)\n\n\n\nDeep learning-based approaches have several advantages over classical machine learning methods:\n\nAutomatic feature extraction: no manual feature extraction procedures are needed.\nHierarchical features: these are particularly valuable for processing and understanding visual data.\nGeneralization: with more training data, deep learning methods generalize better.\nEnd-to-end learning: this approach allows many problems to be modeled similarly.\nRobustness to variability: certain models are naturally invariant to\n\ntransformations like translations, scalings, etc. - Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.\n\n\nWe will now explore the most important milestones in deep learning for image analysis.\n\n\n\nHubel and Wiesel (1959) showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.\n\n\n\n\n\n\nFigure 39: Illustration Source\n\n\n\n\n\n\nFukushima (1980) defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 40: The Neocognitron Fukushima (1980).\n\n\n\n\n\n\nRumelhart, Hinton, and Williams (1986) introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.\n\n\n\n\n\n\nFigure 41: Backpropagation in neural networks Rumelhart, Hinton, and Williams (1986).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good video on backpropagation: 3Blue1Brown Backpropagation Calculus\n\n\n\n\n\nLecun et al. (1998) implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in Figure 42.\n\n\n\n\n\n\nFigure 42: Modern CNN Lecun et al. (1998).\n\n\n\nCNNs became extremely popular after winning the ImageNet competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in Figure 43.\n\n\n\n\n\n\nFigure 43: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the COCO Challenges, with various tasks.\n\n\n\nCNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models Dosovitskiy et al. (2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures Liu et al. (2021). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures Woo et al. (2023). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily Smith et al. (2023)."
  },
  {
    "objectID": "pages/intro.html#challenges",
    "href": "pages/intro.html#challenges",
    "title": "1 - Introduction",
    "section": "",
    "text": "We will now explore some challenges that must be overcome when analyzing images with machine learning models.\n\n\nThe semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.\n\n\n\n\n\n\nFigure 5: Illustration of the semantic gap.\n\n\n\n\n\n\nThe meaning of the image does not change with the viewpoint, but the pixels do.\n\n\n\n\n\n\nFigure 6: Source\n\n\n\n\n\n\nObjects are often flexible and appear in different shapes and poses.\n\n\n\n\n\n\nFigure 7: Source\n\n\n\n\n\n\nChanges in illumination affect pixel values and the visibility of objects.\n\n\n\n\n\n\nFigure 8: Source\n\n\n\n\n\n\nBackground pixels can resemble objects and make their exact delineation or visibility more difficult.\n\n\n\n\n\n\nFigure 9: Source\n\n\n\n\n\n\nObjects are not always fully visible, which can make their detection more difficult.\n\n\n\n\n\n\nFigure 10: Source\n\n\n\n\n\n\nObjects of the same class can exhibit large intra-class variability.\n\n\n\n\n\n\nFigure 11: Source\n\n\n\n\n\n\nFigure 12 shows that context information can be important to correctly classify an object.\n\n\n\n\n\n\nFigure 12: Context Source"
  },
  {
    "objectID": "pages/intro.html#problem-statements",
    "href": "pages/intro.html#problem-statements",
    "title": "1 - Introduction",
    "section": "",
    "text": "There are various problem statements in image processing that can be modeled with Deep Learning. Below, we will see some of them.\n\n\nIn image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. Figure 13 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012) (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.\n\n\n\n\n\n\nFigure 13: Multi-Class Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\n\n\n\nObject detection involves locating and recognizing (multiple) objects in an image. Figure 14 shows an example from the paper by Redmon et al. (2016). Each object is localized with a bounding box and assigned an object class.\n\n\n\n\n\n\nFigure 14: Object Detection example (from Redmon et al. (2016)). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.\n\n\n\n\n\n\nIn segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). Figure 15 shows an example of object segmentation (instance segmentation) from the paper by He et al. (2018), where individual objects are detected and precisely localized (segmented) at the pixel level.\n\n\n\n\n\n\nFigure 15: Object Segmentation example (from He et al. (2018)).\n\n\n\nThe following video shows an example of semantic segmentation:\n\n\n\n\n\nIn keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. Figure 16 shows an example of keypoint detection from the paper by He et al. (2018), where\nindividual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).\n\n\n\n\n\n\nFigure 16: Keypoint Detection example (from He et al. (2018)).\n\n\n\n\n\n\nThere are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below are some examples.\n\n\nFigure 17 shows an example of image generation from the paper by Isola et al. (2018), where images are generated conditioned on image inputs (translated).\n\n\n\n\n\n\nFigure 17: Image Generation example (from Isola et al. (2018)).\n\n\n\n\n\n\nIn the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in Figure 18. This allows for higher frame rates.\n\n\n\n\n\n\nFigure 18: Nvidia DLSS: Link\n\n\n\n\n\n\nImage colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. Figure 19 shows an example.\n\n\n\n\n\n\nFigure 19: Norwegian Bride (est late 1890s) from DeOldify: Link\n\n\n\n\n\n\nIn view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. Figure 20 shows the data on which such a model is trained and what can be generated with it.\n\n\n\n\n\n\nFigure 20: Neural Radiance Fields - example (from Mildenhall et al. (2020)).\n\n\n\n\n\n\nIn unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model’s output. However, you can often make changes to generated images or interpolate between data points. Figure 21 shows generated images from a model trained on portrait images of people.\n\n\n\n\n\n\nFigure 21: StyleGan3 (from Karras et al. (2021)).\n\n\n\nFigure 22 shows how generated images can be further adjusted with manually set reference points.\n\n\n\n\n\n\nFigure 22: DragGAN (from Pan et al. (2023)).\n\n\n\n\n\n\nFigure 23 shows an example of image generation from the paper by Rombach et al. (2022), where images are generated conditioned on text inputs.\n\n\n\n\n\n\nFigure 23: Image Generation example (from Rombach et al. (2022).)\n\n\n\nOn civitai, there are numerous examples and models to admire or download.\n\n\n\n\nVarious models can also be combined into pipelines. One example is Grounded-Segment-Anything, shown in Figure 24. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.\n\n\n\n\n\n\nFigure 24: Example from Grounded-Segment-Anything Link\n\n\n\n\n\n\nThere are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. Figure 25 shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.\n\n\n\n\n\n\nFigure 25: Example from Facebook’s Detectron Library Link\n\n\n\nHugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification."
  },
  {
    "objectID": "pages/intro.html#machine-learning",
    "href": "pages/intro.html#machine-learning",
    "title": "1 - Introduction",
    "section": "",
    "text": "We follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:\n\nCollecting a dataset of images and their labels.\nUsing a machine learning algorithm to train a model that learns to associate images with labels.\nEvaluating/applying the model on new data.\n\n\ndef train(images, labels):\n \"\"\" Train a Model \"\"\"\n # Fit Model here\n return model\n\ndef predict(test_images, model):\n \"\"\" Predict \"\"\"\n predictions = model(test_images)\n return predictions\n\n\n\n\n\n\n\nQuestion\nHow would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.\n\n\n\n\n\nWhen modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. Figure 26 illustrates this process graphically.\n\n\n\n\n\n\nFigure 26: Machine Learning Pipeline (Source: Raschka and Mirjalili (2020))\n\n\n\nAt the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.\n\n\n\nA model is typically described as a function of a data point, generating an output \\(\\hat{y}\\):\n\\[\\begin{align*}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{align*}\\]\nMost models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by \\(\\theta\\).\n\\[\\begin{align*}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{align*}\\]\nFor simplicity, we often omit \\(\\theta\\): \\(f(\\mathbf{x}^{(i)})\\)\n\n\n\nThe coefficients are fitted to a training dataset through an optimization procedure.\nThe optimization procedure can often be influenced by additional factors, called hyperparameters (\\(\\alpha, \\lambda, \\dots\\)). These cannot be directly optimized.\nThe function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use \\(J(\\cdot)\\) to denote the cost function. Often, the cost function is also referred to as the loss function \\(L(\\cdot)\\). We use \\(l(\\cdot)\\) for the per-sample loss, i.e., the computation of the cost function on a single sample.\nOur goal is to find a model (and its parameters) that minimizes the cost function:\n\\[\\begin{equation*}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation*}\\]\nUsually, preprocessing of variables precedes the learning of\nthe coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.\n\n\n\nModel selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.\nIf we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see Figure 27) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.\n\n\n\n\n\n\nFigure 27: Train-Test Split to select and evaluate models.\n\n\n\n\n\n\nImages typically have very high dimensionality. For example, an RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\). Classical machine learning algorithms often struggle with such high dimensionalities:\n\nThey are very slow or require a lot of memory.\nThey cannot exploit the 2-D structure of images.\nThey are very sensitive to slight changes in images (e.g., rotations).\nThey can easily overfit, as the number of features is close to the number of observations (training set).\n\nWhen modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. Figure 28, Figure 29, and Figure 30 show various feature extraction methods.\nFigure 28 shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.\n\n\n\n\n\n\nFigure 28: Color Histograms as Features (Source: Johnson (2022))\n\n\n\nFigure 29 shows that techniques like Histogram of Oriented Gradients (HOG) Dalal and Triggs (2005) can be used to extract structures from images. Such features were successfully used for pedestrian detection Dalal and Triggs (2005).\n\n\n\n\n\n\nFigure 29: HOG as Features (Source: Johnson (2022))\n\n\n\nFigure 30 shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.\n\n\n\n\n\n\nFigure 30: Bag of (visual) words Features (Source: Johnson (2022))\n\n\n\nFinally, all features can be combined, often more is better, as shown in Figure 31.\n\n\n\n\n\n\nFigure 31: Image Features (Source: Johnson (2022))\n\n\n\nDepending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from Figure 31 can still be reduced in dimensionality, e.g., with Principal Component Analysis.\nFigure 32 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.\n\n\n\n\n\n\nFigure 32: CIFAR10 Dataset Source"
  },
  {
    "objectID": "pages/intro.html#deep-learning",
    "href": "pages/intro.html#deep-learning",
    "title": "1 - Introduction",
    "section": "",
    "text": "To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced Everingham et al. (2007). These involved various tasks, such as detecting objects in photographs (Figure 33).\n\n\n\n\n\n\nFigure 33: Images/illustrations from Link and Johnson (2022). On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.\n\n\n\nThe easy availability of images on the internet has made it possible to collect increasingly larger datasets. ImageNet is such a very large, hierarchically annotated image dataset Deng et al. (2009) with over 1.4 million images, categorized into 1,000 object classes. Figure 34 illustrates the dataset.\n\n\n\n\n\n\nFigure 34: ImageNet, Image Source, details in Deng et al. (2009)\n\n\n\nSince 2010, challenges have been regularly conducted on the ImageNet dataset Russakovsky et al. (2015), such as image classification and object detection. Figure 35 shows the development of the error rate over time.\n\n\n\n\n\n\nFigure 35: Source: Johnson (2022)\n\n\n\nIn 2011, a team won Perronnin et al. (2010) by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.\nIn 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. Krizhevsky, Sutskever, and Hinton (2012) impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in Figure 36.\n\n\n\n\n\n\nFigure 36: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nWhile classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms Figure 37, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning Figure 38.\n\n\n\n\n\n\nFigure 37: Illustration from Johnson (2022)\n\n\n\n\n\n\n\n\n\nFigure 38: Illustration from Johnson (2022)\n\n\n\nDeep learning-based approaches have several advantages over classical machine learning methods:\n\nAutomatic feature extraction: no manual feature extraction procedures are needed.\nHierarchical features: these are particularly valuable for processing and understanding visual data.\nGeneralization: with more training data, deep learning methods generalize better.\nEnd-to-end learning: this approach allows many problems to be modeled similarly.\nRobustness to variability: certain models are naturally invariant to\n\ntransformations like translations, scalings, etc. - Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.\n\n\nWe will now explore the most important milestones in deep learning for image analysis.\n\n\n\nHubel and Wiesel (1959) showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.\n\n\n\n\n\n\nFigure 39: Illustration Source\n\n\n\n\n\n\nFukushima (1980) defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 40: The Neocognitron Fukushima (1980).\n\n\n\n\n\n\nRumelhart, Hinton, and Williams (1986) introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.\n\n\n\n\n\n\nFigure 41: Backpropagation in neural networks Rumelhart, Hinton, and Williams (1986).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good video on backpropagation: 3Blue1Brown Backpropagation Calculus\n\n\n\n\n\nLecun et al. (1998) implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in Figure 42.\n\n\n\n\n\n\nFigure 42: Modern CNN Lecun et al. (1998).\n\n\n\nCNNs became extremely popular after winning the ImageNet competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in Figure 43.\n\n\n\n\n\n\nFigure 43: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the COCO Challenges, with various tasks.\n\n\n\nCNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models Dosovitskiy et al. (2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures Liu et al. (2021). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures Woo et al. (2023). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily Smith et al. (2023)."
  },
  {
    "objectID": "pages/mini_projects.html",
    "href": "pages/mini_projects.html",
    "title": "Mini Projects",
    "section": "",
    "text": "The mini-project aims to apply the learned knowledge. Students should be able to model a specific problem in Computer Vision using Deep Learning methods. They should demonstrate their ability to implement, train, and evaluate models with PyTorch and handle GPU resources. Students should be able to present and defend their work."
  },
  {
    "objectID": "pages/mini_projects.html#image-classification-eurosat-land-use-and-land-cover-classification",
    "href": "pages/mini_projects.html#image-classification-eurosat-land-use-and-land-cover-classification",
    "title": "Mini Projects",
    "section": "Image Classification: EuroSAT Land Use and Land Cover Classification",
    "text": "Image Classification: EuroSAT Land Use and Land Cover Classification\nGoal: Develop a model to classify satellite images. You should consider RGB images and images with 13 spectral bands (see wiki) in modeling. There are 10 classes and 27,000 images.\nApproach: Develop CNNs to model the data. Investigate various architectures and decide what works best. Compare pre-trained models with those you train from scratch. Use appropriate data augmentation techniques. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models. Compare RGB-only models with models that use the full 13 spectral bands.\nDataset: The two datasets can be found here: https://github.com/phelber/eurosat. There is a dataset in RGB format and a dataset with 13 spectral bands.\n\n\n\n\n\n\nFigure 1: Source: Link\n\n\n\nDifficulty/Effort: Small - Medium"
  },
  {
    "objectID": "pages/mini_projects.html#semantic-segmentation-underwater-imagery",
    "href": "pages/mini_projects.html#semantic-segmentation-underwater-imagery",
    "title": "Mini Projects",
    "section": "Semantic Segmentation: Underwater Imagery",
    "text": "Semantic Segmentation: Underwater Imagery\nGoal: Develop a segmentation model to segment underwater camera images. You should classify pixels into 8 classes on 1,500 images.\nApproach: Develop your architecture based on your intuition and knowledge from the course. Read the paper Islam et al. (2020) and implement one of the architectures presented (SUIM-Net RSB or SUIM-Net VGG) and compare with your architecture. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models.\nDataset: Available here: https://irvlab.cs.umn.edu/resources/suim-dataset.\n\n\n\n\n\n\nFigure 2: Source: Link\n\n\n\nDifficulty/Effort: Medium"
  },
  {
    "objectID": "pages/mini_projects.html#object-detection-from-ground-up",
    "href": "pages/mini_projects.html#object-detection-from-ground-up",
    "title": "Mini Projects",
    "section": "Object Detection: from Ground Up",
    "text": "Object Detection: from Ground Up\nGoal: Implement object detection with an end-to-end deep learning approach.\nApproach: Define a model type, such as YOLO (Vx), CenterNet, and implement it. Compare various hyperparameters.\nDataset: Use the Pascal VOC dataset for training.\n\n\n\n\n\n\nFigure 3: Source: Redmon et al. (2016)\n\n\n\nDifficulty/Effort: Medium to Very High (depending on the choice of methods)."
  },
  {
    "objectID": "pages/mini_projects.html#image-to-image-steganography",
    "href": "pages/mini_projects.html#image-to-image-steganography",
    "title": "Mini Projects",
    "section": "Image-to-Image: Steganography",
    "text": "Image-to-Image: Steganography\nGoal: Develop a model to hide image A in image B. Image B should remain as unchanged as possible, and image A should be extracted from image B as accurately as possible.\nApproach: Define an encoder model that hides image A in image B. Define a decoder model that extracts image A from image B. Train the models end-to-end. You can be inspired by Baluja (2017).\nDataset: Use an image dataset and generate the training data yourself. You can use the COCO 2017 dataset.\n\n\n\n\n\n\nFigure 4: Source: Baluja (2017)\n\n\n\nDifficulty/Effort: Medium - High."
  },
  {
    "objectID": "pages/mini_projects.html#image-to-image-super-resolution-on-faces",
    "href": "pages/mini_projects.html#image-to-image-super-resolution-on-faces",
    "title": "Mini Projects",
    "section": "Image-to-Image: Super Resolution on Faces",
    "text": "Image-to-Image: Super Resolution on Faces\nGoal: Develop a model that converts a low-res image of a face to a higher resolution.\nApproach: Define an image-to-image architecture and train the model with appropriate data. You can assume fixed input and output resolutions. Implement the method from Dong et al. (2015). Test other, own variations of the architecture.\nDataset: Use images of faces. You can download up to 70,000 images from here: ffhq.\n\n\n\n\n\n\nFigure 5: Source: ffhq\n\n\n\nDifficulty/Effort: Small - Medium"
  },
  {
    "objectID": "slides/intro.html#beispiel-1",
    "href": "slides/intro.html#beispiel-1",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 1",
    "text": "Beispiel 1\n\nBeispiel aus Link. Links das Original-Bild, rechts die mit Deep Learning verbesserte Version."
  },
  {
    "objectID": "slides/intro.html#beispiel-2",
    "href": "slides/intro.html#beispiel-2",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 2",
    "text": "Beispiel 2\n\nBeispiel aus Link. Links das Original-Bild, rechts die Manipulation."
  },
  {
    "objectID": "slides/intro.html#beispiel-3",
    "href": "slides/intro.html#beispiel-3",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 3",
    "text": "Beispiel 3\n\nBeispiel aus Link. Links das Original-Bild, rechts die Manipulation."
  },
  {
    "objectID": "slides/intro.html#beispiel-4",
    "href": "slides/intro.html#beispiel-4",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 4",
    "text": "Beispiel 4\n\nAus Link."
  },
  {
    "objectID": "slides/intro.html#semantic-gap",
    "href": "slides/intro.html#semantic-gap",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\nIllustration des semantic gap."
  },
  {
    "objectID": "slides/intro.html#blickwinkel",
    "href": "slides/intro.html#blickwinkel",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Blickwinkel",
    "text": "Blickwinkel\n\nSource"
  },
  {
    "objectID": "slides/intro.html#deformation",
    "href": "slides/intro.html#deformation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Deformation",
    "text": "Deformation\n\nSource"
  },
  {
    "objectID": "slides/intro.html#beleuchtung",
    "href": "slides/intro.html#beleuchtung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beleuchtung",
    "text": "Beleuchtung\n\nSource"
  },
  {
    "objectID": "slides/intro.html#hintergrund",
    "href": "slides/intro.html#hintergrund",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Hintergrund",
    "text": "Hintergrund\n\nSource"
  },
  {
    "objectID": "slides/intro.html#okklusion",
    "href": "slides/intro.html#okklusion",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Okklusion",
    "text": "Okklusion\n\nSource"
  },
  {
    "objectID": "slides/intro.html#intraklass-variation",
    "href": "slides/intro.html#intraklass-variation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Intraklass-Variation",
    "text": "Intraklass-Variation\n\nSource"
  },
  {
    "objectID": "slides/intro.html#kontext-abhängigkeit",
    "href": "slides/intro.html#kontext-abhängigkeit",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Kontext-Abhängigkeit",
    "text": "Kontext-Abhängigkeit\n\n\n\n\n\n\n\n\n\n\nKontext Source"
  },
  {
    "objectID": "slides/intro.html#image-classification",
    "href": "slides/intro.html#image-classification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012))."
  },
  {
    "objectID": "slides/intro.html#objekt-erkennung",
    "href": "slides/intro.html#objekt-erkennung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Objekt-Erkennung",
    "text": "Objekt-Erkennung\n\nObject Detection Beispiel (aus Redmon et al. (2016)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist."
  },
  {
    "objectID": "slides/intro.html#segmentierung",
    "href": "slides/intro.html#segmentierung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentierung",
    "text": "Segmentierung\n\nObject Segmentation Beispiel (aus He et al. (2018))."
  },
  {
    "objectID": "slides/intro.html#segmentierung-2",
    "href": "slides/intro.html#segmentierung-2",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentierung 2",
    "text": "Segmentierung 2"
  },
  {
    "objectID": "slides/intro.html#keypoint-detektierung",
    "href": "slides/intro.html#keypoint-detektierung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Keypoint Detektierung",
    "text": "Keypoint Detektierung\n\nKeypoint Detection Beispiel (aus He et al. (2018))."
  },
  {
    "objectID": "slides/intro.html#image-translation",
    "href": "slides/intro.html#image-translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Translation",
    "text": "Image Translation\n\nImage Generation Beispiel (aus Isola et al. (2018))."
  },
  {
    "objectID": "slides/intro.html#image-super-resolution",
    "href": "slides/intro.html#image-super-resolution",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Super Resolution",
    "text": "Image Super Resolution\n\nNvidia dlss: Link"
  },
  {
    "objectID": "slides/intro.html#image-colorization",
    "href": "slides/intro.html#image-colorization",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Colorization",
    "text": "Image Colorization\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link"
  },
  {
    "objectID": "slides/intro.html#machine-learning-approach",
    "href": "slides/intro.html#machine-learning-approach",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Approach",
    "text": "Machine Learning Approach\nWith Machine Learning, we follow a data-driven approach to solve various tasks:\n\nCollect a dataset of images and their labels.\nUse a machine learning algorithm to train a model (e.g., a classifier).\nEvaluate and apply the model to new data.\n\ndef train(images, labels):\n    \"\"\" Train a Model \"\"\"\n    # Fit Model here\n    return model\n\ndef predict(test_images, model):\n    \"\"\" Predict \"\"\"\n    predictions = model(test_images)\n    return predictions"
  },
  {
    "objectID": "slides/intro.html#question",
    "href": "slides/intro.html#question",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Question",
    "text": "Question\nImage Super Resolution\nHow would you train a model for image super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality."
  },
  {
    "objectID": "slides/intro.html#machine-learning-pipeline",
    "href": "slides/intro.html#machine-learning-pipeline",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Pipeline",
    "text": "Machine Learning Pipeline\n\nMachine Learning Pipeline (Source: Raschka and Mirjalili (2020))"
  },
  {
    "objectID": "slides/intro.html#model",
    "href": "slides/intro.html#model",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Model",
    "text": "Model\nA model:\n\\[\\begin{equation}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{equation}\\]\nWith model parameters \\(\\theta\\):\n\\[\\begin{equation}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/intro.html#optimization",
    "href": "slides/intro.html#optimization",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Optimization",
    "text": "Optimization\nThe coefficients are adapted to a training dataset in an optimization procedure (learning, fitting).\nIn particular, we want to minimize the cost function \\(J\\).\n\\[\\begin{equation}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation}\\]\nThe optimization procedure is influenced by hyperparameters (\\(\\alpha, \\lambda, \\dots\\))."
  },
  {
    "objectID": "slides/intro.html#train-validation-test-split",
    "href": "slides/intro.html#train-validation-test-split",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Train (Validation) Test Split",
    "text": "Train (Validation) Test Split\n\nTrain-Test Split to select and measure models."
  },
  {
    "objectID": "slides/intro.html#machine-learning-on-images",
    "href": "slides/intro.html#machine-learning-on-images",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning on Images",
    "text": "Machine Learning on Images\nImages are high-dimensional:\nAn RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\).\nClassic ML algorithms are:\n\nSlow and resource-intensive.\nUnable to exploit the 2-D structure.\nSensitive to slight changes (e.g., translations).\nProne to overfitting (since \\(n \\sim p\\))."
  },
  {
    "objectID": "slides/intro.html#machine-learning-on-images-1",
    "href": "slides/intro.html#machine-learning-on-images-1",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning on Images",
    "text": "Machine Learning on Images\nTo model images with classic machine learning algorithms, features must be extracted beforehand.\nWe can use methods from classical computer vision."
  },
  {
    "objectID": "slides/intro.html#color-histograms-as-features",
    "href": "slides/intro.html#color-histograms-as-features",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Color Histograms as Features",
    "text": "Color Histograms as Features\n\n\n\nColor Histograms as Features (Source: Johnson (2022))"
  },
  {
    "objectID": "slides/intro.html#hog-features",
    "href": "slides/intro.html#hog-features",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "HOG Features",
    "text": "HOG Features\n\n\n\nHOG as Features (Source: Johnson (2022))"
  },
  {
    "objectID": "slides/intro.html#bag-of-visual-words",
    "href": "slides/intro.html#bag-of-visual-words",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Bag of (visual) Words",
    "text": "Bag of (visual) Words\n\n\n\nBag of (visual) words Features (Source: Johnson (2022))"
  },
  {
    "objectID": "slides/intro.html#image-features",
    "href": "slides/intro.html#image-features",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Features",
    "text": "Image Features\n\n\n\nImage Features (Source: Johnson (2022))"
  },
  {
    "objectID": "slides/intro.html#cifar10",
    "href": "slides/intro.html#cifar10",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "CIFAR10",
    "text": "CIFAR10\n\nCIFAR10 Dataset Source"
  },
  {
    "objectID": "slides/intro.html#exercise-1---recap-ml",
    "href": "slides/intro.html#exercise-1---recap-ml",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Exercise 1 - Recap ML",
    "text": "Exercise 1 - Recap ML\nIn the first exercise, we will model the CIFAR10 dataset."
  },
  {
    "objectID": "slides/intro.html#pascal-voc",
    "href": "slides/intro.html#pascal-voc",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "PASCAL VOC",
    "text": "PASCAL VOC\n\nImages / Illustrations from Link and Johnson (2022). Left: Object annotations in images, Right: Development of Mean Average Precision over the years."
  },
  {
    "objectID": "slides/intro.html#imagenet",
    "href": "slides/intro.html#imagenet",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "ImageNet",
    "text": "ImageNet\n\nImageNet, Image Source, details in Deng et al. (2009)"
  },
  {
    "objectID": "slides/intro.html#imagenet---performance",
    "href": "slides/intro.html#imagenet---performance",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "ImageNet - Performance",
    "text": "ImageNet - Performance\n\n\n\nSource: Johnson (2022)"
  },
  {
    "objectID": "slides/intro.html#imagenet---winner",
    "href": "slides/intro.html#imagenet---winner",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "ImageNet - Winner",
    "text": "ImageNet - Winner\n\n\n\nAlexNet Krizhevsky, Sutskever, and Hinton (2012)"
  },
  {
    "objectID": "slides/intro.html#classic-ml",
    "href": "slides/intro.html#classic-ml",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Classic ML",
    "text": "Classic ML\n\n\n\nIllustration from Johnson (2022)"
  },
  {
    "objectID": "slides/intro.html#end-to-end",
    "href": "slides/intro.html#end-to-end",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "End-To-End",
    "text": "End-To-End\n\n\n\nIllustration from Johnson (2022)"
  },
  {
    "objectID": "slides/intro.html#deep-learning-benefits",
    "href": "slides/intro.html#deep-learning-benefits",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Deep Learning Benefits",
    "text": "Deep Learning Benefits\n\nAutomatic feature extraction.\nHierarchical features.\nGeneralization.\nEnd-to-end learning.\nRobustness to variability.\nAdaptability and transferability."
  },
  {
    "objectID": "slides/intro.html#experiments-on-cats",
    "href": "slides/intro.html#experiments-on-cats",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Experiments on Cats",
    "text": "Experiments on Cats\n\nIllustration Source"
  },
  {
    "objectID": "slides/intro.html#neocognitron",
    "href": "slides/intro.html#neocognitron",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Neocognitron",
    "text": "Neocognitron\n\nThe Neocognitron Fukushima (1980)"
  },
  {
    "objectID": "slides/intro.html#backpropagation",
    "href": "slides/intro.html#backpropagation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nBackpropagation in Neural Networks Rumelhart, Hinton, and Williams (1986)"
  },
  {
    "objectID": "slides/intro.html#cnns",
    "href": "slides/intro.html#cnns",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "CNNs",
    "text": "CNNs\n\n\n\nModern CNN Lecun et al. (1998)"
  },
  {
    "objectID": "slides/intro.html#alexnet",
    "href": "slides/intro.html#alexnet",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "AlexNet",
    "text": "AlexNet\n\n\n\nAlexNet Krizhevsky, Sutskever, and Hinton (2012)"
  },
  {
    "objectID": "slides/recap.html#question-1",
    "href": "slides/recap.html#question-1",
    "title": "Recap CNNs",
    "section": "Question 1",
    "text": "Question 1\nWhat does a convolutional layer do in a Convolutional Neural Network (CNN)?\n\n\nIt combines input features into a single output.\n\n\nIt applies a set of filters to the input to detect patterns.\n\n\nIt reduces the dimensionality of the input data.\n\n\nIt generates a final classification score."
  },
  {
    "objectID": "slides/recap.html#question-2",
    "href": "slides/recap.html#question-2",
    "title": "Recap CNNs",
    "section": "Question 2",
    "text": "Question 2\nWhat is the main purpose of using pooling layers in CNNs?\n\n\nTo increase the number of parameters in the network.\n\n\nTo reduce the spatial dimensions (width and height) of the input volume.\n\n\nTo apply non-linear transformations to the data.\n\n\nTo convert the input into a one-dimensional vector."
  },
  {
    "objectID": "slides/recap.html#question-3",
    "href": "slides/recap.html#question-3",
    "title": "Recap CNNs",
    "section": "Question 3",
    "text": "Question 3\nIn the context of image classification, what is data augmentation?\n\n\nIncreasing the size of the dataset by generating new images through transformations such as rotation, flipping, and scaling.\n\n\nAdding more layers to the CNN to increase its capacity.\n\n\nUsing pre-trained models to improve accuracy.\n\n\nSplitting the dataset into training and testing sets."
  },
  {
    "objectID": "slides/recap.html#question-4",
    "href": "slides/recap.html#question-4",
    "title": "Recap CNNs",
    "section": "Question 4",
    "text": "Question 4\nWhat is transfer learning in the context of deep learning?\n\n\nTraining a new model from scratch on a specific dataset.\n\n\nUsing a pre-trained model on a new but related problem.\n\n\nCombining multiple models to improve performance.\n\n\nApplying data augmentation techniques to the training data."
  },
  {
    "objectID": "slides/recap.html#question-5",
    "href": "slides/recap.html#question-5",
    "title": "Recap CNNs",
    "section": "Question 5",
    "text": "Question 5\nWhat is meant by translation invariance in CNNs?\n\n\nThe ability of the network to apply the same weights to different parts of the input.\n\n\nThe ability of the network to recognize an object regardless of its position in the image.\n\n\nThe process of converting data from one format to another.\n\n\nThe use of convolutional layers instead of fully connected layers."
  },
  {
    "objectID": "slides/recap.html#question-6",
    "href": "slides/recap.html#question-6",
    "title": "Recap CNNs",
    "section": "Question 6",
    "text": "Question 6\nWhat is weight sharing in CNNs?\n\n\nUsing the same weights for different layers in the network.\n\n\nApplying the same weights across different parts of the input image.\n\n\nSharing weights between different CNNs.\n\n\nUsing pre-trained weights from another model."
  },
  {
    "objectID": "slides/recap.html#question-7",
    "href": "slides/recap.html#question-7",
    "title": "Recap CNNs",
    "section": "Question 7",
    "text": "Question 7\nWhat is the difference between translation invariance and translation equivariance in CNNs?\n\n\nInvariance means the output remains unchanged with translation, while equivariance means the output changes in a predictable way with translation.\n\n\nEquivariance means the output remains unchanged with translation, while invariance means the output changes in a predictable way with translation.\n\n\nInvariance and equivariance are the same concepts.\n\n\nBoth terms refer to the ability of CNNs to handle rotations."
  },
  {
    "objectID": "pages/slides_cas.html",
    "href": "pages/slides_cas.html",
    "title": "Slides",
    "section": "",
    "text": "Recap Quiz slides.\nIntro slides.\nFrameworks slides.\nNeural Networks slides.\nCNNs slides.\nImage Classification slides.\nPractical slides."
  },
  {
    "objectID": "pages/literature.html",
    "href": "pages/literature.html",
    "title": "Books",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#pytorch",
    "href": "pages/literature.html#pytorch",
    "title": "Books",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#deep-learning",
    "href": "pages/literature.html#deep-learning",
    "title": "Books",
    "section": "Deep Learning",
    "text": "Deep Learning\nSimon J.D. Prince, Understanding Deep Learning, MIT Press, Prince (2023)\n\nBrandaktuelles Buch über Deep Learning\nUmfassende Einführung ins Thema mit sehr guten Illustrationen\nOnline verfügbar: Link\n\nGoodfellow, Ian and Bengio, Yoshua and Courville, Aaron, Deep Learning, MIT Press, Goodfellow, Bengio, and Courville (2016)\n\nSehr gute und umfassende Einführung in Deep Learning\nEtwas älter aber immer noch in weiten Teilen aktuell\nOnline verfügbar: Link\n\nChollet, François, Deep Learning with Python, Second Edition, Manning Publications, Chollet (2021)\n\nchapters 8-9 are about computer vision\nfree access with FHNW-Emailadresse in O’Reilly online Mediathek\n\nStevens et al, Deep Learning with PyTorch, Manning Publications, Stevens, Antiga, and Viehmann (2020)",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#machine-learning",
    "href": "pages/literature.html#machine-learning",
    "title": "Books",
    "section": "Machine Learning",
    "text": "Machine Learning\nGéron A, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, O’Reilly 2019\n\nJupyter Notebooks sind öffentlich verfügbar: Link\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nRaschka S, Python Machine Learning, 3rd Edition, PACKT 2019\n\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nKevin P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press 2022\n\nVorabversion gratis verfügbar: Link\nUmfassende Einführung in Machine Learning mit ausführlichen theoretischen Hintergründen\n\nChollet F, Deep Learning with Python, 2nd Edition, MEAP 2020\n\nEin Klassiker für eine Einführung in Deep Learning (und Keras)\n\nHastie T et al., Elements of Statistical Learning, Springer 2009.\n\nKann als pdf gratis runtergeladen werden: Link\nEnthält Machine Learning Grundlagen und viele Methoden (wenig über Neuronale Netzwerke)\n\nVanderPlas J, Python Data Science Handbook, O’Reilly 2017.\n\nWurde mit Jupyter Notebooks geschrieben.\nDer gesamte Inhalt finden sie auf einer website: Link\nDas Repository kann von github runtergelanden werden: Link",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/exercises.html",
    "href": "pages/exercises.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nExercises\nExercises can be found here: Link",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Herzlich willkommen zum Modul Computer Vision mit Deep Learning (1. Teil)!\nHier finden Sie Unterlagen und aktuelle Informationen zum Modul.\nModul Page\nCAS Page",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#grundlagen-convolutional-neural-networks-tag-1",
    "href": "index.html#grundlagen-convolutional-neural-networks-tag-1",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "2.1 Grundlagen Convolutional Neural Networks (Tag 1)",
    "text": "2.1 Grundlagen Convolutional Neural Networks (Tag 1)\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nEinführung Computer Vision mit Deep Learning\n\n\n9:30 - 10:30\nÜbung: Deep Learning mit PyTorch und Bilder\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nConvolutional Neural Networks\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 16:30\nÜbung: CNNs für Bildklassifikation",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#bildklassifikation-und-anwendungen-tag-2",
    "href": "index.html#bildklassifikation-und-anwendungen-tag-2",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "2.2 Bildklassifikation und Anwendungen (Tag 2)",
    "text": "2.2 Bildklassifikation und Anwendungen (Tag 2)\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nRecap & Quizz\n\n\n9:30 - 10:30\nTheorie: Praktische Überlegungen\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nÜbung: Bildklassifikation\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 14:30\nÜbung: Bildklassifikation\n\n\n14:30 - 14:45\nPause\n\n\n14:45 - 15:15\nTheorie: Foundation Models\n\n\n15:15 - 16:30\nÜbung: Foundation Models",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/links.html",
    "href": "pages/links.html",
    "title": "Helpful Links & Resources",
    "section": "",
    "text": "Links and ressources to different topics related to Machine Learning, Deep Learning, and Images.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#theory",
    "href": "pages/links.html#theory",
    "title": "Helpful Links & Resources",
    "section": "Theory",
    "text": "Theory\n\nPyTorch\nPyTorch internals - Blog Post\n\n\nDeep Learning and Computer Vision\nUniversity of Michigan - Deep Learning for Computer Vision\n\nSehr gute Vorlesung zum Thema\n\nUniversity of California, Berkeley - Modern Computer Vision and Deep Learning\n\nSehr gute Vorlesung zum Thema\n\n\n\nNeuronale Netzwerke - Basics\nPerceptron Learning Rule S. Raschka\nCS229 Stanford MLP Backpropagation\nNotes on Backpropagation\n3Blue1Brown Gradient Descent\n3Blue1Brown Backpropagation Calculus\nAndrew Ng Backprop\nAndrej Karpathy - Backpropagation from the ground up\n\n\nModel Selection\nPaper von S.Raschka: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#practical",
    "href": "pages/links.html#practical",
    "title": "Helpful Links & Resources",
    "section": "Practical",
    "text": "Practical\nAndrej Karpathy - A Recipe for Training Neural Networks\n\nML Best Practices Videos\nMartin Zinkevich - Best Practices for ML Engineering\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Try Next\nAndrew Ng - Advice For Applying Machine Learning | Learning Curves\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)\nAndrew Ng - Machine Learning System Design | Prioritizing What To Work On\nAndrew Ng - Machine Learning System Design | Error Analysis\nAndrew Ng - Machine Learning System Design | Data For Machine Learning",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#tools",
    "href": "pages/links.html#tools",
    "title": "Helpful Links & Resources",
    "section": "Tools",
    "text": "Tools\n\nData Science Repository\nBuild a Reproducible and Maintainable Data Science Project\n\ngreat jupyter book to learen about how to structure a repository and more\n\nLightning-Hydra-Template\n\ntemplate to strcuture a repository based on experiment configuration with Hydra and Pytorch-Lightning\n\n\n\nData Handling\ndatasets\n\nGreat package to create and manage (large) image datasets\n\nimg2dataset\n\nPackage to download large image datasets from urls\n\nDVC\n\nPackage for data version control\n\n\n\nPyTorch\nLightning\n\nboilerplate code to easily train models and use gpu, etc.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/slides.html",
    "href": "pages/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Recap Quiz slides.\nIntro slides.\nFrameworks slides.\nNeural Networks slides.\nCNNs slides.\nImage Classification slides.\nPractical slides."
  },
  {
    "objectID": "slides/image_classification.html#overview",
    "href": "slides/image_classification.html#overview",
    "title": "Image Classification",
    "section": "Overview",
    "text": "Overview\n\nIntroduction\nModeling\nLoss Function\nArchitectures"
  },
  {
    "objectID": "slides/image_classification.html#adversarial-panda",
    "href": "slides/image_classification.html#adversarial-panda",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)"
  },
  {
    "objectID": "slides/image_classification.html#adversarial-panda-1",
    "href": "slides/image_classification.html#adversarial-panda-1",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)"
  },
  {
    "objectID": "slides/image_classification.html#image-classification",
    "href": "slides/image_classification.html#image-classification",
    "title": "Image Classification",
    "section": "Image Classification",
    "text": "Image Classification\n\n\n\nExample of Image Classification."
  },
  {
    "objectID": "slides/image_classification.html#image-classification-example",
    "href": "slides/image_classification.html#image-classification-example",
    "title": "Image Classification",
    "section": "Image Classification: Example",
    "text": "Image Classification: Example\n\n\n\nExample of Image Classification (from Krizhevsky, Sutskever, and Hinton (2012))."
  },
  {
    "objectID": "slides/image_classification.html#image-classification-camera-traps",
    "href": "slides/image_classification.html#image-classification-camera-traps",
    "title": "Image Classification",
    "section": "Image Classification: Camera Traps",
    "text": "Image Classification: Camera Traps\n\n\n\nExample images from camera traps."
  },
  {
    "objectID": "slides/image_classification.html#parametric-approach",
    "href": "slides/image_classification.html#parametric-approach",
    "title": "Image Classification",
    "section": "Parametric Approach",
    "text": "Parametric Approach\nWith a parametric approach, we seek a model of the following form:\n\\[\\begin{equation}\n    \\hat{y}^{(i)} = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThe model parameters \\(\\theta\\) define our model and must be learned with an algorithm."
  },
  {
    "objectID": "slides/image_classification.html#softmax-classifier",
    "href": "slides/image_classification.html#softmax-classifier",
    "title": "Image Classification",
    "section": "Softmax Classifier",
    "text": "Softmax Classifier\nWe want to model the following probability:\n\\[\\begin{equation}\n    P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the Softmax function \\(\\sigma(\\mathbf{z})\\)."
  },
  {
    "objectID": "slides/image_classification.html#softmax-transformation",
    "href": "slides/image_classification.html#softmax-transformation",
    "title": "Image Classification",
    "section": "Softmax Transformation",
    "text": "Softmax Transformation\n\\[\\begin{equation}\n    P(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/image_classification.html#logits-to-probabilities",
    "href": "slides/image_classification.html#logits-to-probabilities",
    "title": "Image Classification",
    "section": "Logits to Probabilities",
    "text": "Logits to Probabilities\n\n\n\nLogits (left) to probabilities with the Softmax function."
  },
  {
    "objectID": "slides/image_classification.html#probabilities",
    "href": "slides/image_classification.html#probabilities",
    "title": "Image Classification",
    "section": "Probabilities",
    "text": "Probabilities\n\nImage classifier with confidences."
  },
  {
    "objectID": "slides/image_classification.html#likelihood",
    "href": "slides/image_classification.html#likelihood",
    "title": "Image Classification",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood of a data point:\n\\[\\begin{equation}\n    P(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThis is the modeled probability for the actually observed class \\(y^{(i)}\\)."
  },
  {
    "objectID": "slides/image_classification.html#likelihood-for-multi-class-classification",
    "href": "slides/image_classification.html#likelihood-for-multi-class-classification",
    "title": "Image Classification",
    "section": "Likelihood for Multi-Class Classification",
    "text": "Likelihood for Multi-Class Classification\nThe likelihood of a data point for multi-class classification:\n\\[\\begin{equation}\n    \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nWhere \\(y^{(i)} \\in \\mathbb{R}^{K}\\) is a one-hot encoded vector, with the \\(1\\) at the true class."
  },
  {
    "objectID": "slides/image_classification.html#maximum-likelihood",
    "href": "slides/image_classification.html#maximum-likelihood",
    "title": "Image Classification",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe likelihood of an entire dataset:\n\\[\\begin{equation}\n    \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the maximum likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset."
  },
  {
    "objectID": "slides/image_classification.html#negative-log-likelihood",
    "href": "slides/image_classification.html#negative-log-likelihood",
    "title": "Image Classification",
    "section": "Negative Log-Likelihood",
    "text": "Negative Log-Likelihood\nEquivalently, we can minimize the negative log likelihood:\n\\[\\begin{align}\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& - \\log \\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{align}\\]"
  },
  {
    "objectID": "slides/image_classification.html#cross-entropy",
    "href": "slides/image_classification.html#cross-entropy",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nThe loss function derived with maximum likelihood can also be viewed from the perspective of cross-entropy between two discrete probability distributions.\n\\[\\begin{align}\n    CE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\n    CE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/image_classification.html#cross-entropy-1",
    "href": "slides/image_classification.html#cross-entropy-1",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\n\n\n\nTrue distribution (left) and predicted distribution (right)."
  },
  {
    "objectID": "slides/image_classification.html#alexnet",
    "href": "slides/image_classification.html#alexnet",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\n\n\nAlexNet Krizhevsky, Sutskever, and Hinton (2012)"
  },
  {
    "objectID": "slides/image_classification.html#alexnet-1",
    "href": "slides/image_classification.html#alexnet-1",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\n\n\nAlexNet Llamas et al. (2017)"
  },
  {
    "objectID": "slides/image_classification.html#alexnet-table",
    "href": "slides/image_classification.html#alexnet-table",
    "title": "Image Classification",
    "section": "AlexNet: Table",
    "text": "AlexNet: Table\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/image_classification.html#vgg",
    "href": "slides/image_classification.html#vgg",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\n\nVGG Simonyan and Zisserman (2015)"
  },
  {
    "objectID": "slides/image_classification.html#vgg-1",
    "href": "slides/image_classification.html#vgg-1",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\n\nSource: Link"
  },
  {
    "objectID": "slides/image_classification.html#vgg-2",
    "href": "slides/image_classification.html#vgg-2",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/image_classification.html#resnet",
    "href": "slides/image_classification.html#resnet",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nSource: He et al. (2016)"
  },
  {
    "objectID": "slides/image_classification.html#resnet-1",
    "href": "slides/image_classification.html#resnet-1",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nSource: He et al. (2016)"
  },
  {
    "objectID": "slides/image_classification.html#resnet-2",
    "href": "slides/image_classification.html#resnet-2",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nResNet He et al. (2016) (Image from Johnson (2019))"
  },
  {
    "objectID": "slides/image_classification.html#resnet-3",
    "href": "slides/image_classification.html#resnet-3",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom He et al. (2016)"
  },
  {
    "objectID": "slides/image_classification.html#resnet-4",
    "href": "slides/image_classification.html#resnet-4",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom Li et al. (2018)"
  },
  {
    "objectID": "slides/image_classification.html#convnext",
    "href": "slides/image_classification.html#convnext",
    "title": "Image Classification",
    "section": "ConvNext",
    "text": "ConvNext\n\n\n\nConvNext Liu et al. (2022)"
  },
  {
    "objectID": "slides/image_classification.html#imagenet-performance",
    "href": "slides/image_classification.html#imagenet-performance",
    "title": "Image Classification",
    "section": "ImageNet Performance",
    "text": "ImageNet Performance\n\n\n\nImage from Johnson (2019)"
  },
  {
    "objectID": "slides/image_classification.html#choosing-the-architecture",
    "href": "slides/image_classification.html#choosing-the-architecture",
    "title": "Image Classification",
    "section": "Choosing the Architecture",
    "text": "Choosing the Architecture\nDon’t be a hero!\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets."
  },
  {
    "objectID": "slides/image_classification.html#squeeze-excite-networks",
    "href": "slides/image_classification.html#squeeze-excite-networks",
    "title": "Image Classification",
    "section": "Squeeze / Excite Networks",
    "text": "Squeeze / Excite Networks\n\n\n\nFrom Hu et al. (2019)"
  },
  {
    "objectID": "slides/image_classification.html#normalization-layers",
    "href": "slides/image_classification.html#normalization-layers",
    "title": "Image Classification",
    "section": "Normalization Layers",
    "text": "Normalization Layers\n\n\n\nSource: Qiao et al. (2020)"
  },
  {
    "objectID": "slides/image_classification.html#pre-processing",
    "href": "slides/image_classification.html#pre-processing",
    "title": "Image Classification",
    "section": "Pre-Processing",
    "text": "Pre-Processing\n\nResizing / Cropping to a fixed size\nScaling: from the range [0, 255] to the range [0, 1]\nNormalization: Often normalized along the color channels\n\nPyTorch Examples"
  },
  {
    "objectID": "slides/image_classification.html#transfer-learning",
    "href": "slides/image_classification.html#transfer-learning",
    "title": "Image Classification",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer learning refers to the process of adapting a trained model that models Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations."
  },
  {
    "objectID": "slides/segmentation.html#overview",
    "href": "slides/segmentation.html#overview",
    "title": "Segmentation",
    "section": "Overview",
    "text": "Overview\n\nIntroduction & Motivation\nSemantic Segmentation\nInstance Segmentation\nPanoptic Segmentation\nMetrics"
  },
  {
    "objectID": "slides/segmentation.html#image-segmentation",
    "href": "slides/segmentation.html#image-segmentation",
    "title": "Segmentation",
    "section": "Image Segmentation",
    "text": "Image Segmentation\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#semantic-segmentation-road-segmentation",
    "href": "slides/segmentation.html#semantic-segmentation-road-segmentation",
    "title": "Segmentation",
    "section": "Semantic Segmentation: Road Segmentation",
    "text": "Semantic Segmentation: Road Segmentation\n\n\n\nTop: Photo, Bottom: Annotated Segmentation Map. Source: Cordts et al. (2016)"
  },
  {
    "objectID": "slides/segmentation.html#semantic-segmentation-medical",
    "href": "slides/segmentation.html#semantic-segmentation-medical",
    "title": "Segmentation",
    "section": "Semantic Segmentation: Medical",
    "text": "Semantic Segmentation: Medical\n\n\n\nSource: Novikov et al. (2018)"
  },
  {
    "objectID": "slides/segmentation.html#instance-segmentation",
    "href": "slides/segmentation.html#instance-segmentation",
    "title": "Segmentation",
    "section": "Instance Segmentation",
    "text": "Instance Segmentation\n\n\n\nInstance Segmentation. Source: He et al. (2018)"
  },
  {
    "objectID": "slides/segmentation.html#sliding-window",
    "href": "slides/segmentation.html#sliding-window",
    "title": "Segmentation",
    "section": "Sliding Window",
    "text": "Sliding Window\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#fully-convolutional-network---concept",
    "href": "slides/segmentation.html#fully-convolutional-network---concept",
    "title": "Segmentation",
    "section": "Fully-Convolutional Network - Concept",
    "text": "Fully-Convolutional Network - Concept\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#fully-convolutional-network",
    "href": "slides/segmentation.html#fully-convolutional-network",
    "title": "Segmentation",
    "section": "Fully-Convolutional Network",
    "text": "Fully-Convolutional Network\n\n\n\nSource: Tai et al. (2017). Architecture is applied as in the FCN paper by Shelhamer, Long, and Darrell (2016)"
  },
  {
    "objectID": "slides/segmentation.html#fully-convolutional-network-results",
    "href": "slides/segmentation.html#fully-convolutional-network-results",
    "title": "Segmentation",
    "section": "Fully-Convolutional Network: Results",
    "text": "Fully-Convolutional Network: Results\n\n\n\nFrom left to right shows the results of models with skip connections to increasingly earlier layers. Far right is the ground truth. Source: Shelhamer, Long, and Darrell (2016)"
  },
  {
    "objectID": "slides/segmentation.html#encoder-decoder-concept",
    "href": "slides/segmentation.html#encoder-decoder-concept",
    "title": "Segmentation",
    "section": "Encoder-Decoder: Concept",
    "text": "Encoder-Decoder: Concept\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#encoder-decoder",
    "href": "slides/segmentation.html#encoder-decoder",
    "title": "Segmentation",
    "section": "Encoder-Decoder",
    "text": "Encoder-Decoder\n\n\n\nSource: Noh, Hong, and Han (2015)"
  },
  {
    "objectID": "slides/segmentation.html#upsampling-unpooling",
    "href": "slides/segmentation.html#upsampling-unpooling",
    "title": "Segmentation",
    "section": "Upsampling: Unpooling",
    "text": "Upsampling: Unpooling\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#upsampling-unpooling-with-switch",
    "href": "slides/segmentation.html#upsampling-unpooling-with-switch",
    "title": "Segmentation",
    "section": "Upsampling: Unpooling with Switch",
    "text": "Upsampling: Unpooling with Switch\n\n\n\nSource: Noh, Hong, and Han (2015)"
  },
  {
    "objectID": "slides/segmentation.html#upsampling-unpooling-with-switch-1",
    "href": "slides/segmentation.html#upsampling-unpooling-with-switch-1",
    "title": "Segmentation",
    "section": "Upsampling: Unpooling with Switch",
    "text": "Upsampling: Unpooling with Switch\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#upsampling-interpolation",
    "href": "slides/segmentation.html#upsampling-interpolation",
    "title": "Segmentation",
    "section": "Upsampling: Interpolation",
    "text": "Upsampling: Interpolation\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#transposed-convolution",
    "href": "slides/segmentation.html#transposed-convolution",
    "title": "Segmentation",
    "section": "Transposed Convolution",
    "text": "Transposed Convolution\n\n\n\nTransposed Convolution with kernel size 2 and stride 2."
  },
  {
    "objectID": "slides/segmentation.html#transposed-convolution-1",
    "href": "slides/segmentation.html#transposed-convolution-1",
    "title": "Segmentation",
    "section": "Transposed Convolution",
    "text": "Transposed Convolution\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#transposed-convolution-matrix-notation",
    "href": "slides/segmentation.html#transposed-convolution-matrix-notation",
    "title": "Segmentation",
    "section": "Transposed Convolution: Matrix Notation",
    "text": "Transposed Convolution: Matrix Notation\n\n\n\nSource: Johnson (2019). \\(x\\) is the kernel, \\(X\\) is the kernel as a matrix, \\(a\\) is the input."
  },
  {
    "objectID": "slides/segmentation.html#unet-semantic-segmentation",
    "href": "slides/segmentation.html#unet-semantic-segmentation",
    "title": "Segmentation",
    "section": "UNet: Semantic Segmentation",
    "text": "UNet: Semantic Segmentation\n\n\n\nSource: Ronneberger, Fischer, and Brox (2015)"
  },
  {
    "objectID": "slides/segmentation.html#unet-architecture",
    "href": "slides/segmentation.html#unet-architecture",
    "title": "Segmentation",
    "section": "UNet: Architecture",
    "text": "UNet: Architecture\n\n\n\nSource: Ronneberger, Fischer, and Brox (2015)"
  },
  {
    "objectID": "slides/segmentation.html#loss",
    "href": "slides/segmentation.html#loss",
    "title": "Segmentation",
    "section": "Loss",
    "text": "Loss\n\n\n\nPixel-Level Softmax illustrated for a single pixel. Output is \\(H \\times W \\times K\\)."
  },
  {
    "objectID": "slides/segmentation.html#cross-entropy-loss-function",
    "href": "slides/segmentation.html#cross-entropy-loss-function",
    "title": "Segmentation",
    "section": "Cross-Entropy Loss Function",
    "text": "Cross-Entropy Loss Function\n\\[\nCE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\]\n\n\\(N\\): Number of pixels\n\\(K\\): Number of classes\n\\(\\hat{y}_j, y_j\\): Prediction / Ground truth for class \\(j\\)"
  },
  {
    "objectID": "slides/segmentation.html#mask-r-cnn",
    "href": "slides/segmentation.html#mask-r-cnn",
    "title": "Segmentation",
    "section": "Mask R-CNN",
    "text": "Mask R-CNN\n\n\n\nSource: He et al. (2018)"
  },
  {
    "objectID": "slides/segmentation.html#loss-function",
    "href": "slides/segmentation.html#loss-function",
    "title": "Segmentation",
    "section": "Loss Function",
    "text": "Loss Function\n\\[\n\\text{binary CE} = - \\sum_{i=1}^{N^2}  \\Big( (\\log \\hat{y}_k^{(i)})^{y_k^{(i)}} + (\\log (1-\\hat{y}_k^{(i)}))^{(1 - y_k^{(i)})} \\Big)\n\\]"
  },
  {
    "objectID": "slides/segmentation.html#mask-r-cnn-masks",
    "href": "slides/segmentation.html#mask-r-cnn-masks",
    "title": "Segmentation",
    "section": "Mask R-CNN: Masks",
    "text": "Mask R-CNN: Masks\n\n\n\nSource: He et al. (2018)"
  },
  {
    "objectID": "slides/segmentation.html#mask-r-cnn-mask-ground-truth",
    "href": "slides/segmentation.html#mask-r-cnn-mask-ground-truth",
    "title": "Segmentation",
    "section": "Mask R-CNN: Mask Ground Truth",
    "text": "Mask R-CNN: Mask Ground Truth\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#mask-r-cnn-results",
    "href": "slides/segmentation.html#mask-r-cnn-results",
    "title": "Segmentation",
    "section": "Mask R-CNN: Results",
    "text": "Mask R-CNN: Results\n\n\n\nSource: He et al. (2018)"
  },
  {
    "objectID": "slides/segmentation.html#panoptic-segmentation-1",
    "href": "slides/segmentation.html#panoptic-segmentation-1",
    "title": "Segmentation",
    "section": "Panoptic Segmentation",
    "text": "Panoptic Segmentation\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#panoptic-segmentation-2",
    "href": "slides/segmentation.html#panoptic-segmentation-2",
    "title": "Segmentation",
    "section": "Panoptic Segmentation",
    "text": "Panoptic Segmentation\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/segmentation.html#pixel-accuracy",
    "href": "slides/segmentation.html#pixel-accuracy",
    "title": "Segmentation",
    "section": "Pixel Accuracy",
    "text": "Pixel Accuracy\n\\[\n\\text{PA} = \\frac{\\sum_{i=0}^Kp_{ii}}{\\sum_{i=0}^K\\sum_{j=0}^K p_{ij}}\n\\]"
  },
  {
    "objectID": "slides/segmentation.html#mean-pixel-accuracy",
    "href": "slides/segmentation.html#mean-pixel-accuracy",
    "title": "Segmentation",
    "section": "Mean Pixel Accuracy",
    "text": "Mean Pixel Accuracy\n\\[\n\\text{MPA} = \\frac{1}{K+1} \\sum_{i=0}^K \\frac{p_{ii}}{\\sum_{j=0}^K p_{ij}}\n\\]\nMean accuracy over all classes (equally weighted)."
  },
  {
    "objectID": "slides/segmentation.html#intersection-over-union-iou",
    "href": "slides/segmentation.html#intersection-over-union-iou",
    "title": "Segmentation",
    "section": "Intersection over Union (IoU)",
    "text": "Intersection over Union (IoU)\n\\[\n\\text{IoU} = \\frac{\\lvert A \\cap B \\rvert}{\\lvert A \\cup B \\rvert}\n\\]\nSame metric as in object detection, except that the regions are not rectangular."
  },
  {
    "objectID": "slides/segmentation.html#precision-recall-f1",
    "href": "slides/segmentation.html#precision-recall-f1",
    "title": "Segmentation",
    "section": "Precision / Recall / F1",
    "text": "Precision / Recall / F1\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nF-1 is the harmonic mean of precision and recall:\n\\[\n\\text{F1} = \\frac{2 \\text{Precision Recall}}{\\text{Precision} + \\text{Recall}}\n\\]"
  },
  {
    "objectID": "slides/segmentation.html#dice-coefficient",
    "href": "slides/segmentation.html#dice-coefficient",
    "title": "Segmentation",
    "section": "Dice Coefficient",
    "text": "Dice Coefficient\n\\[\n\\text{Dice} = \\frac{2 \\lvert A \\cap B \\rvert}{\\lvert A \\rvert + \\lvert  B \\rvert}\n\\]\nVery similar to IoU. Usage is domain-specific, e.g., medical."
  },
  {
    "objectID": "pages/notation.html",
    "href": "pages/notation.html",
    "title": "",
    "section": "",
    "text": "ResourcesNotation Code",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#numbers-and-arrays",
    "href": "pages/notation.html#numbers-and-arrays",
    "title": "",
    "section": "Numbers and Arrays",
    "text": "Numbers and Arrays\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#indexing",
    "href": "pages/notation.html#indexing",
    "title": "",
    "section": "Indexing",
    "text": "Indexing\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(a_i\\)\nElement \\(i\\) of vector \\(\\mathbf{a}\\), with indexing starting at 1\n\n\n\\(A_{i,j}\\)\nElement \\(i, j\\) of matrix \\(\\mathbf{A}\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#datasets-and-distributions",
    "href": "pages/notation.html#datasets-and-distributions",
    "title": "",
    "section": "Datasets and Distributions",
    "text": "Datasets and Distributions\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\mathbf{X}\\)\nThe design matrix with dimensionality \\(nxp\\) with \\(n\\) samples with \\(p\\) features.\n\n\n\\(\\mathbf{x}^{(i)}\\)\nThe i-th training example.\n\n\n\\(\\mathbf{y}^{(i)}\\)\nThe label-vector for the i-th training example.\n\n\n\\(y^{(i)}\\)\nThe label for the i-th training example.",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#probability-theory",
    "href": "pages/notation.html#probability-theory",
    "title": "",
    "section": "Probability Theory",
    "text": "Probability Theory\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(P(x)\\)\nA probability distribution over a discrete variable.\n\n\n\\(p(x)\\)\nA probability distribution over a contiuous variable or over a variable whose type has not been specified.\n\n\n\\(\\mathbb{E}_{x \\sim P} [ f(x) ]\\text{ or } \\mathbb{E} f(x)\\)\nExpectation of \\(f(x)\\) with respect to \\(P(x)\\)\n\n\n\\(\\mathcal{N} ( \\mathbf{x} ; \\mu , \\Sigma)\\)\nGaussian distribution over \\(\\mathbf{x}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\)\n\n\n\\(x \\sim \\mathcal{N} (\\mu , \\sigma)\\)\nGaussian distribution over \\(x\\) with mean \\(\\mu\\) and variance \\(\\sigma\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#calculus",
    "href": "pages/notation.html#calculus",
    "title": "",
    "section": "Calculus",
    "text": "Calculus\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\nabla_{\\mathbf{w}} J\\)\nGradient of \\(J\\) with respect to \\(\\mathbf{w}\\)\n\n\n\\(\\frac{\\partial J}{\\partial w}\\)\nPartial derivative of \\(J\\) with respect to \\(w\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#functions",
    "href": "pages/notation.html#functions",
    "title": "",
    "section": "Functions",
    "text": "Functions\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\log x\\)\nThe natural logarithm of \\(x\\).\n\n\n\\(\\lVert \\mathbf{x} \\rVert_p\\)\n\\(L^p\\) norm of \\(\\mathbf{x}\\)\n\n\n\\(\\lVert \\mathbf{x} \\rVert\\)\n\\(L^2\\) norm of \\(\\mathbf{x}\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#deep-learning",
    "href": "pages/notation.html#deep-learning",
    "title": "",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\nNCHW\nThe input format of images in PyTorch. N: number of images (batch size), C: number of channels, H: height, W: width",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "pages/object_detection.html",
    "href": "pages/object_detection.html",
    "title": "6 - Object Detection",
    "section": "",
    "text": "Object detection is a core task of computer vision. In object detection, the goal is to localize and classify all objects (from a set of known object classes) in an image. Figure 1 shows an example from the paper by Redmon et al. (2016b). Each object is localized with a bounding box and assigned an object class. A bounding box is defined by four parameters: \\(x, y\\), height, and width.\n\n\n\n\n\n\nFigure 1: Object detection example (from Redmon et al. (2016b)). Bounding boxes localize the objects, with the most probable class and confidence for each object.\n\n\n\nFigure 2 illustrates the differences between image classification, classification with localization, and object detection.\n\n\n\n\n\n\nFigure 2: Classification and detection (from Austin et al. (2022)).\n\n\n\nWe will now look step-by-step at how to go from image classification to object detection. First, we will look at landmark detection. In this step, we want to localize specific points in an image or object. This could be the nose, eyes, etc., of a person. Figure 3 shows an example of landmark detection: On the one hand, we want to determine which object class is in the image, and on the other hand, we want to determine the position of the nose. If there are 3 classes, as shown in the example, the network has 3 outputs (logits) \\(C1, C2, C3\\), which are converted into a probability distribution via the softmax transformation.\nThe question now is: How can I additionally localize the nose?\n\n\n\n\n\n\nFigure 3: Landmark detection (from Austin et al. (2022)).\n\n\n\nFigure 4 shows that a simple extension of the network output by 2 scalars is sufficient. This can be used to model the \\(x\\) and \\(y\\) coordinates of the nose. The coordinates could be defined relative to the entire image in the range \\(x,y \\in [0,1]\\).\n\n\n\n\n\n\nFigure 4: Landmark detection (from Austin et al. (2022)).\n\n\n\nIn the next step, we can go from landmark detection to classification with localization. Figure 5 shows the problem. Now, we want to classify an image and simultaneously localize the object. In addition to \\(x,y\\) coordinates, further outputs need to be defined.\n\n\n\n\n\n\nFigure 5: Classification and localization (from Austin et al. (2022)).\n\n\n\nFigure 6 illustrates that with two additional outputs, a bounding box can be defined, which specifies height, width, and a corner point (or the center).\n\n\n\n\n\n\nFigure 6: Classification and localization (from Austin et al. (2022)).\n\n\n\nFigure 7 shows how to modify a CNN to localize and classify a single object. One could add two outputs (heads) to the CNN backbone: a classification head that models the probability of the object class via softmax transformation and 4 parameters for the bounding box coordinates, which could be optimized with a regression loss, such as Euclidean distance. Thus, two tasks (localization and classification) would be solved simultaneously (multitask loss).\n\n\n\n\n\n\nFigure 7: Source: Johnson (2019)."
  },
  {
    "objectID": "pages/object_detection.html#r-cnn-region-based-cnn",
    "href": "pages/object_detection.html#r-cnn-region-based-cnn",
    "title": "6 - Object Detection",
    "section": "R-CNN: Region-Based CNN",
    "text": "R-CNN: Region-Based CNN\nR-CNN (Regions with CNN Features) was published in 2014 Girshick et al. (2014). A generic region proposal method is applied to a given image. The idea behind region proposals is to find good candidates for bounding boxes (regions) that possibly contain an object. This would significantly reduce the effort to classify regions (compared to the sliding window).\nA well-known algorithm for identifying possible objects is selective search (Uijlings et al. (2013)). This identifies object candidates based on regions with similar color, texture, or shape. Selective search can be run on the CPU and finds many, e.g., 2000 regions for an image in a few seconds. Figure 11 shows an example of applying this algorithm to an image.\n\n\n\n\n\n\nFigure 11: Left: Results of selective search (at different scales), right: Object hypotheses. Source: Uijlings et al. (2013).\n\n\n\nThen, each of these regions is aligned in dimensionality (warping) so that all ROIs have the same spatial dimensions. This is necessary so that they can be processed with the same CNN (batch-wise). These warped ROIs are then individually classified with a CNN. Figure 12 illustrates the process.\n\n\n\n\n\n\nFigure 12: Source: Girshick et al. (2014).\n\n\n\nAdditionally, the region proposals are improved by learning a bounding box regression. Figure 13 shows the entire process.\n\n\n\n\n\n\nFigure 13: Source: Johnson (2019).\n\n\n\n\n\n\n\n\n\nNote\nBounding box regression models a transformation of the ROIs. The transformation is parameterized by four numbers \\((t_x, t_y, t_h, t_w)\\), just like the ROI proposals \\((p_x, p_y, p_h, p_w)\\). The predicted bounding box is then \\((b_x, b_y, b_h, b_w)\\). The position and extent are modeled as follows:\n\\[\\begin{equation}\nb_x = p_x + p_w t_x \\\\\nb_y = p_y + p_h t_y \\\\\nb_w = p_w \\cdot e^{t_w} \\\\\nb_h = p_h \\cdot e^{t_h}\n\\end{equation}\\]\nThe position of the box is modeled scale-invariant (relative to width/height). Width/height are modeled in log-space, so only valid values are possible (negative would not be possible).\nThe individual transformations \\(t_*\\) are modeled with a ridge regression, which uses the ROI features \\(x\\) as input, where \\(i\\) indexes individual proposals.\n\\[\\begin{equation}\nJ(w) = \\sum_i (t_*^i - w_*^T x_i)^2 + \\lambda \\lVert w_* \\rVert^2\n\\end{equation}\\]\nFigure 14 illustrates bounding-box regression with an example.\n\n\n\n\n\n\nFigure 14: Source: Johnson (2022).\n\n\n\n\n\n\nR-CNN optimizes cross-entropy for classification and least squares for bounding box coordinates."
  },
  {
    "objectID": "pages/object_detection.html#fast-r-cnn",
    "href": "pages/object_detection.html#fast-r-cnn",
    "title": "6 - Object Detection",
    "section": "Fast R-CNN",
    "text": "Fast R-CNN\nR-CNN is very slow because a forward pass through the CNN is required for each region proposal. The follow-up paper to R-CNN Girshick (2015) changed the method somewhat. Instead of processing each region proposal separately, the entire image is processed once with a CNN (feature extraction) to obtain activation maps that are somewhat reduced in spatial dimension (see Figure 15).\n\n\n\n\n\n\nFigure 15: Source: Johnson (2019).\n\n\n\nThen, the region proposals generated by a separate method (e.g., selective search) are projected onto the extracted activation maps (see Figure 16).\n\n\n\n\n\n\nFigure 16: Source: Johnson (2022).\n\n\n\nNext, the extracted features are warped and processed through a small network of region of interest pooling and fully connected layers (see Figure 17).\n\n\n\n\n\n\nFigure 17: Source: Johnson (2022).\n\n\n\nFinally, a classification and adjustment of the region of interest are output. Figure 18 shows the entire architecture and the losses. During model training, classification loss and bounding box regression loss can be calculated on these outputs.\n\n\n\n\n\n\nFigure 18: Source: Johnson (2019).\n\n\n\nFigure 19 shows the architecture of Fast R-CNN with a ResNet backbone.\n\n\n\n\n\n\nFigure 19: Source: Johnson (2019).\n\n\n\nThe region proposals from the proposal method must be projected onto the activation maps. An important innovation of Fast R-CNN was ROI pooling, see Figure 20. Here, the spatial dimension is reduced, for example, with max pooling so that all ROIs have the same dimensionality. This is necessary so that all can be further processed with the same network for classification and regression.\n\n\n\n\n\n\nFigure 20: Source: Johnson (2019).\n\n\n\nFigure 21 shows the training and test time for the model, respectively for a single image. In Fast-RCNN, the test time is dominated by the region proposals generated by a separate method.\n\n\n\n\n\n\nFigure 21: Source: Johnson (2019).\n\n\n\n\n\n\n\n\n\nNote\nThe loss function of Fast R-CNN is as follows:\n\\[\\begin{equation}\nL(p, u, t^u, v) = L_{\\text{cls}}(p, u) + \\lambda \\lbrack u &gt; 1 \\rbrack L_{\\text{loc}}(t^u, v)\n\\end{equation}\\]\nwhere \\(u\\) represents the true class, \\(p\\) the modeled probability for \\(u\\). \\(t^u\\) are the modeled bounding box coordinates (a tuple with 4 numbers) for the class \\(u\\), and \\(v\\) are the true bounding box coordinates for the class \\(u\\). \\(L_{\\text{cls}}(p, u)\\) is the cross-entropy loss. \\(L_{\\text{loc}}(t^u, v)\\) is only evaluated for non-background classes with \\(\\lbrack u &gt; 1 \\rbrack\\), as there is no bounding box for the background class. \\(L_{\\text{loc}}(t^u, v)\\) is a slightly modified \\(L_1\\) loss (absolute distance).\nFor bounding-box regression, a smooth-L1 loss is used.\n\\[\\begin{equation}\nL_{\\text{loc}}(t^{u}, v) = \\sum_{i \\in \\{x,y,w,h\\}} \\text{smooth}_{L_1}(t^{u}_{i} - v_{i}),\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{smooth}_{L_1}(x) =\n\\begin{cases}\n0.5x^2 & \\text{if } |x| &lt; 1 \\\\\n|x| - 0.5 & \\text{otherwise},\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "pages/object_detection.html#faster-r-cnn",
    "href": "pages/object_detection.html#faster-r-cnn",
    "title": "6 - Object Detection",
    "section": "Faster R-CNN",
    "text": "Faster R-CNN\nWith Faster R-CNN Ren et al. (2017), the R-CNN family was further improved. In particular, the generation of region proposals was integrated into the method by creating them with a Region Proposal Network (RPN). In line with the end-to-end learning principle, the aim was to use as few heuristics as possible, such as selective search.\n\n\n\n\n\n\nFigure 22: Source: Ren et al. (2017)\n\n\n\nThe rest of the architecture corresponds to Fast R-CNN. Figure 22 shows the architecture.\nThe RPN generates object proposals (bounding boxes) in a sliding window approach (implemented as a convolution) on the activation maps of the backbone CNN. These proposals are locations where an object is likely to be found. Figure 23 illustrates an example image (left) with dimensions \\(3 \\times 640 \\times 480\\), the resolution of the activation map (right) with \\(512 \\times 5 \\times 4\\) on which the RPN operates. Each point/grid cell represents the spatial coverage on the input image. It becomes apparent that the spatial resolution has been reduced by the CNN backbone (e.g., with pooling layers or convolutions with stride \\(&gt;2\\)). It is illustrated that the activation map (in this case) has 512 channels, i.e., complex and rich features that represent each region defined by the grid cells. As a comparison: In the paper by Ren et al. (2017), they write that an image with a spatial resolution of \\(1000 \\times 600\\) results in activation maps with \\(60 \\times 40\\) resolution.\n\n\n\n\n\n\nFigure 23: Source: Johnson (2022).\n\n\n\nThe RPN now models whether an object is present at each point/for each grid cell and whether a correction of a reference bounding box is necessary. Figure 24 illustrates the reference box (blue) for one point. In this case, there is no object nearby. This reference box is also called an anchor.\n\n\n\n\n\n\nFigure 24: Source: Johnson (2022).\n\n\n\nIn Figure 25, you see a positive (green) anchor (with an object) and a negative (red) one without an object. The RPN models an objectness score that is high for positive anchors and low for negatives.\n\n\n\n\n\n\nFigure 25: Source: Johnson (2022).\n\n\n\nIn addition to objectness scores, transformations for the anchors are also modeled (bounding box regression) so that they fully cover the object. During RPN training, transformations for the positive anchors (green box) are calculated/modelled relative to the ground truth box (orange).\n\n\n\n\n\n\nFigure 26: Source: Johnson (2022).\n\n\n\n\n\n\n\n\n\nQuestion:\nWhat happens if 2 or more objects are in the same place?\n\n\n\nIf two or more objects are in the same place, often the objects overlapping have a different shape. Figure 27 illustrates two objects with almost the same center but with significantly different bounding boxes. If anchor boxes with different aspect ratios are defined, e.g., for long and tall/narrow objects, this problem can be partially circumvented.\n\n\n\n\n\n\nFigure 27: Anchor boxes.\n\n\n\nFigure 28 shows that the RPN in Faster R-CNN models \\(k\\) anchors per location. This allows almost all possible objects, even those close to each other, to be assigned to an anchor\nand detected.\n\n\n\n\n\n\nFigure 28: Source: Johnson (2022).\n\n\n\nOverall, Faster R-CNN is trained with 4 different losses, as seen in Figure 29.\n\n\n\n\n\n\nFigure 29: Source: Johnson (2019).\n\n\n\nFaster R-CNN is a two-stage detector because the RPN is conceptually separated from the final classification/bounding box regression. In particular, the found regions of the RPN must be warped and arranged in a batch of samples so they can then be processed through the second stage.\n\n\n\n\n\n\nFigure 30: Source: Johnson (2019)."
  },
  {
    "objectID": "pages/object_detection.html#yolo",
    "href": "pages/object_detection.html#yolo",
    "title": "6 - Object Detection",
    "section": "YOLO",
    "text": "YOLO\nA well-known representative is YOLO (You Only Look Once) Redmon et al. (2016a) and its variants. An image is reduced to the spatial dimensionality of \\(SxS\\) with a CNN (see Figure 31).\n\n\n\n\n\n\nFigure 31: Source: Redmon et al. (2016a)\n\n\n\nThen, a classification is performed for each grid cell (value on the activation map) (see Figure 32).\n\n\n\n\n\n\nFigure 32: Source: Redmon et al. (2016a)\n\n\n\nFinally, for each grid cell, the following values are modeled for each of the \\(B\\) bounding boxes: a bounding box regression (4 parameters) and an object score that models whether the center of an object is in the grid cell.\n\n\n\n\n\n\nFigure 33: Source: Redmon et al. (2016a)\n\n\n\nThen, the classification and bounding box regression are merged. Figure 34 shows the whole picture.\n\n\n\n\n\n\nFigure 34: Source: Redmon et al. (2016a)\n\n\n\nThe full architecture of YOLO is shown in Figure 35. The output is a tensor of dimension \\((C+1)xKxHxW\\) for the classification of detections and a tensor of \\(Cx4KxHxW\\) for the bounding box regression, where \\(C\\) is the number of classes (\\(C+1\\) with included background class), \\(K\\) is the number of anchor boxes, and \\(HxW\\) is the spatial dimension of the output activation maps.\n\n\n\n\n\n\nFigure 35: Source: Redmon et al. (2016a)\n\n\n\nSince objects often occupy multiple grid cells, it may be that multiple cells detect the same object. Figure 36 shows an example where two cells detect the dog and create two bounding boxes accordingly. This is problematic because exactly one bounding box is needed per detection, and no duplicates.\n\n\n\n\n\n\nFigure 36: Inspired by Austin et al. (2022)\n\n\n\nWith non-max suppression, such duplicates can be avoided. Figure 37 shows the effect of NMS on our example image. More on NMS in {ref}non-max-suppression.\n\n\n\n\n\n\nFigure 37: Inspired by Austin et al. (2022)\n\n\n\nAnother difficulty is detecting objects that have their center in the same grid cell. Therefore, YOLO uses \\(B\\) bounding boxes per cell. This allows \\(B\\) objects per cell to be detected, as illustrated in Figure 38. A better variant is to work with anchor boxes.\n\n\n\n\n\n\nFigure 38: Source: Austin et al. (2022)\n\n\n\n\n\n\n\n\n\nNote\nThe cost function of YOLO is shown below. \\(\\lambda\\) weights are for the different terms, and \\(\\mathbb{1}\\) turns certain loss terms on and off, depending on whether an object is present or not. \\(\\hat{C}_i\\) models the IoU for the predicted box, and \\(\\hat{p}_i(c)\\) the presence of class \\(c\\) in a grid cell. It is interesting that the classification part is penalized with a squared error and not, for example, with a cross-entropy loss.\n\\[\\begin{align}\n& \\lambda_{coord} \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}[(x_i-\\hat{x}_i)^2 + (y_i-\\hat{y}_i)^2 ] + \\lambda_{coord} \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2 +(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2 ]\\\\\n& + \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}(C_i - \\hat{C}_i)^2 + \\lambda_{noobj}\\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{noobj}(C_i - \\hat{C}_i)^2 \\\\\n& + \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj}\\sum_{c \\in classes}(p_i(c) - \\hat{p}_i(c))^2\n\\end{align}\\]\n\n\n\nYOLO did not work with anchors. More modern versions of single-stage detectors sometimes use anchors, just like many two-stage detectors. This allows a single-stage detector to directly output a tensor of dimensions \\((C+1)xKxHxW\\) for classification of detections and a tensor of \\(Cx4KxHxW\\) for bounding box regression, where \\(C\\) is the number of classes (\\(C+1\\) with included background class), \\(K\\) is the number of anchor boxes, and \\(HxW\\) is the spatial dimension of the output activation maps. Figure 39 illustrates single-shot detection.\n\n\n\n\n\n\nFigure 39: Source: Johnson (2019)."
  },
  {
    "objectID": "pages/object_detection.html#centernet---objects-as-points",
    "href": "pages/object_detection.html#centernet---objects-as-points",
    "title": "6 - Object Detection",
    "section": "CenterNet - Objects as Points",
    "text": "CenterNet - Objects as Points\nA modern representative of single-shot detectors is CenterNet Zhou, Wang, and Krähenbühl (2019). CenterNet divides an image into a finer grid compared to YOLO. The global stride is about 4, meaning the spatial resolution of the grid is 4 times smaller than that of the image. CenterNet assigns each object, according to its center, a grid point. Then, all object properties, such as the bounding box coordinates, are modeled based on the features from the center. Figure 40 illustrates various objects, their centers, and their height and width, which are modeled.\n\n\n\n\n\n\nFigure 40: Source: Zhou, Wang, and Krähenbühl (2019).\n\n\n\nThe output of CenterNet is a keypoint heatmap \\(\\hat{Y} \\in [0, 1]^{\\frac{W}{R} \\times \\frac{H}{R} \\times C}\\), where \\(H, W\\) are the spatial resolution of the image, \\(R\\) the global stride, and \\(C\\) the number of object classes to be detected. \\(\\hat{Y}_{x, y, c} = 1\\) corresponds to a keypoint (center of an object in object detection), \\(\\hat{Y}_{x, y, c} = 0\\) corresponds to the background. Additionally, an offset is modeled: the deviation of the object center from the center of the grid cell: \\(\\hat{O} \\in \\mathbb{R}^{\\frac{W}{R} \\times \\frac{H}{R} \\times 2}\\), and the bounding box coordinates (height/width): \\(\\hat{S} \\in \\mathbb{R}^{\\frac{W}{R} \\times \\frac{H}{R} \\times 2}\\).\nFigure 41 illustrates the keypoint heatmap, the offset prediction, and the object size with an example.\n\n\n\n\n\n\nFigure 41: Source: Zhou, Wang, and Krähenbühl (2019).\n\n\n\nFigure 42 contrasts anchor-based methods with center points. Anchors are divided into positive (green) and negative (red), or ignored (gray), depending on the overlap with ground truth objects. These are then used in model training. CenterNet does not use anchors and thus does not need manually selected positive and negative anchors. Non-max suppression is also unnecessary. This results in fewer manual hyperparameters and heuristics.\n\n\n\n\n\n\nFigure 42: Source: Zhou, Wang, and Krähenbühl (2019).\n\n\n\n\n\n\n\n\n\nQuestion\nWhat could be an inherent limitation of CenterNet?"
  },
  {
    "objectID": "pages/object_detection.html#class-imbalance",
    "href": "pages/object_detection.html#class-imbalance",
    "title": "6 - Object Detection",
    "section": "Class Imbalance",
    "text": "Class Imbalance\nAn important topic is class imbalance when training models: Often, there is much more background than object classes (e.g., a ratio of 1:1000). This can lead to problems during learning, as the model may have a strong bias towards the background class, and the gradient during model training may be dominated by simple predictions (for the background class). In this context, the focal loss is an important milestone Lin et al. (2018). This reduces the loss for simple samples and increases the relative loss for difficult samples. Figure 43 shows the effect of focal loss (compared to cross-entropy) for different values of the parameter \\(\\gamma\\), which regulates the strength of the focal loss.\n\n\n\n\n\n\nFigure 43: Source: Lin et al. (2018)."
  },
  {
    "objectID": "pages/object_detection.html#feature-pyramid-networks",
    "href": "pages/object_detection.html#feature-pyramid-networks",
    "title": "6 - Object Detection",
    "section": "Feature Pyramid Networks",
    "text": "Feature Pyramid Networks\nOne challenge in object detection is the different spatial scaling of various objects. Both relatively small and relatively large objects need to be detected. Global features that provide important contextual information are helpful for classifying objects. Additionally, fine (pixel-accurate), more local features are important for accurately modeling bounding boxes. An innovation is feature pyramid networks (FPNs) Lin et al. (2017). This approach laterally combines and aggregates features from different layers. This allows global and local information to be combined. Additionally, it is possible to model objects of different sizes on different layers in the network. Figure 44 shows how features are increasingly condensed (left) as the spatial resolution decreases. You can see (right) that global information flows back to deeper levels, allowing smaller objects to be better classified with global information.\n\n\n\n\n\n\nFigure 44: Source: Lin et al. (2017)."
  },
  {
    "objectID": "pages/object_detection.html#transformers",
    "href": "pages/object_detection.html#transformers",
    "title": "6 - Object Detection",
    "section": "Transformers",
    "text": "Transformers\nA newer architecture is the transformer. This was successfully used in natural language processing (NLP) and has also taken an important place in object detection. With a transformer, object detection can be reformulated as a set-prediction problem Carion et al. (2020). This allows object detection to be trained end-to-end, and all objects can be detected in a single forward pass. This makes hand-designed features like anchor boxes or heuristics like non-max suppression unnecessary. Many state-of-the-art models are now based on the transformer architecture, although CNN-based models are still well represented.\n\n\n\n\n\n\nFigure 45: Source: Carion et al. (2020)."
  },
  {
    "objectID": "pages/object_detection.html#intersection-over-union-iou",
    "href": "pages/object_detection.html#intersection-over-union-iou",
    "title": "6 - Object Detection",
    "section": "Intersection over Union (IoU)",
    "text": "Intersection over Union (IoU)\nIntersection over Union (IoU) is a metric to compare two bounding boxes. Figure 46, Figure 47, and Figure 48 illustrate the concept. An IoU \\(&gt; 0.5\\) is usually considered just acceptable.\n\n\n\n\n\n\nFigure 46: Source: Johnson (2022).\n\n\n\n\n\n\n\n\n\nFigure 47: Source: Johnson (2022).\n\n\n\n\n\n\n\n\n\nFigure 48: Source: Johnson (2022).\n\n\n\n\nNon-Max Suppression\nIn many methods, such as Faster R-CNN, the same objects can be detected multiple times. Therefore, potential duplicates must be eliminated during test time (inference or applying the model to an image). In practice, such cases are often resolved with non-max suppression (NMS).\nThen, using the following heuristic (NMS), duplicates can be removed:\n\nSelect the box with the highest score (probability of a certain class (excluding background))\nEliminate boxes with a lower score that have an IoU \\(&gt; \\epsilon\\), where \\(\\epsilon\\) is an arbitrary threshold (e.g., 0.7).\nRepeat until no overlapping boxes remain.\n\nFigure 49 illustrates NMS with an example:\n\n\n\n\n\n\nFigure 49: Source: Johnson (2019).\n\n\n\nThis becomes problematic when many objects are densely packed or there is a lot of overlap, as shown in Figure 50. Here, many valid objects would be removed.\n\n\n\n\n\n\nFigure 50: Source"
  },
  {
    "objectID": "pages/object_detection.html#mean-average-precision-map",
    "href": "pages/object_detection.html#mean-average-precision-map",
    "title": "6 - Object Detection",
    "section": "Mean Average Precision (mAP)",
    "text": "Mean Average Precision (mAP)\nEvaluating object detection models is not easy. The most commonly used metric is mean average precision (mAP).\nThe following metrics are important for understanding mAP. These are based on the confusion matrix, which can be created for all classes. Figure 51 shows a confusion matrix.\n\n\n\n\n\n\nFigure 51: Source\n\n\n\nPrecision is the proportion of positively classified samples that are actually positive:\n\\[\\begin{equation}\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\end{equation}\\]\nRecall is the proportion of positive samples that were correctly identified as such:\n\\[\\begin{equation}\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\end{equation}\\]\nAverage precision is the area under the precision/recall curve for a particular class. All detections of this class are sorted in descending order of their confidence (class score), and precision and recall are calculated after each sample. It is determined how many detections were correct and had a minimum IoU with a ground-truth box. These points can then be plotted. Figure 52 shows the calculation of average precision with an example.\n\n\n\n\n\n\nFigure 52: Source: Johnson (2019).\n\n\n\nMean average precision (mAP) is the average of all average precisions across all classes. Sometimes, an average over different IoU thresholds is also calculated, which a model must achieve for a hit.\nMore details can be found in this blog: Link"
  },
  {
    "objectID": "slides/cnns.html#overview",
    "href": "slides/cnns.html#overview",
    "title": "Convolutional Neural Networks",
    "section": "Overview",
    "text": "Overview\n\nIntroduction & Motivation\nConvolutional Layers\nProperties\nVariants and Layers\nVisualizations and Architectures"
  },
  {
    "objectID": "slides/cnns.html#multilayer-perceptron",
    "href": "slides/cnns.html#multilayer-perceptron",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\n\n\n\nSource: Li (2022)"
  },
  {
    "objectID": "slides/cnns.html#mlps-on-images",
    "href": "slides/cnns.html#mlps-on-images",
    "title": "Convolutional Neural Networks",
    "section": "MLPs on Images",
    "text": "MLPs on Images\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#mlps-on-images-1",
    "href": "slides/cnns.html#mlps-on-images-1",
    "title": "Convolutional Neural Networks",
    "section": "MLPs on Images",
    "text": "MLPs on Images\n\n\n\nSource: Li (2023)"
  },
  {
    "objectID": "slides/cnns.html#cnns",
    "href": "slides/cnns.html#cnns",
    "title": "Convolutional Neural Networks",
    "section": "CNNs",
    "text": "CNNs\n\n\n\nThe activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: Li (2022)"
  },
  {
    "objectID": "slides/cnns.html#convolution",
    "href": "slides/cnns.html#convolution",
    "title": "Convolutional Neural Networks",
    "section": "Convolution?",
    "text": "Convolution?\nConvolution in Deep Learning is typically implemented as cross-correlation.\n\\[\\begin{equation}\nS(i, j) = (K * I)(i, j) = \\sum_m \\sum_n I(i + m, j + n) K(m, n)\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-1",
    "href": "slides/cnns.html#convolutional-layers-1",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-2",
    "href": "slides/cnns.html#convolutional-layers-2",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-3",
    "href": "slides/cnns.html#convolutional-layers-3",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-4",
    "href": "slides/cnns.html#convolutional-layers-4",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-5",
    "href": "slides/cnns.html#convolutional-layers-5",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#hyper-parameters",
    "href": "slides/cnns.html#hyper-parameters",
    "title": "Convolutional Neural Networks",
    "section": "Hyper-Parameters",
    "text": "Hyper-Parameters\nConvolutional Layers are parameterized:\n\nDepth: How many activation maps?\nPadding: How much padding is added to the input?\nStride: What is the step size of the convolution?\nKernel-Size: What is the kernel size?"
  },
  {
    "objectID": "slides/cnns.html#padding-why",
    "href": "slides/cnns.html#padding-why",
    "title": "Convolutional Neural Networks",
    "section": "Padding: Why?",
    "text": "Padding: Why?\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#padding",
    "href": "slides/cnns.html#padding",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nLeft: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output."
  },
  {
    "objectID": "slides/cnns.html#padding-1",
    "href": "slides/cnns.html#padding-1",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nLeft: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output."
  },
  {
    "objectID": "slides/cnns.html#padding-and-stride",
    "href": "slides/cnns.html#padding-and-stride",
    "title": "Convolutional Neural Networks",
    "section": "Padding and Stride",
    "text": "Padding and Stride\n\n\n\nStride with Padding. Red indicates the position of the corresponding filter value on the input activations."
  },
  {
    "objectID": "slides/cnns.html#padding-and-stride-animations",
    "href": "slides/cnns.html#padding-and-stride-animations",
    "title": "Convolutional Neural Networks",
    "section": "Padding and Stride: Animations",
    "text": "Padding and Stride: Animations\nDumoulin and Visin (2016) has created some animations to better understand convolutions, available here: Link."
  },
  {
    "objectID": "slides/cnns.html#calculations",
    "href": "slides/cnns.html#calculations",
    "title": "Convolutional Neural Networks",
    "section": "Calculations",
    "text": "Calculations\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)"
  },
  {
    "objectID": "slides/cnns.html#calculations-1",
    "href": "slides/cnns.html#calculations-1",
    "title": "Convolutional Neural Networks",
    "section": "Calculations",
    "text": "Calculations\nThis formula covers all scenarios!\nSize of Activation Map\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/cnns.html#quiz",
    "href": "slides/cnns.html#quiz",
    "title": "Convolutional Neural Networks",
    "section": "Quiz",
    "text": "Quiz\nScenario:\n\nInput: 3 x 32 x 32\nConvolution: 10 filters with 5x5 kernel size, stride=1, pad=2\n\nWhat is the size of the activation map?\nHow many weights are there?\nSize of Activation Map\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/cnns.html#sparse-connectivity-and-parameter-sharing",
    "href": "slides/cnns.html#sparse-connectivity-and-parameter-sharing",
    "title": "Convolutional Neural Networks",
    "section": "Sparse Connectivity and Parameter Sharing",
    "text": "Sparse Connectivity and Parameter Sharing\nLocal (Sparse) Connectivity: Neurons are only locally connected.\nParameter Sharing: Weights of a neuron are applied locally but are the same across the entire input."
  },
  {
    "objectID": "slides/cnns.html#convolution-is-parameter-sharing-always-useful",
    "href": "slides/cnns.html#convolution-is-parameter-sharing-always-useful",
    "title": "Convolutional Neural Networks",
    "section": "Convolution: Is Parameter Sharing Always Useful?",
    "text": "Convolution: Is Parameter Sharing Always Useful?\nQuestion: Is parameter sharing always useful?"
  },
  {
    "objectID": "slides/cnns.html#mlp-parameters",
    "href": "slides/cnns.html#mlp-parameters",
    "title": "Convolutional Neural Networks",
    "section": "MLP Parameters",
    "text": "MLP Parameters\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)\n        self.hidden_layer2 = nn.Linear(64, 32)\n        self.output_layer = nn.Linear(32, 10)\n     \n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.hidden_layer1(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x\n\nnet = MLP()\nprint(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n=========================================================================================="
  },
  {
    "objectID": "slides/cnns.html#cnn-parameters",
    "href": "slides/cnns.html#cnn-parameters",
    "title": "Convolutional Neural Networks",
    "section": "CNN Parameters",
    "text": "CNN Parameters\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.output_layer = nn.Linear(16 * 8 * 8, 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.flatten(x)\n        x = self.output_layer(x)\n        return x\n\ncnn = CNN()\nprint(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n=========================================================================================="
  },
  {
    "objectID": "slides/cnns.html#quiz-linear-transformation-vs-convolution",
    "href": "slides/cnns.html#quiz-linear-transformation-vs-convolution",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Linear Transformation vs Convolution",
    "text": "Quiz: Linear Transformation vs Convolution\n\n\n\nInput in 2-D (top left), the flat version (bottom left), expected output (right), and unknown transformation (center)."
  },
  {
    "objectID": "slides/cnns.html#translation-invariance-equivariance",
    "href": "slides/cnns.html#translation-invariance-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Translation Invariance / Equivariance",
    "text": "Translation Invariance / Equivariance\nGiven a translation \\(g()\\), which spatially shifts inputs:\n\nTranslation invariance: \\(f(g(x))=f(x)\\)\nTranslation equivariance: \\(f(g(x))=g(f(x))\\)\n\nConvolutions are translation equivariant: Example Video"
  },
  {
    "objectID": "slides/cnns.html#stacking-convolutions",
    "href": "slides/cnns.html#stacking-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Stacking Convolutions",
    "text": "Stacking Convolutions\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#receptive-field",
    "href": "slides/cnns.html#receptive-field",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#receptive-field-1",
    "href": "slides/cnns.html#receptive-field-1",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#dilated-convolutions",
    "href": "slides/cnns.html#dilated-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Dilated Convolutions",
    "text": "Dilated Convolutions\n\n\n\nConvolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1."
  },
  {
    "objectID": "slides/cnns.html#dilated-convolutions-1",
    "href": "slides/cnns.html#dilated-convolutions-1",
    "title": "Convolutional Neural Networks",
    "section": "Dilated Convolutions",
    "text": "Dilated Convolutions\n\n\n\nConvolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2."
  },
  {
    "objectID": "slides/cnns.html#x1-convolutions",
    "href": "slides/cnns.html#x1-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "1x1 Convolutions",
    "text": "1x1 Convolutions\n\n\n\nSource: Johnson (2019)"
  },
  {
    "objectID": "slides/cnns.html#depthwise-separable-convolutions",
    "href": "slides/cnns.html#depthwise-separable-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Depthwise Separable Convolutions",
    "text": "Depthwise Separable Convolutions\n\n\n\nSource: https://paperswithcode.com/method/depthwise-convolution"
  },
  {
    "objectID": "slides/cnns.html#depthwise-separable-convolutions-1",
    "href": "slides/cnns.html#depthwise-separable-convolutions-1",
    "title": "Convolutional Neural Networks",
    "section": "Depthwise Separable Convolutions",
    "text": "Depthwise Separable Convolutions\n\n\n\nSource: Yu and Koltun (2016)"
  },
  {
    "objectID": "slides/cnns.html#pooling-layers",
    "href": "slides/cnns.html#pooling-layers",
    "title": "Convolutional Neural Networks",
    "section": "Pooling Layers",
    "text": "Pooling Layers\n\nSource: Li (2022)"
  },
  {
    "objectID": "slides/cnns.html#max-pooling",
    "href": "slides/cnns.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max Pooling",
    "text": "Max Pooling\n\n\n\nMax pooling, input (left) and output (right)."
  },
  {
    "objectID": "slides/cnns.html#average-pooling",
    "href": "slides/cnns.html#average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Average Pooling",
    "text": "Average Pooling\n\n\n\nAverage pooling, input (left) and output (right)."
  },
  {
    "objectID": "slides/cnns.html#other-pooling-layers",
    "href": "slides/cnns.html#other-pooling-layers",
    "title": "Convolutional Neural Networks",
    "section": "Other Pooling Layers",
    "text": "Other Pooling Layers\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers."
  },
  {
    "objectID": "slides/cnns.html#global-average-pooling",
    "href": "slides/cnns.html#global-average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Global Average Pooling",
    "text": "Global Average Pooling\n\n\n\nGlobal Average pooling, input (left) and output (right)."
  },
  {
    "objectID": "slides/cnns.html#learned-filters",
    "href": "slides/cnns.html#learned-filters",
    "title": "Convolutional Neural Networks",
    "section": "Learned Filters",
    "text": "Learned Filters\n\n\n\nSource: Krizhevsky, Sutskever, and Hinton (2012)"
  },
  {
    "objectID": "slides/practical.html#leaky-abstraction",
    "href": "slides/practical.html#leaky-abstraction",
    "title": "Practical Considerations",
    "section": "Leaky Abstraction",
    "text": "Leaky Abstraction\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)"
  },
  {
    "objectID": "slides/practical.html#silent-failure",
    "href": "slides/practical.html#silent-failure",
    "title": "Practical Considerations",
    "section": "Silent Failure",
    "text": "Silent Failure\nTraining neural networks fails silently!"
  },
  {
    "objectID": "slides/practical.html#get-to-know-the-data",
    "href": "slides/practical.html#get-to-know-the-data",
    "title": "Practical Considerations",
    "section": "1) Get to Know the Data",
    "text": "1) Get to Know the Data\nThoroughly inspect the data!"
  },
  {
    "objectID": "slides/practical.html#camera-traps-errors",
    "href": "slides/practical.html#camera-traps-errors",
    "title": "Practical Considerations",
    "section": "Camera Traps: Errors",
    "text": "Camera Traps: Errors\n\n\n\nExamples of images from camera traps."
  },
  {
    "objectID": "slides/practical.html#camera-traps-difficulties",
    "href": "slides/practical.html#camera-traps-difficulties",
    "title": "Practical Considerations",
    "section": "Camera Traps: Difficulties",
    "text": "Camera Traps: Difficulties\n\n\n\nExamples of images from camera traps. Source: Beery, Van Horn, and Perona (2018)"
  },
  {
    "objectID": "slides/practical.html#rare-classes",
    "href": "slides/practical.html#rare-classes",
    "title": "Practical Considerations",
    "section": "Rare Classes",
    "text": "Rare Classes\n\n\n\nAn image of a serval. Below are the model confidences."
  },
  {
    "objectID": "slides/practical.html#multiple-classes",
    "href": "slides/practical.html#multiple-classes",
    "title": "Practical Considerations",
    "section": "Multiple Classes",
    "text": "Multiple Classes\n\nExamples of an image from a camera trap with different species."
  },
  {
    "objectID": "slides/practical.html#baselines-1",
    "href": "slides/practical.html#baselines-1",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEvaluation pipeline, metrics, experiment tracking, and baseline model."
  },
  {
    "objectID": "slides/practical.html#ml-process",
    "href": "slides/practical.html#ml-process",
    "title": "Practical Considerations",
    "section": "ML Process",
    "text": "ML Process\n\n\n\nThe components of a typical machine learning process. Source: Raschka and Mirjalili (2020)"
  },
  {
    "objectID": "slides/practical.html#experiment-tracking",
    "href": "slides/practical.html#experiment-tracking",
    "title": "Practical Considerations",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\n\n\n\nWeights and Biases experiment tracking."
  },
  {
    "objectID": "slides/practical.html#baselines-2",
    "href": "slides/practical.html#baselines-2",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEnsure reproducibility.\nimport torch\ntorch.manual_seed(0)"
  },
  {
    "objectID": "slides/practical.html#baselines-3",
    "href": "slides/practical.html#baselines-3",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nAvoid unnecessary techniques and complexities. Reduce error susceptibility."
  },
  {
    "objectID": "slides/practical.html#baselines-4",
    "href": "slides/practical.html#baselines-4",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nIf possible, use a human baseline. How good can the model be?"
  },
  {
    "objectID": "slides/practical.html#difficult-cases",
    "href": "slides/practical.html#difficult-cases",
    "title": "Practical Considerations",
    "section": "Difficult Cases",
    "text": "Difficult Cases\n\nAn image from a camera trap that is difficult to classify. Here, annotators had a 96.6% agreement with experts."
  },
  {
    "objectID": "slides/practical.html#baselines-5",
    "href": "slides/practical.html#baselines-5",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nTrain an input-independent baseline. Is the model learning anything at all?"
  },
  {
    "objectID": "slides/practical.html#baselines-6",
    "href": "slides/practical.html#baselines-6",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nOverfit the model on a batch of data. Does the optimization work?"
  },
  {
    "objectID": "slides/practical.html#baselines-7",
    "href": "slides/practical.html#baselines-7",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nVisualize what goes into the model. Is my preprocessing working?\ny_hat = model(x)"
  },
  {
    "objectID": "slides/practical.html#fixed-sample-segmentation-example",
    "href": "slides/practical.html#fixed-sample-segmentation-example",
    "title": "Practical Considerations",
    "section": "Fixed Sample: Segmentation Example",
    "text": "Fixed Sample: Segmentation Example\n\n\n\nExample of a segmentation problem: input on the left and output on the right."
  },
  {
    "objectID": "slides/practical.html#overfit-1",
    "href": "slides/practical.html#overfit-1",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nAt this point, you should have a good understanding of the dataset, high confidence in the evaluation pipeline, and initial baselines from simple models. Now, look for a model that performs well on the training set."
  },
  {
    "objectID": "slides/practical.html#overfit-2",
    "href": "slides/practical.html#overfit-2",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nLook for a good model architecture. Follow the principle “Don’t be a hero”. Prefer already implemented/established architectures."
  },
  {
    "objectID": "slides/practical.html#regularization-1",
    "href": "slides/practical.html#regularization-1",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAt this point, you should have achieved good performance on the training set. Now, focus on the validation set."
  },
  {
    "objectID": "slides/practical.html#regularization-2",
    "href": "slides/practical.html#regularization-2",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nThe simplest measure to achieve better performance (and also reduce overfitting) is to collect more training data. However, this is often expensive!"
  },
  {
    "objectID": "slides/practical.html#learning-curve",
    "href": "slides/practical.html#learning-curve",
    "title": "Practical Considerations",
    "section": "Learning Curve",
    "text": "Learning Curve\nIs it worth collecting more data?\n\nExample of a learning curve. X-axis: Performance, Y-axis: Number of training samples. Left panel with Gaussian Naive Bayes and right panel with Support Vector Classifier."
  },
  {
    "objectID": "slides/practical.html#regularization-3",
    "href": "slides/practical.html#regularization-3",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAnother possibility is data augmentation. New data points are generated from existing ones by making random changes to the data. Typically, data points are augmented on-the-fly."
  },
  {
    "objectID": "slides/practical.html#data-augmentation-augly",
    "href": "slides/practical.html#data-augmentation-augly",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Augly",
    "text": "Data Augmentation: Augly\n\n\n\nAugLy"
  },
  {
    "objectID": "slides/practical.html#data-augmentation-albumentations",
    "href": "slides/practical.html#data-augmentation-albumentations",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Albumentations",
    "text": "Data Augmentation: Albumentations\n\nAlbumentations"
  },
  {
    "objectID": "slides/practical.html#data-augmentation-kornia",
    "href": "slides/practical.html#data-augmentation-kornia",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Kornia",
    "text": "Data Augmentation: Kornia\n\n\n\nKornia"
  },
  {
    "objectID": "slides/practical.html#data-augmentation-example",
    "href": "slides/practical.html#data-augmentation-example",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Example",
    "text": "Data Augmentation: Example\n\n\n\nData augmentation example."
  },
  {
    "objectID": "slides/practical.html#data-augmentation-synthetic-data",
    "href": "slides/practical.html#data-augmentation-synthetic-data",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Synthetic Data",
    "text": "Data Augmentation: Synthetic Data\n\n\n\nFrom Beery et al. (2020). Synthetic and semi-synthetic data."
  },
  {
    "objectID": "slides/practical.html#regularization-4",
    "href": "slides/practical.html#regularization-4",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith early stopping, a model is trained and periodically evaluated on a validation set, e.g., after each epoch. Training is stopped if no significant improvement is achieved after x evaluation cycles."
  },
  {
    "objectID": "slides/practical.html#early-stopping",
    "href": "slides/practical.html#early-stopping",
    "title": "Practical Considerations",
    "section": "Early Stopping",
    "text": "Early Stopping\n\n\n\nSource: Link"
  },
  {
    "objectID": "slides/practical.html#regularization-5",
    "href": "slides/practical.html#regularization-5",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nEarly stopping in PyTorch.\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)"
  },
  {
    "objectID": "slides/practical.html#regularization-6",
    "href": "slides/practical.html#regularization-6",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith weight decay, a model can be regularized. The update step in gradient descent is modified.\n\\[\\begin{equation}\n\\theta_{t+1} = \\theta_t (1 - \\lambda) - \\eta \\nabla J(\\theta)\n\\end{equation}\\]\nWhere \\(t\\) is the iteration, \\(\\theta\\) the model parameters, \\(\\eta\\) the learning rate, and \\(\\lambda\\) the decay parameter."
  },
  {
    "objectID": "slides/practical.html#regularization-7",
    "href": "slides/practical.html#regularization-7",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nTransfer learning involves adapting a pre-trained model on a large dataset (e.g., ImageNet) to a new task. The last layer is removed and replaced according to the new task. The network is then further trained. Layers can be frozen (weights not updated) or fine-tuned (weights further trained)."
  },
  {
    "objectID": "slides/practical.html#transfer-learning",
    "href": "slides/practical.html#transfer-learning",
    "title": "Practical Considerations",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nSource: Johnson and Fouhey (2021)"
  },
  {
    "objectID": "slides/practical.html#regularization-8",
    "href": "slides/practical.html#regularization-8",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nIn PyTorch, you can freeze the parameters:\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False"
  },
  {
    "objectID": "slides/practical.html#hyper-parameter-tuning",
    "href": "slides/practical.html#hyper-parameter-tuning",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nIn this step, different hyperparameters and architectures are systematically evaluated. Techniques such as grid search or random search can be used, with random search being preferred."
  },
  {
    "objectID": "slides/practical.html#hyper-parameter-tuning-1",
    "href": "slides/practical.html#hyper-parameter-tuning-1",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nParameterized architecture:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice",
    "href": "slides/practical.html#squeeze-out-the-juice",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nAfter finding the best architectures and hyperparameters, there are further ways to squeeze out more performance."
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice-1",
    "href": "slides/practical.html#squeeze-out-the-juice-1",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nModel ensembling."
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice-2",
    "href": "slides/practical.html#squeeze-out-the-juice-2",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nTrain longer."
  },
  {
    "objectID": "slides/practical.html#double-descent",
    "href": "slides/practical.html#double-descent",
    "title": "Practical Considerations",
    "section": "Double Descent",
    "text": "Double Descent\n\nSource: Nakkiran et al. (2019)"
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice-3",
    "href": "slides/practical.html#squeeze-out-the-juice-3",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nOther training techniques:\n\nSpecial optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing"
  },
  {
    "objectID": "slides/practical.html#huggingface",
    "href": "slides/practical.html#huggingface",
    "title": "Practical Considerations",
    "section": "HuggingFace",
    "text": "HuggingFace\nHuggingFace"
  },
  {
    "objectID": "slides/practical.html#timm",
    "href": "slides/practical.html#timm",
    "title": "Practical Considerations",
    "section": "timm",
    "text": "timm\nPyTorch Image Models (timm)"
  },
  {
    "objectID": "slides/practical.html#links",
    "href": "slides/practical.html#links",
    "title": "Practical Considerations",
    "section": "Links",
    "text": "Links\n\nDS-cookie cutter\nPyTorch Lightning\nHydra\nWeights & Biases\nNeptune AI\nVersion Control Systems for ML Projects"
  },
  {
    "objectID": "pages/demos.html",
    "href": "pages/demos.html",
    "title": "Demos",
    "section": "",
    "text": "CNN Filters Visualization\nVisualize filters and their effect of a pre-trained ResNet-18.\n\n  \n\n\n\nCLIP Demo\nThis model can calculate similarities between and among images and texts. It can be used for zero-shot (no labels) image classification.\n\n  \n\n\n\nVisual Question Answering\nTest a model that can answer questions given an image. The notebook contains a small model which can be run on cpu and a much larger model which ideally is run on GPU."
  },
  {
    "objectID": "slides_cas/cnns.html#overview",
    "href": "slides_cas/cnns.html#overview",
    "title": "Convolutional Neural Networks",
    "section": "Overview",
    "text": "Overview\n\nIntroduction & Motivation\nConvolutional Layers\nProperties\nVariants and Layers\nVisualizations and Architectures",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#properties-of-image-data",
    "href": "slides_cas/cnns.html#properties-of-image-data",
    "title": "Convolutional Neural Networks",
    "section": "Properties of Image Data",
    "text": "Properties of Image Data\n\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) (height, width) has = \\(150'528\\) values.\nLocality: Nearby pixels are statistically related\nStability under transformations: Interpretation of an image does not change under many geomoetric transformations.\n\n\nImage SourceHow does a Multilayer-Perceptron handle images?",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptron-and-images",
    "href": "slides_cas/cnns.html#multilayer-perceptron-and-images",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptron and Images",
    "text": "Multilayer-Perceptron and Images\n\nMLP on example image.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-high-dimensional-inputs",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-high-dimensional-inputs",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and High-Dimensional Inputs",
    "text": "Multilayer-Perceptrons and High-Dimensional Inputs\n\nMLP on example image.Dimensionality of weight matrix \\(\\mathbf{W}\\) scales with input size. \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\), while \\(d\\) the dimensionality of the inputs, and \\(k\\) the number of neurons in the first hidden layer. \\(k\\) must be sufficiently large to learn the necessary patterns. This can lead to practical problems regarding memory and compute.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.The columns of the weight matrix \\(\\mathbf{W}\\) (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned by reshaping them to the dimensionality of the input images \\(\\mathbf{x} \\in \\mathbb{R}^{h \\times w}\\).",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-1",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-1",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-2",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-2",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.Can an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e. \\(f(P(\\mathbf{x}))\\)?",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-3",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-pattern-learning-3",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.Yes! An MLP has no notion of distance and treats every connection between every input equally.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#multilayer-perceptrons-and-translations",
    "href": "slides_cas/cnns.html#multilayer-perceptrons-and-translations",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Translations",
    "text": "Multilayer-Perceptrons and Translations\n\n\nFigure 7\nOften the patterns we want to learn are not spatially constraint. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation.\nHow do MLPs deal with this?",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#mlp-and-images",
    "href": "slides_cas/cnns.html#mlp-and-images",
    "title": "Convolutional Neural Networks",
    "section": "MLP and Images",
    "text": "MLP and Images\n\n\nFigure 8\nMLPs need to (re-) learn the same pattern at all positions, which is incredibly inefficient. This also means the training data must contain all patterns at all feasible positions, otherwise the MLP won’t be able to learn how to recognize them.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#mlps-and-images",
    "href": "slides_cas/cnns.html#mlps-and-images",
    "title": "Convolutional Neural Networks",
    "section": "MLPs and Images",
    "text": "MLPs and Images\n\nHigh-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.\nLocality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.\nStability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.\n\nWhat do we need?",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#invariance-and-equivariance",
    "href": "slides_cas/cnns.html#invariance-and-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Invariance and Equivariance",
    "text": "Invariance and Equivariance\nFor many tasks small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\nA function \\(f(\\mathbf{x})\\) (such as a layer in a neural network) of an image \\(\\mathbf{x}\\) is equivariant with respect to a transformation \\(g(\\mathbf{\\mathbf{x}})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = g(f(\\mathbf{x}))\n\\end{align}\\]\nA function \\(f(\\mathbf{x})\\) is invariant to a transformation \\(g(\\mathbf{x})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = f(\\mathbf{x})\n\\end{align}\\]",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#example-invariance",
    "href": "slides_cas/cnns.html#example-invariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Invariance",
    "text": "Example Invariance\nExample where invariance is required:\n\nWhen objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#example-equivariance",
    "href": "slides_cas/cnns.html#example-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Equivariance",
    "text": "Example Equivariance\nExample where equivariance is required:\n\nWhen objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#biological-inspiration",
    "href": "slides_cas/cnns.html#biological-inspiration",
    "title": "Convolutional Neural Networks",
    "section": "Biological Inspiration",
    "text": "Biological Inspiration\n\nSchematische Darstellung von verbundenen Neuronen. Source: Phillips (2015)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#experiments-on-cats",
    "href": "slides_cas/cnns.html#experiments-on-cats",
    "title": "Convolutional Neural Networks",
    "section": "Experiments on Cats",
    "text": "Experiments on Cats\n\nIllustration Source",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#visual-cortex",
    "href": "slides_cas/cnns.html#visual-cortex",
    "title": "Convolutional Neural Networks",
    "section": "Visual Cortex",
    "text": "Visual Cortex\n\nRepresentation of transformations in the visual cortex. Source: Kubilius (2017)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#cnns",
    "href": "slides_cas/cnns.html#cnns",
    "title": "Convolutional Neural Networks",
    "section": "CNNs",
    "text": "CNNs\n\nThe activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: Li (2022)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers",
    "href": "slides_cas/cnns.html#convolutional-layers",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolution-operation",
    "href": "slides_cas/cnns.html#convolution-operation",
    "title": "Convolutional Neural Networks",
    "section": "Convolution Operation",
    "text": "Convolution Operation\nConvolution in Deep Learning is typically implemented as cross-correlation.\n\\[\\begin{equation}\nS(i, j) = (K * I)(i, j) = b + \\sum_m \\sum_n I(i + m, j + n) K(m, n)\n\\end{equation}\\]\n\\(I\\) is the input (for example an image), \\(K\\) is the kernel (typically smaller than \\(I\\)) and \\(b\\) is a bias term which is being added to the weighted sum.\nIf \\(I\\) is an RGB image (in the first layer of a CNN for example), the kernel \\(K\\) would have dimensionality \\(3 \\times K \\times K\\) (assuming a square kernel). More generally we learn kernels of the dimensionality \\(\\C_{in} \\times K \\times K\\).\nMultiple kernels, let’s say $\\(C_o\\) kernels, can be grouped together: \\(C_o \\times C_{in} \\times K \\times K\\).\nWe often referr to such tensors as filters or filter banks.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolution-on-rgb-images",
    "href": "slides_cas/cnns.html#convolution-on-rgb-images",
    "title": "Convolutional Neural Networks",
    "section": "Convolution on RGB Images",
    "text": "Convolution on RGB Images\n\n\nA 2D convolution applied to an image. Illustrated is the element-wise multiplication of the kernel weights \\(\\Omega\\) with the corresponding RGB input values at a specific position. Source: Prince (2023)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-1",
    "href": "slides_cas/cnns.html#convolutional-layers-1",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-2",
    "href": "slides_cas/cnns.html#convolutional-layers-2",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-3",
    "href": "slides_cas/cnns.html#convolutional-layers-3",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-4",
    "href": "slides_cas/cnns.html#convolutional-layers-4",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolutional-layers-5",
    "href": "slides_cas/cnns.html#convolutional-layers-5",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#hyper-parameters",
    "href": "slides_cas/cnns.html#hyper-parameters",
    "title": "Convolutional Neural Networks",
    "section": "Hyper-Parameters",
    "text": "Hyper-Parameters\nConvolutional Layers are parameterized:\n\nDepth: How many activation maps?\nPadding: How much padding is added to the input?\nStride: What is the step size of the convolution?\nKernel-Size: What is the kernel size?",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding-why",
    "href": "slides_cas/cnns.html#padding-why",
    "title": "Convolutional Neural Networks",
    "section": "Padding: Why?",
    "text": "Padding: Why?\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding",
    "href": "slides_cas/cnns.html#padding",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nLeft: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding-1",
    "href": "slides_cas/cnns.html#padding-1",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\n\n\nLeft: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding-and-stride",
    "href": "slides_cas/cnns.html#padding-and-stride",
    "title": "Convolutional Neural Networks",
    "section": "Padding and Stride",
    "text": "Padding and Stride\n\n\n\nStride with Padding. Red indicates the position of the corresponding filter value on the input activations.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#padding-and-stride-animations",
    "href": "slides_cas/cnns.html#padding-and-stride-animations",
    "title": "Convolutional Neural Networks",
    "section": "Padding and Stride: Animations",
    "text": "Padding and Stride: Animations\nDumoulin and Visin (2016) has created some animations to better understand convolutions, available here: Link.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#calculations",
    "href": "slides_cas/cnns.html#calculations",
    "title": "Convolutional Neural Networks",
    "section": "Calculations",
    "text": "Calculations\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#calculations-1",
    "href": "slides_cas/cnns.html#calculations-1",
    "title": "Convolutional Neural Networks",
    "section": "Calculations",
    "text": "Calculations\nThis formula covers all scenarios!\nSize of Activation Map\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n\\end{equation}\\]",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#quiz",
    "href": "slides_cas/cnns.html#quiz",
    "title": "Convolutional Neural Networks",
    "section": "Quiz",
    "text": "Quiz\nScenario:\n\nInput: 3 x 32 x 32\nConvolution: 10 filters with 5x5 kernel size, stride=1, pad=2\n\nWhat is the size of the activation map?\nHow many weights are there?\nSize of Activation Map\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n\\end{equation}\\]",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#sparse-connectivity-and-parameter-sharing",
    "href": "slides_cas/cnns.html#sparse-connectivity-and-parameter-sharing",
    "title": "Convolutional Neural Networks",
    "section": "Sparse Connectivity and Parameter Sharing",
    "text": "Sparse Connectivity and Parameter Sharing\nLocal (Sparse) Connectivity: Neurons are only locally connected.\nParameter Sharing: Weights of a neuron are applied locally but are the same across the entire input.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#convolution-is-parameter-sharing-always-useful",
    "href": "slides_cas/cnns.html#convolution-is-parameter-sharing-always-useful",
    "title": "Convolutional Neural Networks",
    "section": "Convolution: Is Parameter Sharing Always Useful?",
    "text": "Convolution: Is Parameter Sharing Always Useful?\nQuestion: Is parameter sharing always useful?",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#mlp-parameters",
    "href": "slides_cas/cnns.html#mlp-parameters",
    "title": "Convolutional Neural Networks",
    "section": "MLP Parameters",
    "text": "MLP Parameters\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)\n        self.hidden_layer2 = nn.Linear(64, 32)\n        self.output_layer = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.hidden_layer1(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x\n\nnet = MLP()\nprint(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n==========================================================================================",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#cnn-parameters",
    "href": "slides_cas/cnns.html#cnn-parameters",
    "title": "Convolutional Neural Networks",
    "section": "CNN Parameters",
    "text": "CNN Parameters\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.output_layer = nn.Linear(16 * 8 * 8, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.flatten(x)\n        x = self.output_layer(x)\n        return x\n\ncnn = CNN()\nprint(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n==========================================================================================",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#quiz-linear-transformation-vs-convolution",
    "href": "slides_cas/cnns.html#quiz-linear-transformation-vs-convolution",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Linear Transformation vs Convolution",
    "text": "Quiz: Linear Transformation vs Convolution\n\n\n\nInput in 2-D (top left), the flat version (bottom left), expected output (right), and unknown transformation (center).",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#translation-invariance-equivariance",
    "href": "slides_cas/cnns.html#translation-invariance-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Translation Invariance / Equivariance",
    "text": "Translation Invariance / Equivariance\nGiven a translation \\(g()\\), which spatially shifts inputs:\n\nTranslation invariance: \\(f(g(x))=f(x)\\)\nTranslation equivariance: \\(f(g(x))=g(f(x))\\)\n\nConvolutions are translation equivariant: Example Video",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#stacking-convolutions",
    "href": "slides_cas/cnns.html#stacking-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Stacking Convolutions",
    "text": "Stacking Convolutions\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field",
    "href": "slides_cas/cnns.html#receptive-field",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#receptive-field-1",
    "href": "slides_cas/cnns.html#receptive-field-1",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#dilated-convolutions",
    "href": "slides_cas/cnns.html#dilated-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Dilated Convolutions",
    "text": "Dilated Convolutions\n\nConvolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#dilated-convolutions-1",
    "href": "slides_cas/cnns.html#dilated-convolutions-1",
    "title": "Convolutional Neural Networks",
    "section": "Dilated Convolutions",
    "text": "Dilated Convolutions\n\nConvolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#x1-convolutions",
    "href": "slides_cas/cnns.html#x1-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "1x1 Convolutions",
    "text": "1x1 Convolutions\n\n\n\n\n\n\nSource: Johnson (2019)\n\n\n\n\nFigure 18",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#depthwise-separable-convolutions",
    "href": "slides_cas/cnns.html#depthwise-separable-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Depthwise Separable Convolutions",
    "text": "Depthwise Separable Convolutions\n\n\n\n\n\n\nSource: https://paperswithcode.com/method/depthwise-convolution\n\n\n\n\nFigure 19",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#depthwise-separable-convolutions-1",
    "href": "slides_cas/cnns.html#depthwise-separable-convolutions-1",
    "title": "Convolutional Neural Networks",
    "section": "Depthwise Separable Convolutions",
    "text": "Depthwise Separable Convolutions\n\n\n\nSource: Yu and Koltun (2016)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#pooling-layers",
    "href": "slides_cas/cnns.html#pooling-layers",
    "title": "Convolutional Neural Networks",
    "section": "Pooling Layers",
    "text": "Pooling Layers\n\nSource: Li (2022)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#max-pooling",
    "href": "slides_cas/cnns.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max Pooling",
    "text": "Max Pooling\n\nMax pooling, input (left) and output (right).",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#average-pooling",
    "href": "slides_cas/cnns.html#average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Average Pooling",
    "text": "Average Pooling\n\n\n\n\n\n\nAverage pooling, input (left) and output (right).\n\n\n\n\nFigure 21",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#other-pooling-layers",
    "href": "slides_cas/cnns.html#other-pooling-layers",
    "title": "Convolutional Neural Networks",
    "section": "Other Pooling Layers",
    "text": "Other Pooling Layers\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#global-average-pooling",
    "href": "slides_cas/cnns.html#global-average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Global Average Pooling",
    "text": "Global Average Pooling\n\n\n\n\n\n\nGlobal Average pooling, input (left) and output (right).\n\n\n\n\nFigure 22",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/cnns.html#learned-filters",
    "href": "slides_cas/cnns.html#learned-filters",
    "title": "Convolutional Neural Networks",
    "section": "Learned Filters",
    "text": "Learned Filters\n\n\n\nSource: Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides CAS",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#what-are-they",
    "href": "slides_cas/foundation_models.html#what-are-they",
    "title": "Foundation Models",
    "section": "What are they?",
    "text": "What are they?\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#characteristics-of-foundation-models",
    "href": "slides_cas/foundation_models.html#characteristics-of-foundation-models",
    "title": "Foundation Models",
    "section": "Characteristics of Foundation Models",
    "text": "Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#clip-a-foundation-model-example",
    "href": "slides_cas/foundation_models.html#clip-a-foundation-model-example",
    "title": "Foundation Models",
    "section": "CLIP: A Foundation Model Example",
    "text": "CLIP: A Foundation Model Example\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#how-clip-works",
    "href": "slides_cas/foundation_models.html#how-clip-works",
    "title": "Foundation Models",
    "section": "How CLIP Works",
    "text": "How CLIP Works\n\nSource: Radford et al. (2021)CLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#how-clip-can-be-applied",
    "href": "slides_cas/foundation_models.html#how-clip-can-be-applied",
    "title": "Foundation Models",
    "section": "How CLIP can be applied",
    "text": "How CLIP can be applied\n\nSource: Radford et al. (2021)",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#applications-of-clip",
    "href": "slides_cas/foundation_models.html#applications-of-clip",
    "title": "Foundation Models",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\nContent Moderation: CLIP can assist in identifying inappropriate content in images based on textual cues.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#example",
    "href": "slides_cas/foundation_models.html#example",
    "title": "Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport torch\nimport clip\nfrom PIL import Image\n\n# Load the model and the preprocess function\nmodel, preprocess = clip.load(\"ViT-B/32\")\n\n# Load an image\nimage = preprocess(Image.open(\"path/to/your/image.jpg\")).unsqueeze(0)\n\n# Define a set of labels\nlabels = [\"a dog\", \"a cat\", \"a car\", \"a tree\"]\n\n# Tokenize the labels\ntext = clip.tokenize(labels)\n\n# Compute the image and text features\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\n# Compute the similarity between the image and each label\nsimilarities = (image_features @ text_features.T).softmax(dim=-1)\n\n# Print the most similar label\nprint(\"Label:\", labels[similarities.argmax().item()])",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#how-vqa-works",
    "href": "slides_cas/foundation_models.html#how-vqa-works",
    "title": "Foundation Models",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#applications-of-vqa",
    "href": "slides_cas/foundation_models.html#applications-of-vqa",
    "title": "Foundation Models",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nEducational Tools: VQA systems can be used in educational applications to assist students in learning by providing answers to questions about visual content.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#example-1",
    "href": "slides_cas/foundation_models.html#example-1",
    "title": "Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of a VQA system using a hypothetical multi-modal model:\n\n# Hypothetical code for a Visual Question Answering system\nimport torch\nfrom PIL import Image\nfrom transformers import VQAModel, VQATokenizer\n\n# Load the model and the tokenizer\nmodel = VQAModel.from_pretrained(\"hypothetical-vqa-model\")\ntokenizer = VQATokenizer.from_pretrained(\"hypothetical-vqa-model\")\n\n# Load an image\nimage = Image.open(\"path/to/your/image.jpg\")\n\n# Define a question\nquestion = \"What is in the image?\"\n\n# Preprocess the image and the question\ninputs = tokenizer(image, question, return_tensors=\"pt\")\n\n# Get the model's answer\nwith torch.no_grad():\n    outputs = model(**inputs)\n    answer = outputs.logits.argmax(-1)\n\n# Print the answer\nprint(\"Answer:\", tokenizer.decode(answer))",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides_cas/foundation_models.html#conclusion",
    "href": "slides_cas/foundation_models.html#conclusion",
    "title": "Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nFoundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape.",
    "crumbs": [
      "Slides CAS",
      "Foundation Models"
    ]
  },
  {
    "objectID": "slides/frameworks.html#deep-learning-frameworks",
    "href": "slides/frameworks.html#deep-learning-frameworks",
    "title": "Deep Learning Frameworks",
    "section": "Deep Learning Frameworks",
    "text": "Deep Learning Frameworks\nComponents / important features of such frameworks are:\n\nFast development and testing of neural networks\nAutomatic differentiation of operations\nEfficient execution on diverse hardware"
  },
  {
    "objectID": "slides/frameworks.html#frameworks",
    "href": "slides/frameworks.html#frameworks",
    "title": "Deep Learning Frameworks",
    "section": "Frameworks",
    "text": "Frameworks\n\n\n\nFrameworks (from Li (2022))."
  },
  {
    "objectID": "slides/frameworks.html#computational-graph-autograd",
    "href": "slides/frameworks.html#computational-graph-autograd",
    "title": "Deep Learning Frameworks",
    "section": "Computational Graph & Autograd",
    "text": "Computational Graph & Autograd\nThe core of neural networks is the Computational Graph. Dependent operations are automatically integrated into a directed acyclic graph (DAG). Gradients are tracked as needed, allowing variables to be efficiently updated/trained."
  },
  {
    "objectID": "slides/frameworks.html#computational-graph",
    "href": "slides/frameworks.html#computational-graph",
    "title": "Deep Learning Frameworks",
    "section": "Computational Graph",
    "text": "Computational Graph\n\\[\\begin{equation*}\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) =  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij}\n\\end{equation*}\\]\n\nComputational Graph"
  },
  {
    "objectID": "slides/frameworks.html#computational-graph-autograd-1",
    "href": "slides/frameworks.html#computational-graph-autograd-1",
    "title": "Deep Learning Frameworks",
    "section": "Computational Graph & Autograd",
    "text": "Computational Graph & Autograd\n\\[\\begin{align*}\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) &=  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij} \\\\\n    \\mathbf{D} &= \\mathbf{A} \\odot \\mathbf{B} \\\\\n    \\mathbf{E} &= D + \\mathbf{C} \\\\\n    F &= \\sum_{ij} \\mathbf{E}_{ij}\n\\end{align*}\\]\nPartial Derivative with Autograd and Chain Rule\n\\[\\begin{equation*}\n    \\frac{\\partial F}{\\partial A_{ij}} = \\frac{\\partial F}{\\partial \\mathbf{E}} \\frac{\\partial \\mathbf{E}}{\\partial \\mathbf{D}} \\frac{\\partial \\mathbf{D}}{\\partial \\mathbf{A}_{ij}}\n\\end{equation*}\\]\n\nComputational Graph"
  },
  {
    "objectID": "slides/frameworks.html#numpy-example",
    "href": "slides/frameworks.html#numpy-example",
    "title": "Deep Learning Frameworks",
    "section": "Numpy Example",
    "text": "Numpy Example\n\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0\nde_dd = 1.0\nde_dc = c\ndd_da = b\n\ndf_da = df_de * de_dd * dd_da\n\nprint(df_da)\n\n[[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\n\n\n\nComputational Graph"
  },
  {
    "objectID": "slides/frameworks.html#pytorch-example",
    "href": "slides/frameworks.html#pytorch-example",
    "title": "Deep Learning Frameworks",
    "section": "PyTorch Example",
    "text": "PyTorch Example\n\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n\n\n\nComputational Graph"
  },
  {
    "objectID": "slides/frameworks.html#pytorch---why",
    "href": "slides/frameworks.html#pytorch---why",
    "title": "Deep Learning Frameworks",
    "section": "PyTorch - Why?",
    "text": "PyTorch - Why?\nIn this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources."
  },
  {
    "objectID": "slides/frameworks.html#fundamental-concepts",
    "href": "slides/frameworks.html#fundamental-concepts",
    "title": "Deep Learning Frameworks",
    "section": "Fundamental Concepts",
    "text": "Fundamental Concepts\n\nTensor: N-dimensional array, like numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks"
  },
  {
    "objectID": "slides/frameworks.html#tensors",
    "href": "slides/frameworks.html#tensors",
    "title": "Deep Learning Frameworks",
    "section": "Tensors",
    "text": "Tensors\nThe central data structure in PyTorch is torch.Tensor.\nIt is very similar to numpy.array but can be easily loaded onto GPUs. Tensors can be created in various ways, for example from lists:\n\nimport torch\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)"
  },
  {
    "objectID": "slides/frameworks.html#autograd",
    "href": "slides/frameworks.html#autograd",
    "title": "Deep Learning Frameworks",
    "section": "Autograd",
    "text": "Autograd\nWith torch.autograd, gradients can be automatically computed for a computational graph.\n\nimport torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\nloss.backward()"
  },
  {
    "objectID": "slides/frameworks.html#torch.nn",
    "href": "slides/frameworks.html#torch.nn",
    "title": "Deep Learning Frameworks",
    "section": "torch.nn",
    "text": "torch.nn\nWith torch.nn, components of neural networks can be defined. These components provide methods and have a state to store data such as weights and gradients."
  },
  {
    "objectID": "slides/frameworks.html#torch.nn---example",
    "href": "slides/frameworks.html#torch.nn---example",
    "title": "Deep Learning Frameworks",
    "section": "torch.nn - Example",
    "text": "torch.nn - Example\n\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits"
  },
  {
    "objectID": "slides/frameworks.html#torch.optim",
    "href": "slides/frameworks.html#torch.optim",
    "title": "Deep Learning Frameworks",
    "section": "torch.optim",
    "text": "torch.optim\nWith torch.optim, model parameters can be optimized using various algorithms.\n\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nloss_fn = torch.nn.CrossEntropyLoss()\nfor i in range(0, 3):\n    input, target = torch.rand(1, 28, 28), torch.randint(low=0, high=10, size=(1, ))\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "slides/frameworks.html#training-loops",
    "href": "slides/frameworks.html#training-loops",
    "title": "Deep Learning Frameworks",
    "section": "Training Loops",
    "text": "Training Loops\nA training loop iterates over mini-batches and optimizes the model parameters.\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")"
  },
  {
    "objectID": "slides/frameworks.html#training-loops-2",
    "href": "slides/frameworks.html#training-loops-2",
    "title": "Deep Learning Frameworks",
    "section": "Training Loops 2",
    "text": "Training Loops 2\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")"
  },
  {
    "objectID": "slides/frameworks.html#pytorch-ecosystem",
    "href": "slides/frameworks.html#pytorch-ecosystem",
    "title": "Deep Learning Frameworks",
    "section": "PyTorch Ecosystem",
    "text": "PyTorch Ecosystem\nThere are many packages that extend PyTorch with functionalities. One example is PyTorch-Lightning which simplifies managing training loops."
  },
  {
    "objectID": "slides/frameworks.html#other-frameworks-and-tools",
    "href": "slides/frameworks.html#other-frameworks-and-tools",
    "title": "Deep Learning Frameworks",
    "section": "Other Frameworks and Tools",
    "text": "Other Frameworks and Tools\n\nTensorFlow with Keras\nScikit-Learn\nONNX\nMonitoring: TensorBoard\nMonitoring: Weights & Biases"
  },
  {
    "objectID": "slides/frameworks.html#tensor-operations",
    "href": "slides/frameworks.html#tensor-operations",
    "title": "Deep Learning Frameworks",
    "section": "Tensor Operations",
    "text": "Tensor Operations\n\n\n\nMatrix Multiplication (from Li (2022))."
  },
  {
    "objectID": "slides/frameworks.html#cpu-vs-gpu",
    "href": "slides/frameworks.html#cpu-vs-gpu",
    "title": "Deep Learning Frameworks",
    "section": "CPU vs GPU",
    "text": "CPU vs GPU\n\n\n\nCPU vs GPU example (from Li (2022))."
  },
  {
    "objectID": "slides/frameworks.html#speed-comparison",
    "href": "slides/frameworks.html#speed-comparison",
    "title": "Deep Learning Frameworks",
    "section": "Speed Comparison",
    "text": "Speed Comparison\n\n\n\nSpeed comparison (from Li (2022)), data from Link."
  },
  {
    "objectID": "slides/frameworks.html#data-loading",
    "href": "slides/frameworks.html#data-loading",
    "title": "Deep Learning Frameworks",
    "section": "Data Loading",
    "text": "Data Loading\nA critical bottleneck in practice is transferring data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is called GPU starvation. Solutions include:\n\nReading data into RAM\nUsing fast disks like SSDs\nUtilizing multiple CPU threads to read data in parallel and keep it in RAM (pre-fetching)"
  },
  {
    "objectID": "slides/frameworks.html#gpu-starvation",
    "href": "slides/frameworks.html#gpu-starvation",
    "title": "Deep Learning Frameworks",
    "section": "GPU Starvation",
    "text": "GPU Starvation\n\n\n\nThe Y-axis shows the GPU utilization in percent, while the X-axis represents time. Source."
  },
  {
    "objectID": "slides/frameworks.html#gpu-parallelism",
    "href": "slides/frameworks.html#gpu-parallelism",
    "title": "Deep Learning Frameworks",
    "section": "GPU Parallelism",
    "text": "GPU Parallelism\n\n\n\nData and Model Parallelism (from Li (2022))."
  },
  {
    "objectID": "pages/cnns.html",
    "href": "pages/cnns.html",
    "title": "4 - Convolutional Neural Networks",
    "section": "",
    "text": "CNNs work similarly to MLPs: They consist of neurons with weights and biases arranged in layers. CNNs also have an output with which a differentiable loss function can be calculated so that the weights and biases can be adjusted using backpropagation.\nUnlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).\nThe input to an MLP is a vector \\(\\mathbf{x}^{(i)}\\), which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers), see Figure 1.\n\n\n\n\n\n\nFigure 1: Source: Li (2022)\n\n\n\nThe fully connected layers can only process 1-D vectors. Therefore, images \\(\\in \\mathbb{R}^{H \\times W \\times C}\\) must be flattened into 1-D vectors \\(\\in \\mathbb{R}^p\\). Here, \\(p= H \\times W \\times C\\). This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see Figure 3 for an illustration in an MLP and Figure 2 for an illustration on a linear model).\n\n\n\n\n\n\nFigure 2: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 3: Source: Li (2023)\n\n\n\nFor larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.\nA single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see Figure 4). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.\n\n\n\n\n\n\nFigure 4: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.\n\n\n\n\nCNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.\nFigure 5 shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.\n\n\n\n\n\n\nFigure 5: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: Li (2022).\n\n\n\nSometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block."
  },
  {
    "objectID": "pages/cnns.html#architecture",
    "href": "pages/cnns.html#architecture",
    "title": "4 - Convolutional Neural Networks",
    "section": "",
    "text": "CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.\nFigure 5 shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.\n\n\n\n\n\n\nFigure 5: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: Li (2022).\n\n\n\nSometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block."
  },
  {
    "objectID": "pages/cnns.html#hyper-parameters",
    "href": "pages/cnns.html#hyper-parameters",
    "title": "4 - Convolutional Neural Networks",
    "section": "Hyper-Parameters",
    "text": "Hyper-Parameters\nTo define a convolutional layer, various hyperparameters need to be set. Some of the most important ones are:\n\nDepth\nPadding\nStride\nKernel Size\n\nDepth determines how many filters are to be learned and thus defines the dimensionality (\\(C_{\\text{out}}\\)) of the output volume (the number of activation maps).\nStride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation. If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height.\nPadding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. Figure 11 illustrates the problem. Figure 12 shows an example with padding.\nFigure 13 shows the interplay between stride and padding.\n\n\n\n\n\n\nFigure 11: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 12: Left: Input (yellow) with zero-padding (1, 1) (white border), middle: Filter, right: Output.\n\n\n\n\n\n\n\n\n\nFigure 13: Stride with padding. The red mark indicates the filter value position on the input activations.\n\n\n\nDumoulin and Visin (2016) has created some animations for better understanding of convolutions and published them here: https://github.com/vdumoulin/conv_arithmetic.\n\nCalculations\nThe dimensionality of the activation maps can be calculated using the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernels)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along the spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along the spatial dimensions)\n\nScenario: stride = 1 and without padding\n\\[\\begin{equation}\no = (i - k) + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 14: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\nScenario: stride = 1 with padding\n\\[\\begin{equation}\no = (i - k) + 2p + 1\n\\end{equation}\\]\nScenario: Half (same) Padding -&gt; \\(o = i\\)\nValid for any \\(i\\) and for odd \\(k = 2n + 1, n ∈ N\\), \\(s = 1\\) and \\(p = \\lfloor k/2\\rfloor = n\\).\n\\[\\begin{align}\no &= i + 2 \\lfloor k/2\\rfloor − (k − 1) \\\\\no &= i + 2 n - 2 n \\\\\no &= i\n\\end{align}\\]\n\n\n\n\n\n\nFigure 15: Convolving a 3x3 kernel over a 5x5 input with 1x1 padding and stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\nScenario: Full Padding\nThe dimensionality of the output activation can also be increased.\nValid for any \\(i\\) and \\(k\\), \\(s = 1\\) and \\(p = k - 1\\).\n\\[\\begin{align}\no &= i + 2 (k − 1) - (k - 1) \\\\\no &= i + (k - 1)\n\\end{align}\\]\n\n\n\n\n\n\nFigure 16: Convolving a 3x3 kernel over a 5x5 input with 2x2 padding and stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\nScenario: No Padding, stride &gt; 1\nValid for any \\(i\\) and \\(k\\), \\(s &gt; 1\\) and \\(p = 0\\).\n\\[\\begin{equation}\no = \\lfloor\\frac{i - k}{s}\\rfloor + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 17: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 2x2. Source Dumoulin and Visin (2016)\n\n\n\nScenario: padding and stride &gt; 1\nValid for any \\(i, k, s, p\\).\n\\[\\begin{equation}\no = \\lfloor \\frac{i + 2p - k}{s} \\rfloor + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 18: Convolving a 3x3 kernel over a 5x5 input with 1x1 zero-padding and stride 2x2. Source Dumoulin and Visin (2016)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this formula, all scenarios are covered!\n\\[\\begin{equation}\no = \\lfloor \\frac{i + 2p - k}{s} \\rfloor + 1\n\\end{equation}\\]\n\n\n\n\nQuiz\n\n\n\n\n\n\nQuestion\nInput: 3 x 32 x 32\nConvolution: 10 filters with 5x5 kernel size, stride=1, and padding=2\nWhat is the size of the activation map?\n\n\n\n\n\n\n\n\n\nQuestion\nInput: 3 x 32 x 32\nConvolution: 10 filters with 5x5 kernel size, stride=1, and padding=2\nHow many weights are there?"
  },
  {
    "objectID": "pages/cnns.html#properties",
    "href": "pages/cnns.html#properties",
    "title": "4 - Convolutional Neural Networks",
    "section": "Properties",
    "text": "Properties\n\nLocal (Sparse) Connectivity & Parameter Sharing\nFully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.\nParameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.\n\n\n\n\n\n\nNote\n\n\n\nSometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).\n\n\nThe following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)\n        self.hidden_layer2 = nn.Linear(64, 32)\n        self.output_layer = nn.Linear(32, 10)\n     \n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.hidden_layer1(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x\n\nnet = MLP()\nprint(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n==========================================================================================\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.output_layer = nn.Linear(16 * 8 * 8 , 10)\n        \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.flatten(x)\n        x = self.output_layer(x)\n        return x\n\ncnn = CNN()\nprint(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n==========================================================================================\n\n\n\n\n\n\n\n\nFigure 19: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).\n\n\n\n\n\n\n\n\n\nQuestion\nHow should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?\n\n\n\n\n\nTranslation Invariance / Equivariance\nTranslation invariant is a function that produces the same value under translations \\(g()\\) of the input \\(x\\):\n\\[\\begin{equation}\nf(g(x))=f(x)\n\\end{equation}\\]\nTranslation equivariant is a function that produces the same value under translations \\(g()\\) of the input \\(x\\), provided that it is also shifted by \\(g()\\):\n\\[\\begin{equation}\nf(g(x))=g(f(x))\n\\end{equation}\\]\nConvolutions are translation equivariant, as illustrated well in the following example:\n\n\n\n\nStacking & Receptive Field\nMultiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. Figure 20 illustrates the result.\n\n\n\n\n\n\nFigure 20: Source: Johnson (2019)\n\n\n\nA convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality \\(H \\times W \\times C\\)! (There are also variants in higher dimensions.)\nHowever, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).\nThe receptive field defines which inputs influence the activations of a neuron. Figure 21 illustrates the concept.\n\n\n\n\n\n\nFigure 21: Source: Johnson (2019)\n\n\n\nStacking multiple convolutions increases the receptive field of a neuron relative to the original input (see Figure 22).\n\n\n\n\n\n\nFigure 22: Source: Johnson (2019)"
  },
  {
    "objectID": "pages/cnns.html#variations",
    "href": "pages/cnns.html#variations",
    "title": "4 - Convolutional Neural Networks",
    "section": "Variations",
    "text": "Variations\n\nDilated Convolutions\nDilated convolutions have an additional hyperparameter, the dilation. Dilated convolutions have holes between the rows and columns of a kernel. This increases the receptive field without having to learn more parameters. This variant is also called à trous convolution. Figure 23, Figure 24, and Figure 25 show examples.\n\n\n\n\n\n\nFigure 23: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2. Source Dumoulin and Visin (2016)\n\n\n\n\n\n\n\n\n\nFigure 24: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.\n\n\n\n\n\n\n\n\n\nFigure 25: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.\n\n\n\n\n\n1x1 (pointwise) Convolutions\n1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (\\(C\\)) of activation maps with few parameters. For example, activation maps of dimensionality (\\(C \\times H \\times W\\)) can be changed to a volume of (\\(C2 \\times H \\times W\\)) with \\(C2 * (C + 1)\\). This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (\\(C2=3\\)) for image generation models. Figure 26 shows an example.\n\n\n\n\n\n\nFigure 26: Source: Johnson (2019)\n\n\n\n\n\nDepthwise Separable Convolutions\nDepthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (\\(1 \\times K \\times K\\)). Figure 27 shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See Figure 28 for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.\n\n\n\n\n\n\nFigure 27: Source: https://paperswithcode.com/method/depthwise-convolution\n\n\n\n\n\n\n\n\n\nFigure 28: Source: Yu and Koltun (2016)"
  },
  {
    "objectID": "pages/cnns.html#pooling-layers",
    "href": "pages/cnns.html#pooling-layers",
    "title": "4 - Convolutional Neural Networks",
    "section": "Pooling Layers",
    "text": "Pooling Layers\nOften, the spatial dimensionality of the activation maps needs to be successively reduced in a CNN. This reduces the number of computations and memory required. Also, information is condensed and aggregated layer by layer: high spatial resolution is often no longer necessary. We have already seen that convolutional layers with a stride &gt; 1 can achieve this goal. However, it is also possible to use pooling layers, which do not have (learnable) parameters.\nA frequently used variant is the max-pooling layer. This layer operates independently on each slice along the depth dimension and returns the maximum value. The kernel size and stride must also be defined. A stride of \\(2 \\times 2\\) with a kernel of \\(2 \\times 2\\) halves the dimensionality of the activation maps along height and width.\nFor any \\(i,k,s\\) and without padding:\n\\[\\begin{equation}\no = \\lfloor\\frac{i - k}{s}\\rfloor + 1\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 29: Source: Li (2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30: Input\n\n\n\n\n\n\n\n\n\n\n\nFigure 31: Max Pooling\n\n\n\n\n\n\n\n\n\n\n\nFigure 32: Average Pooling\n\n\n\n\n\n\n\n\n\n\n\nFigure 33: Global Average Pooling\n\n\n\n\n\n\nFigure 33 shows the result of max-pooling, average-pooling and global average-pooling. In average-pooling, instead of choosing the maximum, the average activation is calculated. Otherwise, average-pooling works the same way as max-pooling. A crucial pooling variant is global average pooling. The average of the activations is calculated along the depth dimension (i.e., no kernel size or stride needs to be defined). Activation maps with (\\(C \\times H \\times W\\)) are reduced to (\\(C \\times 1 \\times 1\\)). This is useful, for example, to directly model logits in a classification problem with \\(C\\) classes. This allows architectures to be created that do not use fully connected layers at all."
  },
  {
    "objectID": "pages/cnns.html#pytorch-examples",
    "href": "pages/cnns.html#pytorch-examples",
    "title": "4 - Convolutional Neural Networks",
    "section": "PyTorch Examples",
    "text": "PyTorch Examples\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nimport torchshow as ts\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n\n#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')\nimage_path = \"../assets/images/cnns/cat.jpg\"\nimg = Image.open(image_path)\nimg\n\n\n\n\n\n\n\n\n\nfilter = torch.tensor(\n    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B\n    ]).unsqueeze(0).float()\nts.show(filter, show_axis=False)\n\n\n\n\n\n\n\n\n\ninput = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)\ninput /= 255.0\ninput -= 1.0\nresult = F.conv2d(input, filter, stride=1, padding=0, dilation=1, groups=1)\n\n\nts.show(result)\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/torchshow/visualization.py:388: UserWarning:\n\nOriginal input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.\n\n\n\n\n\n\n\n\n\n\n2D-Convolution:\n\nresult = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)\nts.show(result)\n\n\n\n\n\n\n\n\nTransposed convolution:\n\nresult = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)\nresult = F.conv_transpose2d(result, weight=torch.ones_like(filter))\nts.show(result)\n\n\n\n\n\n\n\n\nMax-Pooling:\n\nresult = F.max_pool2d(input, kernel_size=8, stride=8)\nts.show(result)"
  }
]