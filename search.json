[
  {
    "objectID": "slides/neural_networks/index.html#neuronen",
    "href": "slides/neural_networks/index.html#neuronen",
    "title": "Neural Networks",
    "section": "Neuronen",
    "text": "Neuronen\n\nSchematische Darstellung von verbundenen Neuronen. Source: Phillips (2015)"
  },
  {
    "objectID": "slides/neural_networks/index.html#visueller-cortex",
    "href": "slides/neural_networks/index.html#visueller-cortex",
    "title": "Neural Networks",
    "section": "Visueller Cortex",
    "text": "Visueller Cortex\n\nRepresentation von Transformationen im visuellen Cortex. Source: Kubilius (2017)"
  },
  {
    "objectID": "slides/neural_networks/index.html#multilayer-perceptron",
    "href": "slides/neural_networks/index.html#multilayer-perceptron",
    "title": "Neural Networks",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\n\nEin neuronales Netzwerk mit zwei Hidden Layer. Die Linien zeigen Verbindungen zwischen den Neuronen. Source: Li (2022)"
  },
  {
    "objectID": "slides/neural_networks/index.html#lineares-modell",
    "href": "slides/neural_networks/index.html#lineares-modell",
    "title": "Neural Networks",
    "section": "Lineares Modell",
    "text": "Lineares Modell\nEin lineares Modell hat folgende Form:\n\\[\\begin{equation}\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W} \\mathbf{x}^{(i)}  +  \\mathbf{b}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/neural_networks/index.html#neuronales-netzwerk",
    "href": "slides/neural_networks/index.html#neuronales-netzwerk",
    "title": "Neural Networks",
    "section": "Neuronales Netzwerk",
    "text": "Neuronales Netzwerk\n\\[\\begin{equation*}\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W}^{(2)} g\\big(\\mathbf{W}^{(1)} \\mathbf{x}^{(i)}  +  \\mathbf{b}^{(1)} \\big)  +  \\mathbf{b}^{(2)}\n\\end{equation*}\\]"
  },
  {
    "objectID": "slides/neural_networks/index.html#activation-funtion",
    "href": "slides/neural_networks/index.html#activation-funtion",
    "title": "Neural Networks",
    "section": "Activation Funtion",
    "text": "Activation Funtion\n\\[\\begin{equation}\n\\text{ReLU}(x) = \\begin{cases}\nx, & \\text{if } x \\geq 0 \\\\\n0, & \\text{if } x &lt; 0\n\\end{cases}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\nFigure 1: Linear (left) vs non-linear (right) activation function."
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(a_i\\)\nElement \\(i\\) of vector \\(\\mathbf{a}\\), with indexing starting at 1\n\n\n\\(A_{i,j}\\)\nElement \\(i, j\\) of matrix \\(\\mathbf{A}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\mathbf{X}\\)\nThe design matrix with dimensionality \\(nxp\\) with \\(n\\) samples with \\(p\\) features.\n\n\n\\(\\mathbf{x}^{(i)}\\)\nThe i-th training example.\n\n\n\\(\\mathbf{y}^{(i)}\\)\nThe label-vector for the i-th training example.\n\n\n\\(y^{(i)}\\)\nThe label for the i-th training example.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(P(x)\\)\nA probability distribution over a discrete variable.\n\n\n\\(p(x)\\)\nA probability distribution over a contiuous variable or over a variable whose type has not been specified.\n\n\n\\(\\mathbb{E}_{x \\sim P} [ f(x) ]\\text{ or } \\mathbb{E} f(x)\\)\nExpectation of \\(f(x)\\) with respect to \\(P(x)\\)\n\n\n\\(\\mathcal{N} ( \\mathbf{x} ; \\mu , \\Sigma)\\)\nGaussian distribution over \\(\\mathbf{x}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\)\n\n\n\\(x \\sim \\mathcal{N} (\\mu , \\sigma)\\)\nGaussian distribution over \\(x\\) with mean \\(\\mu\\) and variance \\(\\sigma\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\nabla_{\\mathbf{w}} J\\)\nGradient of \\(J\\) with respect to \\(\\mathbf{w}\\)\n\n\n\\(\\frac{\\partial J}{\\partial w}\\)\nPartial derivative of \\(J\\) with respect to \\(w\\)\n\n\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\log x\\)\nThe natural logarithm of \\(x\\).\n\n\n\\(\\lVert \\mathbf{x} \\rVert_p\\)\n\\(L^p\\) norm of \\(\\mathbf{x}\\)\n\n\n\\(\\lVert \\mathbf{x} \\rVert\\)\n\\(L^2\\) norm of \\(\\mathbf{x}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\nNCHW\nThe input format of images in PyTorch. N: number of images (batch size), C: number of channels, H: height, W: width",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#numbers-and-arrays",
    "href": "notation.html#numbers-and-arrays",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#indexing",
    "href": "notation.html#indexing",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a_i\\)\nElement \\(i\\) of vector \\(\\mathbf{a}\\), with indexing starting at 1\n\n\n\\(A_{i,j}\\)\nElement \\(i, j\\) of matrix \\(\\mathbf{A}\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#datasets-and-distributions",
    "href": "notation.html#datasets-and-distributions",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(\\mathbf{X}\\)\nThe design matrix with dimensionality \\(nxp\\) with \\(n\\) samples with \\(p\\) features.\n\n\n\\(\\mathbf{x}^{(i)}\\)\nThe i-th training example.\n\n\n\\(\\mathbf{y}^{(i)}\\)\nThe label-vector for the i-th training example.\n\n\n\\(y^{(i)}\\)\nThe label for the i-th training example.",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#probability-theory",
    "href": "notation.html#probability-theory",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(P(x)\\)\nA probability distribution over a discrete variable.\n\n\n\\(p(x)\\)\nA probability distribution over a contiuous variable or over a variable whose type has not been specified.\n\n\n\\(\\mathbb{E}_{x \\sim P} [ f(x) ]\\text{ or } \\mathbb{E} f(x)\\)\nExpectation of \\(f(x)\\) with respect to \\(P(x)\\)\n\n\n\\(\\mathcal{N} ( \\mathbf{x} ; \\mu , \\Sigma)\\)\nGaussian distribution over \\(\\mathbf{x}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\)\n\n\n\\(x \\sim \\mathcal{N} (\\mu , \\sigma)\\)\nGaussian distribution over \\(x\\) with mean \\(\\mu\\) and variance \\(\\sigma\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#calculus",
    "href": "notation.html#calculus",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(\\nabla_{\\mathbf{w}} J\\)\nGradient of \\(J\\) with respect to \\(\\mathbf{w}\\)\n\n\n\\(\\frac{\\partial J}{\\partial w}\\)\nPartial derivative of \\(J\\) with respect to \\(w\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#functions",
    "href": "notation.html#functions",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(\\log x\\)\nThe natural logarithm of \\(x\\).\n\n\n\\(\\lVert \\mathbf{x} \\rVert_p\\)\n\\(L^p\\) norm of \\(\\mathbf{x}\\)\n\n\n\\(\\lVert \\mathbf{x} \\rVert\\)\n\\(L^2\\) norm of \\(\\mathbf{x}\\)",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "notation.html#deep-learning",
    "href": "notation.html#deep-learning",
    "title": "Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\nNCHW\nThe input format of images in PyTorch. N: number of images (batch size), C: number of channels, H: height, W: width",
    "crumbs": [
      "Resources",
      "Notation"
    ]
  },
  {
    "objectID": "slides/intro/index.html#beispiel-1",
    "href": "slides/intro/index.html#beispiel-1",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 1",
    "text": "Beispiel 1\n\nBeispiel aus Link. Links das Original-Bild, rechts die mit Deep Learning verbesserte Version."
  },
  {
    "objectID": "slides/intro/index.html#beispiel-2",
    "href": "slides/intro/index.html#beispiel-2",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 2",
    "text": "Beispiel 2\n\nBeispiel aus Link. Links das Original-Bild, rechts die Manipulation."
  },
  {
    "objectID": "slides/intro/index.html#beispiel-3",
    "href": "slides/intro/index.html#beispiel-3",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 3",
    "text": "Beispiel 3\n\nBeispiel aus Link. Links das Original-Bild, rechts die Manipulation."
  },
  {
    "objectID": "slides/intro/index.html#beispiel-4",
    "href": "slides/intro/index.html#beispiel-4",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beispiel 4",
    "text": "Beispiel 4\n\nAus Link."
  },
  {
    "objectID": "slides/intro/index.html#semantic-gap",
    "href": "slides/intro/index.html#semantic-gap",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\nIllustration des semantic gap."
  },
  {
    "objectID": "slides/intro/index.html#blickwinkel",
    "href": "slides/intro/index.html#blickwinkel",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Blickwinkel",
    "text": "Blickwinkel\n\nSource"
  },
  {
    "objectID": "slides/intro/index.html#deformation",
    "href": "slides/intro/index.html#deformation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Deformation",
    "text": "Deformation\n\nSource"
  },
  {
    "objectID": "slides/intro/index.html#beleuchtung",
    "href": "slides/intro/index.html#beleuchtung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Beleuchtung",
    "text": "Beleuchtung\n\nSource"
  },
  {
    "objectID": "slides/intro/index.html#hintergrund",
    "href": "slides/intro/index.html#hintergrund",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Hintergrund",
    "text": "Hintergrund\n\nSource"
  },
  {
    "objectID": "slides/intro/index.html#okklusion",
    "href": "slides/intro/index.html#okklusion",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Okklusion",
    "text": "Okklusion\n\nSource"
  },
  {
    "objectID": "slides/intro/index.html#intraklass-variation",
    "href": "slides/intro/index.html#intraklass-variation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Intraklass-Variation",
    "text": "Intraklass-Variation\n\nSource"
  },
  {
    "objectID": "slides/intro/index.html#kontext-abhängigkeit",
    "href": "slides/intro/index.html#kontext-abhängigkeit",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Kontext-Abhängigkeit",
    "text": "Kontext-Abhängigkeit\n\n\n\n\n\n\n\n\n\n\nKontext Source"
  },
  {
    "objectID": "slides/intro/index.html#image-classification",
    "href": "slides/intro/index.html#image-classification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012))."
  },
  {
    "objectID": "slides/intro/index.html#objekt-erkennung",
    "href": "slides/intro/index.html#objekt-erkennung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Objekt-Erkennung",
    "text": "Objekt-Erkennung\n\nObject Detection Beispiel (aus Redmon et al. (2016)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist."
  },
  {
    "objectID": "slides/intro/index.html#segmentierung",
    "href": "slides/intro/index.html#segmentierung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentierung",
    "text": "Segmentierung\n\nObject Segmentation Beispiel (aus He et al. (2018))."
  },
  {
    "objectID": "slides/intro/index.html#segmentierung-2",
    "href": "slides/intro/index.html#segmentierung-2",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentierung 2",
    "text": "Segmentierung 2"
  },
  {
    "objectID": "slides/intro/index.html#keypoint-detektierung",
    "href": "slides/intro/index.html#keypoint-detektierung",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Keypoint Detektierung",
    "text": "Keypoint Detektierung\n\nKeypoint Detection Beispiel (aus He et al. (2018))."
  },
  {
    "objectID": "slides/intro/index.html#image-translation",
    "href": "slides/intro/index.html#image-translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Translation",
    "text": "Image Translation\n\nImage Generation Beispiel (aus (isola_image?)–image_2018)."
  },
  {
    "objectID": "slides/intro/index.html#image-super-resolution",
    "href": "slides/intro/index.html#image-super-resolution",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Super Resolution",
    "text": "Image Super Resolution\n\nNvidia dlss: Link"
  },
  {
    "objectID": "slides/intro/index.html#image-colorization",
    "href": "slides/intro/index.html#image-colorization",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Colorization",
    "text": "Image Colorization\n\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link"
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Recommended Literature",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020) - Kann als PDF gratis heruntergeladen werden - Einführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Recommended Literature"
    ]
  },
  {
    "objectID": "literature.html#pytorch",
    "href": "literature.html#pytorch",
    "title": "Recommended Literature",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020) - Kann als PDF gratis heruntergeladen werden - Einführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Recommended Literature"
    ]
  },
  {
    "objectID": "literature.html#deep-learning",
    "href": "literature.html#deep-learning",
    "title": "Recommended Literature",
    "section": "Deep Learning",
    "text": "Deep Learning\nSimon J.D. Prince, Understanding Deep Learning, MIT Press, Prince (2023) - Brandaktuelles Buch über Deep Learning - Umfassende Einführung ins Thema mit sehr guten Illustrationen - Online verfügbar: Link\nGoodfellow, Ian and Bengio, Yoshua and Courville, Aaron, Deep Learning, MIT Press, Goodfellow, Bengio, and Courville (2016) - Sehr gute und umfassende Einführung in Deep Learning - Etwas älter aber immer noch in weiten Teilen aktuell - Online verfügbar: Link",
    "crumbs": [
      "Resources",
      "Recommended Literature"
    ]
  },
  {
    "objectID": "literature.html#machine-learning",
    "href": "literature.html#machine-learning",
    "title": "Recommended Literature",
    "section": "Machine Learning",
    "text": "Machine Learning\nGéron A, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, O’Reilly 2019 - Jupyter Notebooks sind öffentlich verfügbar: Link - Einsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\nRaschka S, Python Machine Learning, 3rd Edition, PACKT 2019 - Einsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\nKevin P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press 2022 - Vorabversion gratis verfügbar: Link - Umfassende Einführung in Machine Learning mit ausführlichen theoretischen Hintergründen\nChollet F, Deep Learning with Python, 2nd Edition, MEAP 2020 - Ein Klassiker für eine Einführung in Deep Learning (und Keras)\nHastie T et al., Elements of Statistical Learning, Springer 2009. - Kann als pdf gratis runtergeladen werden: Link - Enthält Machine Learning Grundlagen und viele Methoden (wenig über Neuronale Netzwerke)\nVanderPlas J, Python Data Science Handbook, O’Reilly 2017. - Wurde mit Jupyter Notebooks geschrieben. - Der gesamte Inhalt finden sie auf einer website: Link - Das Repository kann von github runtergelanden werden: Link",
    "crumbs": [
      "Resources",
      "Recommended Literature"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Links zu verschiedenen Themen in Machine Learning & Deep Learning.\n\n\nPyTorch internals - Blog Post\n\n\n\nUniversity of Michigan - Deep Learning for Computer Vision\n\nSehr gute Vorlesung zum Thema\n\nUniversity of California, Berkeley - Modern Computer Vision and Deep Learning\n\nSehr gute Vorlesung zum Thema\n\n\n\n\nPerceptron Learning Rule S. Raschka\nCS229 Stanford MLP Backpropagation\nNotes on Backpropagation\n3Blue1Brown Gradient Descent\n3Blue1Brown Backpropagation Calculus\nAndrew Ng Backprop\nAndrej Karpathy - Backpropagation from the ground up\n\n\n\nPaper von S.Raschka: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”\n\n\n\nMartin Zinkevich - Best Practices for ML Engineering\nAndrej Karpathy - A Recipe for Training Neural Networks\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Try Next\nAndrew Ng - Advice For Applying Machine Learning | Learning Curves\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)\nAndrew Ng - Machine Learning System Design | Prioritizing What To Work On\nAndrew Ng - Machine Learning System Design | Error Analysis\nAndrew Ng - Machine Learning System Design | Data For Machine Learning",
    "crumbs": [
      "Resources",
      "Links"
    ]
  },
  {
    "objectID": "links.html#pytorch",
    "href": "links.html#pytorch",
    "title": "Links",
    "section": "",
    "text": "PyTorch internals - Blog Post",
    "crumbs": [
      "Resources",
      "Links"
    ]
  },
  {
    "objectID": "links.html#deep-learning-and-computer-vision",
    "href": "links.html#deep-learning-and-computer-vision",
    "title": "Links",
    "section": "",
    "text": "University of Michigan - Deep Learning for Computer Vision\n\nSehr gute Vorlesung zum Thema\n\nUniversity of California, Berkeley - Modern Computer Vision and Deep Learning\n\nSehr gute Vorlesung zum Thema",
    "crumbs": [
      "Resources",
      "Links"
    ]
  },
  {
    "objectID": "links.html#neuronale-netzwerke---basics",
    "href": "links.html#neuronale-netzwerke---basics",
    "title": "Links",
    "section": "",
    "text": "Perceptron Learning Rule S. Raschka\nCS229 Stanford MLP Backpropagation\nNotes on Backpropagation\n3Blue1Brown Gradient Descent\n3Blue1Brown Backpropagation Calculus\nAndrew Ng Backprop\nAndrej Karpathy - Backpropagation from the ground up",
    "crumbs": [
      "Resources",
      "Links"
    ]
  },
  {
    "objectID": "links.html#model-selection",
    "href": "links.html#model-selection",
    "title": "Links",
    "section": "",
    "text": "Paper von S.Raschka: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”",
    "crumbs": [
      "Resources",
      "Links"
    ]
  },
  {
    "objectID": "links.html#ml-best-practices",
    "href": "links.html#ml-best-practices",
    "title": "Links",
    "section": "",
    "text": "Martin Zinkevich - Best Practices for ML Engineering\nAndrej Karpathy - A Recipe for Training Neural Networks\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Try Next\nAndrew Ng - Advice For Applying Machine Learning | Learning Curves\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)\nAndrew Ng - Machine Learning System Design | Prioritizing What To Work On\nAndrew Ng - Machine Learning System Design | Error Analysis\nAndrew Ng - Machine Learning System Design | Data For Machine Learning",
    "crumbs": [
      "Resources",
      "Links"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Herzlich willkommen zum Modul Computer Vision mit Deep Learning (1. Teil)!\nHier finden Sie Unterlagen und aktuelle Informationen zum Modul.\nModul Page\nCAS Page\n\n\n\n\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nEinführung Computer Vision mit Deep Learning\n\n\n9:30 - 10:30\nÜbung: Deep Learning für Bilder\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 11:30\nTheory CNNs\n\n\n11:30 - 12:15\nÜbung CNNs\n\n\n12:15 - 13:15\nMittagspause\n\n\n13:15 - 14:00\nBildklassifikation\n\n\n14:00 - 16:30\nÜbung: CNNs für Bildklassifikation\n\n\n\n\n\n\n\n\n\nBildklassifikation vertieft verstehen\nMethoden implementieren, anwenden und interpretieren können\nMit Deep Learning Frameworks umgehen und Libraries verwenden können\nPraktische Anwendung von Deep Learning Modellen demonstrieren und vorstellen können"
  },
  {
    "objectID": "index.html#inhalte",
    "href": "index.html#inhalte",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Zeit\nThema\n\n\n\n\n8:45 - 9:30\nEinführung Computer Vision mit Deep Learning\n\n\n9:30 - 10:30\nÜbung: Deep Learning für Bilder\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 11:30\nTheory CNNs\n\n\n11:30 - 12:15\nÜbung CNNs\n\n\n12:15 - 13:15\nMittagspause\n\n\n13:15 - 14:00\nBildklassifikation\n\n\n14:00 - 16:30\nÜbung: CNNs für Bildklassifikation"
  },
  {
    "objectID": "index.html#lernziele",
    "href": "index.html#lernziele",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Bildklassifikation vertieft verstehen\nMethoden implementieren, anwenden und interpretieren können\nMit Deep Learning Frameworks umgehen und Libraries verwenden können\nPraktische Anwendung von Deep Learning Modellen demonstrieren und vorstellen können"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "slides/cnns/index.html#beispiel-1",
    "href": "slides/cnns/index.html#beispiel-1",
    "title": "Convolutional Neural Networks",
    "section": "Beispiel 1",
    "text": "Beispiel 1\n\nBeispiel aus Link. Links das Original-Bild, rechts die mit Deep Learning verbesserte Version."
  },
  {
    "objectID": "slides/cnns/index.html#beispiel-2",
    "href": "slides/cnns/index.html#beispiel-2",
    "title": "Convolutional Neural Networks",
    "section": "Beispiel 2",
    "text": "Beispiel 2\n\nBeispiel aus Link. Links das Original-Bild, rechts die Manipulation."
  },
  {
    "objectID": "slides/cnns/index.html#beispiel-3",
    "href": "slides/cnns/index.html#beispiel-3",
    "title": "Convolutional Neural Networks",
    "section": "Beispiel 3",
    "text": "Beispiel 3\n\nBeispiel aus Link. Links das Original-Bild, rechts die Manipulation."
  },
  {
    "objectID": "slides/cnns/index.html#beispiel-4",
    "href": "slides/cnns/index.html#beispiel-4",
    "title": "Convolutional Neural Networks",
    "section": "Beispiel 4",
    "text": "Beispiel 4\n\nAus Link."
  },
  {
    "objectID": "slides/cnns/index.html#semantic-gap",
    "href": "slides/cnns/index.html#semantic-gap",
    "title": "Convolutional Neural Networks",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\nIllustration des semantic gap."
  },
  {
    "objectID": "slides/cnns/index.html#blickwinkel",
    "href": "slides/cnns/index.html#blickwinkel",
    "title": "Convolutional Neural Networks",
    "section": "Blickwinkel",
    "text": "Blickwinkel\n\nSource"
  },
  {
    "objectID": "slides/cnns/index.html#deformation",
    "href": "slides/cnns/index.html#deformation",
    "title": "Convolutional Neural Networks",
    "section": "Deformation",
    "text": "Deformation\n\nSource"
  },
  {
    "objectID": "slides/cnns/index.html#beleuchtung",
    "href": "slides/cnns/index.html#beleuchtung",
    "title": "Convolutional Neural Networks",
    "section": "Beleuchtung",
    "text": "Beleuchtung\n\nSource"
  },
  {
    "objectID": "slides/cnns/index.html#hintergrund",
    "href": "slides/cnns/index.html#hintergrund",
    "title": "Convolutional Neural Networks",
    "section": "Hintergrund",
    "text": "Hintergrund\n\nSource"
  },
  {
    "objectID": "slides/cnns/index.html#okklusion",
    "href": "slides/cnns/index.html#okklusion",
    "title": "Convolutional Neural Networks",
    "section": "Okklusion",
    "text": "Okklusion\n\nSource"
  },
  {
    "objectID": "slides/cnns/index.html#intraklass-variation",
    "href": "slides/cnns/index.html#intraklass-variation",
    "title": "Convolutional Neural Networks",
    "section": "Intraklass-Variation",
    "text": "Intraklass-Variation\n\nSource"
  },
  {
    "objectID": "slides/cnns/index.html#kontext-abhängigkeit",
    "href": "slides/cnns/index.html#kontext-abhängigkeit",
    "title": "Convolutional Neural Networks",
    "section": "Kontext-Abhängigkeit",
    "text": "Kontext-Abhängigkeit\n\n\n\n\n\n\n\n\n\n\nKontext Source"
  },
  {
    "objectID": "slides/cnns/index.html#image-classification",
    "href": "slides/cnns/index.html#image-classification",
    "title": "Convolutional Neural Networks",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012))."
  },
  {
    "objectID": "slides/cnns/index.html#objekt-erkennung",
    "href": "slides/cnns/index.html#objekt-erkennung",
    "title": "Convolutional Neural Networks",
    "section": "Objekt-Erkennung",
    "text": "Objekt-Erkennung\n\nObject Detection Beispiel (aus Redmon et al. (2016)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist."
  },
  {
    "objectID": "slides/cnns/index.html#segmentierung",
    "href": "slides/cnns/index.html#segmentierung",
    "title": "Convolutional Neural Networks",
    "section": "Segmentierung",
    "text": "Segmentierung\n\nObject Segmentation Beispiel (aus He et al. (2018))."
  },
  {
    "objectID": "slides/cnns/index.html#segmentierung-2",
    "href": "slides/cnns/index.html#segmentierung-2",
    "title": "Convolutional Neural Networks",
    "section": "Segmentierung 2",
    "text": "Segmentierung 2"
  },
  {
    "objectID": "slides/cnns/index.html#keypoint-detektierung",
    "href": "slides/cnns/index.html#keypoint-detektierung",
    "title": "Convolutional Neural Networks",
    "section": "Keypoint Detektierung",
    "text": "Keypoint Detektierung\n\nKeypoint Detection Beispiel (aus He et al. (2018))."
  },
  {
    "objectID": "slides/cnns/index.html#image-translation",
    "href": "slides/cnns/index.html#image-translation",
    "title": "Convolutional Neural Networks",
    "section": "Image Translation",
    "text": "Image Translation\n\nImage Generation Beispiel (aus (isola_image?)–image_2018)."
  },
  {
    "objectID": "slides/cnns/index.html#image-super-resolution",
    "href": "slides/cnns/index.html#image-super-resolution",
    "title": "Convolutional Neural Networks",
    "section": "Image Super Resolution",
    "text": "Image Super Resolution\n\nNvidia dlss: Link"
  },
  {
    "objectID": "slides/cnns/index.html#image-colorization",
    "href": "slides/cnns/index.html#image-colorization",
    "title": "Convolutional Neural Networks",
    "section": "Image Colorization",
    "text": "Image Colorization\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link"
  },
  {
    "objectID": "slides/practical/index.html#leaky-abstraction",
    "href": "slides/practical/index.html#leaky-abstraction",
    "title": "Practical Considerations",
    "section": "Leaky Abstraction",
    "text": "Leaky Abstraction\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)"
  },
  {
    "objectID": "slides/practical/index.html#silent-failure",
    "href": "slides/practical/index.html#silent-failure",
    "title": "Practical Considerations",
    "section": "Silent Failure",
    "text": "Silent Failure\nDas Trainieren von Neuronalen Netzwerken fails silently!"
  },
  {
    "objectID": "slides/practical/index.html#eins-werden-mit-den-daten",
    "href": "slides/practical/index.html#eins-werden-mit-den-daten",
    "title": "Practical Considerations",
    "section": "1) Eins werden mit den Daten",
    "text": "1) Eins werden mit den Daten\nGenaues Inspizieren der Daten!"
  },
  {
    "objectID": "slides/practical/index.html#camera-traps-fehler",
    "href": "slides/practical/index.html#camera-traps-fehler",
    "title": "Practical Considerations",
    "section": "Camera Traps: Fehler",
    "text": "Camera Traps: Fehler\n\n\n\nBeispiele von Bildern aus Kamerafallen."
  },
  {
    "objectID": "slides/practical/index.html#camera-traps-schwierigkeiten",
    "href": "slides/practical/index.html#camera-traps-schwierigkeiten",
    "title": "Practical Considerations",
    "section": "Camera Traps: Schwierigkeiten",
    "text": "Camera Traps: Schwierigkeiten\n\n\n\nBeispiele von Bildern aus Kamerafallen. Source: Beery, Van Horn, and Perona (2018)"
  },
  {
    "objectID": "slides/practical/index.html#seltene-klassen",
    "href": "slides/practical/index.html#seltene-klassen",
    "title": "Practical Considerations",
    "section": "Seltene Klassen",
    "text": "Seltene Klassen\n\n\n\nEin Bild von einem Serval. Unterhalb sind die Modell-Confidences abgebildet."
  },
  {
    "objectID": "slides/practical/index.html#mehrere-klassen",
    "href": "slides/practical/index.html#mehrere-klassen",
    "title": "Practical Considerations",
    "section": "Mehrere Klassen",
    "text": "Mehrere Klassen\n\nBeispiele einem Bild aus einer Kamerafalle mit verschiedenen Spezies."
  },
  {
    "objectID": "slides/practical/index.html#baselines-1",
    "href": "slides/practical/index.html#baselines-1",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEvaluierungs-Pipeline, Metiken, Experiment-Tracking und Baseline Modell."
  },
  {
    "objectID": "slides/practical/index.html#ml-prozess",
    "href": "slides/practical/index.html#ml-prozess",
    "title": "Practical Considerations",
    "section": "ML-Prozess",
    "text": "ML-Prozess\n\n\n\nDie Komponenten eines typischen Machine Learning Prozesses. Source: Raschka and Mirjalili (2020)"
  },
  {
    "objectID": "slides/practical/index.html#experiment-tracking",
    "href": "slides/practical/index.html#experiment-tracking",
    "title": "Practical Considerations",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\n\n\n\nWeights and Biases Experiment-Tracking."
  },
  {
    "objectID": "slides/practical/index.html#baselines-2",
    "href": "slides/practical/index.html#baselines-2",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nReproduzierbarkeit sicherstellen.\nimport torch\ntorch.manual_seed(0)"
  },
  {
    "objectID": "slides/practical/index.html#baselines-3",
    "href": "slides/practical/index.html#baselines-3",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nKeine unnötigen Techniken und Komplexitäten anwenden. Reduziere die Fehleranfälligkeit."
  },
  {
    "objectID": "slides/practical/index.html#baselines-4",
    "href": "slides/practical/index.html#baselines-4",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nVerwende, falls möglich, eine human baseline. Wie gut kann das Modell überhaupt sein?"
  },
  {
    "objectID": "slides/practical/index.html#schwierige-fälle",
    "href": "slides/practical/index.html#schwierige-fälle",
    "title": "Practical Considerations",
    "section": "Schwierige Fälle",
    "text": "Schwierige Fälle\n\nBild einer Kamerafalle das schwierig zu klassifizieren ist. Hier hatten Annotatoren mit Experten eine Übereinstimmung von 96.6%."
  },
  {
    "objectID": "slides/practical/index.html#baselines-5",
    "href": "slides/practical/index.html#baselines-5",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nInput-Unabhängige Baseline trainieren. Lernt das Modell überhaupt etwas?"
  },
  {
    "objectID": "slides/practical/index.html#baselines-6",
    "href": "slides/practical/index.html#baselines-6",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nDas Modell auf einem Batch an Daten overfitten. Funktioniert die Optimierung?"
  },
  {
    "objectID": "slides/practical/index.html#baselines-7",
    "href": "slides/practical/index.html#baselines-7",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nVisualisieren was in das Modell geht. Funktioniert mein Pre-Processing?\ny_hat = model(x)"
  },
  {
    "objectID": "slides/practical/index.html#fixes-sample-beispiel-segmentierung",
    "href": "slides/practical/index.html#fixes-sample-beispiel-segmentierung",
    "title": "Practical Considerations",
    "section": "Fixes Sample: Beispiel Segmentierung",
    "text": "Fixes Sample: Beispiel Segmentierung\n\n\n\nBeispiel eines Segmentierungsproblems: links der Input und rechts der Output."
  },
  {
    "objectID": "slides/practical/index.html#overfit-1",
    "href": "slides/practical/index.html#overfit-1",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nJetzt sollten ein gutes Verständnis des Datensatzes, ein hohes Vertrauen in die Evaluierungs-Pipeline und erste Baselines von einfachen Modellen vorhanden sein. Nun sucht man ein Modell, das eine möglichst gute Performance auf dem Trainings-Set aufweist."
  },
  {
    "objectID": "slides/practical/index.html#overfit-2",
    "href": "slides/practical/index.html#overfit-2",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nMan sucht eine gute Modell-Architektur. Dabei gilt das Prinzip “Don’t be a hero”. Man sollte also bereits implementierte / etablierte Architekturen bevorzugen."
  },
  {
    "objectID": "slides/practical/index.html#regularisierung",
    "href": "slides/practical/index.html#regularisierung",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nAn diesem Punkt sollte man eine gute Performance auf dem Training-Set erreicht haben. Nun kann der Fokus auf das Validation-Set gelegt werden."
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-1",
    "href": "slides/practical/index.html#regularisierung-1",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nDie einfachste Massnahme bessere Performance zu erreichen (und auch Overfitting zu reduzieren) ist das Sammeln von mehr Trainings-Daten. Das ist jedoch oft teuer!"
  },
  {
    "objectID": "slides/practical/index.html#learning---curve",
    "href": "slides/practical/index.html#learning---curve",
    "title": "Practical Considerations",
    "section": "Learning - Curve",
    "text": "Learning - Curve\nLohnt sich das Sammeln von mehr Daten?\n\nBeispiel einer learning curve. X-Achse Performance, Y-Achse die Anzahl Trainings-Samples. Linkes Panel mit Gauissian Naive Bayes und rechtes Panel mit Support Vector Classifier."
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-2",
    "href": "slides/practical/index.html#regularisierung-2",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nEine andere Möglichkeit ist data augmentation. Dabei werden neue Datenpunkte aus bestehenden generiert, indem man zufällige Änderungen an den Daten vornimmt. Dabei werden Datenpunkte typischerweise on-the-fly augmentiert."
  },
  {
    "objectID": "slides/practical/index.html#data-augmentation-augly",
    "href": "slides/practical/index.html#data-augmentation-augly",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Augly",
    "text": "Data Augmentation: Augly\n\n\n\nAugLy"
  },
  {
    "objectID": "slides/practical/index.html#data-augmentation-albumentations",
    "href": "slides/practical/index.html#data-augmentation-albumentations",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Albumentations",
    "text": "Data Augmentation: Albumentations\n\nAlbumentations"
  },
  {
    "objectID": "slides/practical/index.html#data-augmentation-kornia",
    "href": "slides/practical/index.html#data-augmentation-kornia",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Kornia",
    "text": "Data Augmentation: Kornia\n\n\n\nKornia"
  },
  {
    "objectID": "slides/practical/index.html#data-augmentation-beispiel",
    "href": "slides/practical/index.html#data-augmentation-beispiel",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Beispiel",
    "text": "Data Augmentation: Beispiel\n\n\n\nData Augmentation Beispiel."
  },
  {
    "objectID": "slides/practical/index.html#data-augmentation-synthetische-daten",
    "href": "slides/practical/index.html#data-augmentation-synthetische-daten",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Synthetische Daten",
    "text": "Data Augmentation: Synthetische Daten\n\n\n\nAus Beery et al. (2020). Synthetische und Halb-Synthetische Daten."
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-3",
    "href": "slides/practical/index.html#regularisierung-3",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nMit early stopping wird ein Modell trainiert und in periodischen Abständen, z.B. nach jeder Epoche, auf einem Validation-Set evaluiert. Dabei wird das Training abgebrochen, falls nach x Evaluierungs-Zyklen keine nennenswerte Verbesserung mehr erreicht wird."
  },
  {
    "objectID": "slides/practical/index.html#early-stopping",
    "href": "slides/practical/index.html#early-stopping",
    "title": "Practical Considerations",
    "section": "Early Stopping",
    "text": "Early Stopping\n\n\n\nSource: Link"
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-4",
    "href": "slides/practical/index.html#regularisierung-4",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nEarly Stopping in PyTorch.\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)"
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-5",
    "href": "slides/practical/index.html#regularisierung-5",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nMit weight decay kann ein Modell regularisiert werden. Dabei wird der Update-Schritt bei gradient descent modifiziert.\n\\[\\begin{equation}\n\\theta_{t+1} = \\theta_t (1 - \\lambda) - \\eta \\nabla J(\\theta)\n\\end{equation}\\]\nDabei ist \\(t\\) die Iteration, \\(\\theta\\) die Modell-Parameter, \\(\\eta\\) die learning rate und \\(\\lambda\\) der Decay-Parameter."
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-6",
    "href": "slides/practical/index.html#regularisierung-6",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nTransfer-Learning ist das Adaptieren von einem trainierten Modell, das auf einem grossen Datensatz vortrainiert worden ist (z.B. ImageNet). Bei diesem wird anschliessend der letzte Layer entfernt und entsprechend dem neu zu lernenden Task ersetzt. Anschliessend trainiert man das Netzwerk weiter. Dabei kann man beliebige Layers einfrieren (die Gewichte nicht mehr aktualisieren) oder fine-tunen (die Gewichte weiter trainieren)."
  },
  {
    "objectID": "slides/practical/index.html#transfer-learning",
    "href": "slides/practical/index.html#transfer-learning",
    "title": "Practical Considerations",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nSource: Johnson and Fouhey (2021)"
  },
  {
    "objectID": "slides/practical/index.html#regularisierung-7",
    "href": "slides/practical/index.html#regularisierung-7",
    "title": "Practical Considerations",
    "section": "4) Regularisierung",
    "text": "4) Regularisierung\nMan kann in PyTorch die Parameter einfrieren:\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False"
  },
  {
    "objectID": "slides/practical/index.html#hyper-parameter-tuning",
    "href": "slides/practical/index.html#hyper-parameter-tuning",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nIn diesem Schritt geht es nun darum verschiedene Hyper-Parameter und Architekturen Systematisch zu evaluieren. Dazu gibt es verschiedene Techniken, wie grid-search oder random-search. Wobei random-search zu bevorzugen ist."
  },
  {
    "objectID": "slides/practical/index.html#hyper-parameter-tuning-1",
    "href": "slides/practical/index.html#hyper-parameter-tuning-1",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nParametrisierbare Architektur:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
  },
  {
    "objectID": "slides/practical/index.html#squeeze-out-the-juice",
    "href": "slides/practical/index.html#squeeze-out-the-juice",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nNachdem man die besten Architekturen und Hyper-Parameter gefunden hat gibt es weitere Möglichkeiten noch mehr rauszuholen."
  },
  {
    "objectID": "slides/practical/index.html#squeeze-out-the-juice-1",
    "href": "slides/practical/index.html#squeeze-out-the-juice-1",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nModel Ensembling."
  },
  {
    "objectID": "slides/practical/index.html#squeeze-out-the-juice-2",
    "href": "slides/practical/index.html#squeeze-out-the-juice-2",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nLänger trainieren."
  },
  {
    "objectID": "slides/practical/index.html#double-descent",
    "href": "slides/practical/index.html#double-descent",
    "title": "Practical Considerations",
    "section": "Double Descent",
    "text": "Double Descent\n\nSource: Nakkiran et al. (2019)"
  },
  {
    "objectID": "slides/practical/index.html#squeeze-out-the-juice-3",
    "href": "slides/practical/index.html#squeeze-out-the-juice-3",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nWeitere Trainings-Techniken:\n\nSpezieller Optimiser (AdamW)\nKomplexe Data Augmentation Techniken (Mixup, Cutmix, RandAugment)\nRegularisierungs-Techniken (Stochastic Depth)\nLabel Smoothing"
  },
  {
    "objectID": "slides/practical/index.html#huggingface",
    "href": "slides/practical/index.html#huggingface",
    "title": "Practical Considerations",
    "section": "HuggingFace",
    "text": "HuggingFace\nHuggingFace"
  },
  {
    "objectID": "slides/practical/index.html#timm",
    "href": "slides/practical/index.html#timm",
    "title": "Practical Considerations",
    "section": "timm",
    "text": "timm\nPyTorch Image Models (timm)"
  },
  {
    "objectID": "slides/practical/index.html#links",
    "href": "slides/practical/index.html#links",
    "title": "Practical Considerations",
    "section": "Links",
    "text": "Links\n\nDS-cookie cutter\nPyTorch Lightning\nHydra\nWeights & Biases\nNeptune AI\nVersion Control Systems für ML Projekte"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Intro slides.\n\n\n\n\n\nCNNs slides.\nNeural Networks slides.\nPractical slides.",
    "crumbs": [
      "Slides"
    ]
  }
]