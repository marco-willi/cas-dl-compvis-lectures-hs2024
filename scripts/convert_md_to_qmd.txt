Convert the following *.md file (File to convert) to a *.qmd file. Consider the following convertion rules (Rules) and the example file (Example File).

Rules 
--------------

Citations:

From: {numref}`dfdfd_xy`

To: @dfdfd_xy

Figures:

From: <img src="{{< meta params.images_path >}}foto_beispiel1.png" alt="image manipulation example1" class="bg-primary mb-1" width="600px">

To: ![]({{< meta params.images_path >}}foto_beispiel1.png){width=600}

Escape all code blocks and other triple quotes with a backslash.

Convert German to English.

Convert admonitions:

To: 

::: {.callout-note appearance="simple"}

**Question**

What steps do you think the model in @fig-intro-photo-example3 performs?

:::

Remove any LaTex commented out code. For example remove anyhting between:

### BEGIN LATEX EXPORT

### END LATEX EXPORT


Convert code cells:

From:

\```{code-cell} ipython3


To: 


\```{python}
#| eval: true
#| echo: true





Example File
--------------

---
title: "Introduction"
params:
   images_path: "/assets/images/intro/"
---

# Motivation

Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.

The integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services. Examples in the field of image processing with Deep Learning are shown in @fig-intro-photo-example1, @fig-intro-photo-example2, and @fig-intro-photo-example3.

::: {#fig-intro-photo-example1}

![]({{< meta params.images_path >}}foto_beispiel1.png){width=600}

Example from [Link](https://store.google.com/intl/en/ideas/pixel-camera-features/). Left is the original image, right is the version enhanced with Deep Learning.
:::

::: {#fig-intro-photo-example2}

![]({{< meta params.images_path >}}foto_beispiel2.png){width=600}


Example from [Link](https://store.google.com/intl/en/ideas/pixel-camera-features/). Left is the original image, right is the manipulated version.
:::

::: {#fig-intro-photo-example3}

![]({{< meta params.images_path >}}foto_beispiel3.png){width=600}


Example from [Link](https://store.google.com/intl/en/ideas/pixel-camera-features/). Left is the original image, right is the manipulated version.
:::

::: {.callout-note appearance="simple"}

**Question**

What steps do you think the model in @fig-intro-photo-example3 performs?

:::



File to convert
--------------


---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

<!-- ### BEGIN LATEX EXPORT
\input{../../../slides/setup}
\input{../../../slides/macros}


\graphicspath{{../images}}

\subtitle{Segmentierung}

\begin{document}

\begin{frame}
    % Print the title page as the first slide
    \titlepage
\end{frame}

\begin{frame}{Overview}
    \tableofcontents
\end{frame}
### END LATEX EXPORT -->


<!-- ### BEGIN LATEX EXPORT
%------------------------------------------------
\section{Einführung}
%------------------------------------------------
### END LATEX EXPORT -->

# Segmentierung

## Einführung

Bei der Segmentierung (_Segmentation_) von Bildern werden einzelne Pixel im Input-Bild einem bekannten Set von Klassen (_Semantic segmentation_) oder Objekten (_Instance segmentation_) zugeordnet. {numref}`fig-segmentation-overview` illustriert die Unterschiede zwischen Image Classification, Object Detection und Segmentation.


<!-- ### BEGIN SLIDE -->
<!-- Title: Image Segmentation -->
:::{figure-md} fig-segmentation-overview
<img src="./images/overview.jpg" alt="segmentation task" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Semantic Segmentation kann als Klassifikationsproblem betrachtet werden, bei dem jedes Pixel individuell klassifiziert wird.  Semantic Segmentation ist deshalb ähnlich wie Image Classification, jedoch komplexer. {numref}`fig-segmentation-cityscapes` ziegt ein Beispiel aus einem Datensatz mit segmentierten Strassenszenen, z.B. für das Trainieren von Modellen für autonom fahrenden Autos. 


<!-- ### BEGIN SLIDE -->
<!-- Title: Semantic Segmentation: Road Segmentation-->
:::{figure-md} fig-segmentation-cityscapes
<img src="./images/road_segmentation_example.png" alt="cityscapes" class="bg-primary mb-1" width="600px">

Oben das Photo, unten die annotierte Segmentation Map. Source: {cite}`cordts_cityscapes_2016`.
:::
<!-- ### END SLIDE -->

{numref}`fig-segmentation-chest-segmentation` zeigt ein Beispiel aus der Medizin. Hier wurde ein Modell trainiert zum segmentieren von Röntgenbildern des Torsos.

<!-- ### BEGIN SLIDE -->
<!-- Title: Semantic Segmentation: Medical -->
:::{figure-md} fig-segmentation-chest-segmentation
<img src="./images/chest_segmentation.jpg" alt="chest segmentation" class="bg-primary mb-1" width="600px">

Source: {cite}`novikov_fully_2018`.
:::
<!-- ### END SLIDE -->

 Instance segmentation ist vergleichbar zu Object Detection, aber ebenfalls komplexer, da nicht nur Bounding Boxes vorhergesagt werden müssen, sondern ganze Pixel-Masken, welche die räumliche Ausdehnung einzelner Objekte definiert. {numref}`fig-segmentation-instance-segmentation` zeigt ein Beispiel.


<!-- ### BEGIN SLIDE -->
<!-- Title: Instance Segmentation -->
:::{figure-md} fig-segmentation-instance-segmentation
<img src="./images/instance_segmentation_example.jpg" alt="chest segmentation" class="bg-primary mb-1" width="800px">

Instance Segmentation. Source: {cite}`he_mask_2018`.
:::
<!-- ### END SLIDE -->

Im Folgenden werden wir Methoden für Semantic Segmentation und Instance Segmentation anschauen.


<!-- ### BEGIN LATEX EXPORT
%------------------------------------------------
\section{Semantic Segmentation}
%------------------------------------------------
### END LATEX EXPORT -->


## Semantic Segmentation

### Sliding-Window

Ein Verfahren für Semantic Segmentation wäre es jedes Pixel zu klassifizieren, indem man mit einem Sliding Window Ansatz jeweils das Pixel im Zentrum klassifiziert. Das Sliding Window würde Kontext-Informationen zur Verfügung stellen, sodass man genauer klassifizieren kann. {numref}`fig-segmentation-sliding-window` illustriert den Prozess. 

<!-- ### BEGIN SLIDE -->
<!-- Title: Sliding Window -->
:::{figure-md} fig-segmentation-sliding-window
<img src="./images/sliding_window.jpg" alt="sliding window" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Dieser Ansatz ist jedoch sehr ineffizient, da man für jedes Pixel einen Forward Pass durch das CNN durchführen müsste und man somit extrahierte Features von sich überschneidenden Sliding Windows nicht wiederverwenden würde.


### Fully Convolutional Networks

{cite}`shelhamer_fully_2016` haben eines der ersten _Fully Convolutional Networks (FCNs)_ vorgeschlagen. Ein _FCN_ besteht ausschliesslich aus Convolutional Layers (insbesondere hat es keine Fully-Connected / Linear Layers) und kann dadurch Bilder von beliebigen räumlichen Dimensionen verarbeiten und daraus eine _Segmentation Map_ derselben Dimensionalität erzeugen. Durch das Ersetzten von Fully-Connected / Linear Layers mit Convolutional Layers kann die Abhängigkeit auf eine fixe Input-Grösse eliminiert werden.

{numref}`fig-segmentation-fcn` illustriert ein _FCN_. Das _FNC_ hat im letzten Layer einen Output der Dimensionalität $H \times W \times K$ (Höhe, Breite, Tiefe), wobei $K$ die Anzahl Klassen sind. Die einzelnen klassenspezifischen Activation Maps modellieren die Wahrscheinlichkeit, dass ein Pixel zur entsprechenden Klasse gehört. Mit der _argmax_ Funktion könnte man dann jedes Pixel derjenigen Klasse mit der höchsten Wahrscheinlichkeit zuordnen.

<!-- ### BEGIN SLIDE -->
<!-- Title: Fully-Convolutional Network - Konzept -->
:::{figure-md} fig-segmentation-fcn
<img src="./images/fully_conv_slide.jpg" alt="fcn" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Das Problem mit diesem Ansatz ist, dass es viel _Compute_ (FLOPs) braucht, da die räumlichen Dimensionen der tieferen Layers immernoch der Input-Dimension entsprechen. Dadurch müssen viele Operationen ausgeführt werden da die Filters über einen grösseren Bereich convolved werden müssen.

Die ersten Layers in einem _CNN_ lernen lokale Strukturen (da das Receptive Field sehr klein ist, können diese gar nichts anderes lernen), die in weiteren Layers sukszessive aggregiert werden. Dabei wird die Anzahl Channels typischerweise erhöht, damit das _CNN_ verschiedene Variationen von Mustern erkennen kann. Dies erhöht zusätzlich den Speicherbedarf des Modells. Ausserdem braucht es genügend Layers, damit das Receptive Field (siehe {ref}`receptive-field`) genügend gross ist. Das ist nötig um akkurat zu segmentieren. 


Bei Image Classification modelliert man das globale Label des Bildes. Deshalb gibt es dieses Problem bei Image Classification nicht, da man gleichzeitig graduell die räumliche Dimension der Activation Maps reduzieren kann und somit _Compute_ über das Netzwerk etwa konstant gehalten wird. 

{cite}`shelhamer_fully_2016` haben das Problem gelöst, indem Sie die Activation Maps graduell down-sampeln mit Convolutions mit Stride $>2$ oder Pooling Layers (genau wie bei Image Classification Architekturen) jedoch Activation Maps aus verschiedenen Layers wieder hochskalieren mit einem Upsampling Verfahren (siehe {ref}`upsampling`). Dabei konkatenieren (zusammenfügen) Sie Informationen aus verschiedenen Layers um Activation Maps zu erhalten, die reichhaltige Features mit lokalem und globalem Kontext enthalten. Diese werden dann bei Bedarf mit $1 \times 1$ Convolutions  auf die gewünschte Anzahl Klassen reduziert. Siehe {numref}`fig-segmentation-fcn-upsampling` für eine Illustration.

<!-- ### BEGIN SLIDE -->
<!-- Title: Fully-Convolutional Network -->
:::{figure-md} fig-segmentation-fcn-upsampling
<img src="./images/fcn_architecture.png" alt="upsampling" class="bg-primary mb-1" width="600px">

Source: {cite}`tai_pca-aided_2017`. Architektur ist wie im FCN-Paper {cite}`shelhamer_fully_2016` angewendet.
:::
<!-- ### END SLIDE -->

<!-- 
:::{figure-md} fig-segmentation-fcn-upsampling
<img src="./images/fcn_upsampling.jpg" alt="upsampling" class="bg-primary mb-1" width="600px">

Source: {cite}`shelhamer_fully_2016`.
::: -->

Durch diese Skip-Connections, mit denen Activation Maps mitten in der Architektur direkt mit tieferen Layers verbunden sind, hat man die Ergenisse der Segmentation Maps deutlich verbessert. {numref}`fig-segmentation-improvements-skip-connections` zeigt Beispiele.


<!-- ### BEGIN SLIDE -->
<!-- Title: Fully-Convolutional Network: Resultate-->
:::{figure-md} fig-segmentation-improvements-skip-connections
<img src="./images/improvements_with_skip_connections.jpg" alt="upsampling" class="bg-primary mb-1" width="600px">

Von links nach rechts zeigt die Ergebnisse von Modellen mit Skip Connections zu immer früheren Layers. Ganz rechts ist die Ground Truth. Source: {cite}`shelhamer_fully_2016`.
:::
<!-- ### END SLIDE -->

### Encoder-Decoder Networks

Mit der Encoder-Decoder Architektur wird der Input (das Bild) räumlich graduell verkleinert (_encoded_), bis eine dichte Repräsentation entsteht (Encoding). Dieses Encoding wird dann mit einem Decoder wieder räumlich vergrössert, bis die ursprüngliche Dimensionalität wieder erreicht wurde. {numref}`fig-segmentation-fcn-deconv` illustriert den Prozess. Diese Architektur ist sehr Compute effizient und erzeugt durch die Symmetrie von Encoder und Decoder Segmentation Maps die der Input-Auflösung entsprechen.

<!-- ### BEGIN SLIDE -->
<!-- Title: Encoder-Decoder: Konzept -->
:::{figure-md} fig-segmentation-fcn-deconv
<img src="./images/fully_conv_deconv.jpg" alt="fcn" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Eine extreme Verdichtung (Encoding) wurde z.B. von {cite}`noh_learning_2015` angewendet, siehe {numref}`fig-segmentation-fcn-deconv-paper`. Dadurch wird das Modell wesentlich effizienter da die Activation Maps relativ klein sind.

<!-- ### BEGIN SLIDE -->
<!-- Title: Encoder-Decoder -->
:::{figure-md} fig-segmentation-fcn-deconv-paper
<img src="./images/fcn_deconv.jpg" alt="fcn" class="bg-primary mb-1" width="600px">

Source: {cite}`noh_learning_2015`.
:::
<!-- ### END SLIDE -->


(upsampling)=
### Upsampling


In Encoder-Decoder Architekturen muss das Encoding vom Input wieder decoded werden, sodass die räumliche Dimensionalität vom Input wieder erreicht wird. Es braucht also Komponenten im Netzwerk, die Activation Maps räumlich raufskalieren können (Upsamling). Dazu gibt es mehrere Möglichkeiten.

Die Varianten _Bed of Nails_ (Nagelbrett) und _Nearest Neighbour_ sind in {numref}`fig-segmentation-unpooling` gezeigt. Dabei werden die Inputs einfach kopiert und entlang Höhe/Breite dupliziert oder mit Nullen aufgefüllt.

<!-- ### BEGIN SLIDE -->
<!-- Title: Upsampling: Unpooling -->
:::{figure-md} fig-segmentation-unpooling
<img src="./images/unpooling.jpg" alt="unpooling" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Eine andere Variante vom Upsampling, insbesondere geeignet bei symmetrischen Architekturen wie den Encoder / Decoder Architekturen, ist es Max Pooling Layers  (im Encoder) mit Unpooling Layers (im Decoder) zu verknüpfen. Insbesondere kann man sich jeweils in den Max Pooling Layers merken an welcher Position der maximale Wert aufgetaucht ist. Dann kann beim Unpooling der entsprechende Wert an dieselbe Position gesetzt werden, anstatt wie bei Bed of Nails immer an Position $(0,0)$. Dadurch gehen exakte Positionen von den Activations nicht verloren, was wichtig ist wenn man pixelgenau segmentieren möchte. Um dies zu erreichen muss man beim Trainieren des Modells (und bei Inference) in einer _switch_ Variablen abspeichern wo der maximale Wert aufgetaucht ist. Siehe {numref}`fig-segmentation-unpooling-switch` und {numref}`fig-segmentation-max-unpooling` für eine Illustration.


<!-- ### BEGIN SLIDE -->
<!-- Title: Upsampling: Unpooling with Switch -->
:::{figure-md} fig-segmentation-unpooling-switch
<img src="./images/unpooling_switch.jpg" alt="unpooling switch" class="bg-primary mb-1" width="600px">

Source: {cite}`noh_learning_2015`.
:::
<!-- ### END SLIDE -->

<!-- ### BEGIN SLIDE -->
<!-- Title: Upsampling: Unpooling with Switch -->
:::{figure-md} fig-segmentation-max-unpooling
<img src="./images/max_unpooling.jpg" alt="max unpooling" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Eine andere Möglichkeit ist Interpolation. Man kann einen Input, wie bei der Bildverarbeitung, vergrössern mit Interpolation. {numref}`fig-segmentation-bilinear-interpolation` illustriert ein Beispiel anhand Bilinearer Interpolation.


<!-- ### BEGIN SLIDE -->
<!-- Title: Upsampling: Interpolation -->
:::{figure-md} fig-segmentation-bilinear-interpolation
<img src="./images/bilinear_interpolation.jpg" alt="bilinear interpolation" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

Diese Upsamling Methoden haben alle gemeinsam, dass sie nicht gelernt sind und somit keine Parameter haben, die man mit Gradient Descent optimieren könnte. Eine lernbare Variante von Upsamling sind _transposed Convolutions_. _transposed Convolutions_ (auch _fractionally strided convolutions_ oder _deconvolutions_) erzielen diesen Effekt. Diese Operation definiert keine Inverse der Convolution.


{numref}`fig-segmentation-transposed-conv-simple` illustriert eine _transposed Convolution_ mit Stride 2, Kernel 2 und einem Input mit Seitenlänge 2. Gezeigt sind die einzelnen Resultate an jeder Stelle vom Input und das addierte Resultat.


<!-- ### BEGIN SLIDE -->
<!-- Title: Transposed Convolution -->
:::{figure-md} fig-segmentation-transposed-conv-simple
<img src="./images/transposed_conv_example.png" alt="transposed convolution" class="bg-primary mb-1" width="600px">

Transposed Convolution mit Kernel-Size 2 und Stride 2.
:::
<!-- ### END SLIDE -->

{numref}`fig-segmentation-transposed-conv` illustriert eine transposed Convolution mit Stride 2, Kernel 3 und einem Input mit Seitenlänge 2. Hier wird gezeigt, dass es im Output Überschneidungen gibt, welche addiert werden.


<!-- ### BEGIN SLIDE -->
<!-- Title: Transposed Convolution -->
:::{figure-md} fig-segmentation-transposed-conv
<img src="./images/transposed_conv.jpg" alt="transposed convolution" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

{numref}`fig-cnn-transposed-gif` zeigt ein Beispiel indem eine transposed Convolution als Convolution visualisiert wird. Dabei wird wird ein $3x3$ Kernel über einen $2x2$ Input convolved, der mit $2x2$ Padding erweitert wurde. Auch komplexere transposed Convolutions z.B. mit Stride $>1$ kann man mit Convolutions darstellen, wenn der Input entsprechend angepasst wird.

:::{figure-md} fig-cnn-transposed-gif
<img src="./images/no_padding_no_strides_transposed.gif" class="bg-primary mb-1" width="200px">

_transposed convolution_ eines 3x3 _kernel_ über einen 2x2 Input ohne _padding_ mit _stride_ 1x1. Source {cite}`dumoulin_guide_2016`
:::

Der Name transposed Convolution kommt daher, dass man eine Convolution mit einer Matrix-Multiplikation ausdrücken kann und die transposed Convolution mit der entsprechenden transponierten Matrix. {numref}`fig-segmentation-transposed-conv-matrix` zeigt ein Beispiel.

<!-- ### BEGIN SLIDE -->
<!-- Title: Transposed Convolution: Matrix Notation -->
:::{figure-md} fig-segmentation-transposed-conv-matrix
<img src="./images/transposed_conv_as_matrix_mult.jpg" alt="transposed convolution matrix" class="bg-primary mb-1" width="600px">

$x$ ist der Kernel, $X$ der Kernel als Matrix, $a$ der Input. Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->


```{note}
Wenn Sie transposed Convolutions mit PyTorch verwenden sollten Sie die Dokumentation: [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) lesen. Dort hat es Formeln um die gewünschte Output-Dimensionalität bei entsprechender Parameterisierung genau zu berechnen.
```


Der folgende Code zeigt ein Beispiel in PyTorch.

```{code-cell} ipython3
from typing import List

from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns
import torch
from torch.nn import functional as F

to_upsample = torch.tensor([[1, 2], [3, 4]]).unsqueeze(0).to(torch.float)

def display_arrays(arrays: List[np.ndarray], titles: List[str]):
    """ Display Arrays """
    num_arrays = len(arrays)
    kwargs = {
        'annot': True, 'cbar': False, 'vmin': 0, 'vmax': 10,
        'xticklabels': False, 'yticklabels': False}
    fig, ax = plt.subplots(
        figsize=(3 * num_arrays, 3), ncols=num_arrays)
    for i, (array, title) in enumerate(zip(arrays, titles)):
        sns.heatmap(array, **kwargs, ax=ax[i]).set(
            title=f"{title} - Shape {array.shape}")
    plt.show()

weight = torch.tensor(
    [[1, 2, 3], [0, 1, 2], [0, 1, 2]]).unsqueeze(
        0).unsqueeze(0).to(torch.float)
weight.shape

out = F.conv_transpose2d(
    input=to_upsample, weight=weight,
    stride=2, padding=0, output_padding=0)

arrays_to_plot = [np.array(x) for x in [
    to_upsample[0, : :], weight[0, 0, : :], out[0, : :]]]
display_arrays(
    arrays=arrays_to_plot,
    titles=["Input", "Filter", "Output"])

```


### UNet

Eine bekannte Architektur ist U-Net {cite}`ronneberger_u-net_2015`. Damit hat man erfolgreich Bilder im Bereich Medizin / Biologie segmentiert. U-Net inspirierte Architekturen werden ausserdem in zahlriechen anderen Anwendungen verwendet (z.B. Image Generation {cite}`rombach_high-resolution_2022`). {numref}`fig-segmentation-unet-example2` zeigt Beispiele einer solchen Segmentation. 

<!-- ### BEGIN SLIDE -->
<!-- Title:  UNet: Semantic Segmentation -->
:::{figure-md} fig-segmentation-unet-example2
<img src="./images/unet_example2.jpg" alt="unet example" class="bg-primary mb-1" width="600px">

Source: {cite}`ronneberger_u-net_2015`.
:::
<!-- ### END SLIDE -->

Das Besondere an U-Net ist, dass man eine Encoder / Decoder Architektur verwendet hat und gleichzeitig Shortcut/Skip Connections verwendet hat um verschiedene Layers direkt miteinander zu verbinden. {numref}`fig-segmentation-unet` zeigt die Architektur (U-förmig, deshalb der Name) von UNet, inklusive der _copy and crop_ Operationen, die die Layers verbinden. Durch diese Verbindungen werden detaillierte low-level Informationen direkt zm Output kopiert und müssen nicht durch das _bottleneck_ im Encoder, wo eventuell nicht genug Kapazität ist diese zu bewahren. Durch das _Bottleneck_ werden globale Informationen encoded, welche für alle Positionen relevant sind. Dadurch wird die Segmentierung im Detail genauer.

<!-- ### BEGIN SLIDE -->
<!-- Title:  UNet: Architektur -->
:::{figure-md} fig-segmentation-unet
<img src="./images/unet.jpg" alt="unet" class="bg-primary mb-1" width="600px">

Source: {cite}`ronneberger_u-net_2015`.
:::
<!-- ### END SLIDE -->

Zusätzlich hat man beim Trainieren der Modelle die einzelnen Pixel unterschiedlich gewichtet. Je näher ein Pixel am Rand eines Objektes ist, desto höher wurde dessen Loss gewichtet. Dadurch kann man mit UNet besonders scharfe Trennungen zwischen Objekten lernen, was in der Medizin wichtig sein kann, wenn man z.B. Zellen segmentiert die sehr nahe beieinander sind.


### Loss

Da Semantic Segmentation im wesentlichen eine Klassifikation auf Pixel-Level durchführt, kann man dieselbe Loss-Funktion verwenden wie bei Image Classification, nur auf Pixel-Ebene. {numref}`fig-segmentation-pixel-level-softmax` zeigt, dass die Softmax-Funktion über alle Pixel-Positionen individuell angewandt wird um Wahrscheinlichkeitsverteilungen pro Pixel zu erhalten. 


<!-- ### BEGIN SLIDE -->
<!-- Title:  Loss -->
:::{figure-md} fig-segmentation-pixel-level-softmax
<img src="./images/pixel_level_softmax.jpg" alt="unet" class="bg-primary mb-1" width="600px">

Pixel-Level Softmax für ein Pixel illustriert. Output ist $H \times W \times K$. 
:::
<!-- ### END SLIDE -->

 Oft verwendet man deshalb _per pixel cross entropy_ als Loss-Funktion, wobei $N$ die gesamtzahl Pixel referenziert:

\begin{align}
CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
\end{align}


<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Cross-Entropy Kostenfunktion}
    \begin{align*}
        CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
    \end{align*}    
    \scriptsize
    \begin{itemize}
        \item $N$: Anzahl Pixel
        \item $K$: Anzahl Klassen
        \item $\hat{y}_j, y_j$: Vorhersage / Ground-Truth für Klasse $j$ 
    \end{itemize}
\end{frame}
### END LATEX EXPORT -->


<!-- ### BEGIN LATEX EXPORT
%------------------------------------------------
\section{Instance Segmentation}
%------------------------------------------------
### END LATEX EXPORT -->


## Instance Segmentation

Bei Instance Segmentation möchte man alle Objekte in einem Bild detektieren und segmentieren. Der Ansatz ist einfach: Man führt Object Detection aus und modelliert dann zusätzlich zur Bounding Box noch eine Segmentation Mask.

### Mask R-CNN

Eine der bekanntesten Modelle ist eine Erweiterung von _Faster R-CNN_: das _Mask R-CNN_. {numref}`fig-segmentation-maskrcnn` illustriert den zusätzlichen Output-Head, der für die Mask-Prediction zuständig ist. 

<!-- ### BEGIN SLIDE -->
<!-- Title: Mask R-CNN -->
:::{figure-md} fig-segmentation-maskrcnn
<img src="./images/mask_rcnn.jpg" alt="mask r-cnn" class="bg-primary mb-1" width="600px">

Source: {cite}`he_mask_2018`.
:::
<!-- ### END SLIDE -->

_Mask R-CNN_ modelliert die Masken mit einem Output der Grösse $NxNxK$, wobei $NxN$ die räumliche Dimension des _RoI pooling_ der einzelnen Objekte ist. $K$ ist die Anzahl Klassen. Es werden also immer Masken für alle Klassen generiert. Beim Trainieren der Modelle wird jeweils die Maske der Ground Truth Klasse $k$ evaluiert und entsprechend der _binary pixel-wise cross-entropy_ Loss berechnet. 

\begin{align}
\text{binary CE} = - \sum_{i=1}^{N^2}  \Big( (\log \hat{y}_k^{(i)})^{y_k^{(i)}} + (\log (1-\hat{y}_k^{(i)}))^{(1 - y_k^{(i)})} \Big) 
\end{align}

<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Kostenfunktion}
    \begin{align*}
        \text{binary CE} = - \sum_{i=1}^{N^2}  \Big( (\log \hat{y}_k^{(i)})^{y_k^{(i)}} + (\log (1-\hat{y}_k^{(i)}))^{(1 - y_k^{(i)})} \Big) 
    \end{align*}
           
\end{frame}
### END LATEX EXPORT -->

Auch verwendet _Mask R-CNN_ eine verbesserte Version des _RoI pooling_, nämlich _RoI align_, damit das Modell die Masken noch genauer mit dem Objekt im Input alignieren kann (da die räumliche Auflösung der _RoI_ wesentlich kleiner ist als im Input Objekt.)

<!-- ### BEGIN SLIDE -->
<!-- Title: Mask R-CNN: Masks -->
:::{figure-md} fig-segmentation-mask-output
<img src="./images/mask_rcnn_output.jpg" alt="mask rcnn output" class="bg-primary mb-1" width="600px">

Source: {cite}`he_mask_2018`.
:::
<!-- ### END SLIDE -->

{numref}`fig-segmentation-mask-targets` zeigt Beispiele von Trainings-Daten. Man beachte, dass die _ground truth_ Masken jeweils relativ zur vorhergesagten _bounding box_ zugeschnitten wird.

<!-- ### BEGIN SLIDE -->
<!-- Title: Mask R-CNN: Mask Ground Truth -->
:::{figure-md} fig-segmentation-mask-targets
<img src="./images/mask_rcnn_targets.jpg" alt="mask targets" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->

_Mask R-CNN_ funktioniert erstaunlichz gut, wie Resultate aus {cite}`he_mask_2018` zeigen, siehe {numref}`fig-segmentation-mask-results`.


<!-- ### BEGIN SLIDE -->
<!-- Title: Mask R-CNN: Resultate -->
:::{figure-md} fig-segmentation-mask-results
<img src="./images/mask_rcnn_results.jpg" alt="mask rcnn results" class="bg-primary mb-1" width="600px">

Source: {cite}`he_mask_2018`.
:::

<!-- ### END SLIDE -->


<!-- ### BEGIN LATEX EXPORT
%------------------------------------------------
\section{Panoptic Segmentation}
%------------------------------------------------
### END LATEX EXPORT -->


## Panoptic Segmentation

In _panoptic segmentation_ möchte man ein Bild vollständig segmentieren und dabei Semantic segmentation und Instance segmentation kombinieren. Dazu unterscheidet man zwischen _things_ (Objekten) und _stuff_ (der Rest, wie Hintergrund, etc.). {numref}`fig-segmentation-things-and-stuff` zeigt ein Beispiel.

<!-- ### BEGIN SLIDE -->
<!-- Title: Panoptic Segmentation -->
:::{figure-md} fig-segmentation-things-and-stuff
<img src="./images/things_and_stuff.jpg" alt="things and stuff" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->


Der Output eines solchen Modells sieht man in {numref}`fig-segmentation-panoptic`.

<!-- ### BEGIN SLIDE -->
<!-- Title: Panoptic Segmentation -->
:::{figure-md} fig-segmentation-panoptic
<img src="./images/panoptic_segmentation.jpg" alt="panoptic" class="bg-primary mb-1" width="600px">

Source: {cite}`johnson_eecs_2019`.
:::
<!-- ### END SLIDE -->



<!-- ### BEGIN LATEX EXPORT
%------------------------------------------------
\section{Metriken}
%------------------------------------------------
### END LATEX EXPORT -->

## Metriken

### Pixel Accuracy (PA)

Die _pixel accuracy_ ist das Verhältnis zwischen korrekt klassifizierten Pixeln und der gesamten Anzahl Pixeln. Für $K + 1$ Klassen (inklusive Background-Klasse), ist _pixel accuracy_ definiert als:

\begin{equation}
\text{PA} = \frac{\sum_{i=0}^Kp_{ii}}{\sum_{i=0}^K\sum_{j=0}^K p_{ij}}
\end{equation}

Wobei $p_{ij}$ die Anzahl Pixel der Klasse $i$ welche als Klasse $j$ vorhergesagt wurde.

<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Pixel Accuracy}
    \begin{equation*}
        \text{PA} = \frac{\sum_{i=0}^Kp_{ii}}{\sum_{i=0}^K\sum_{j=0}^K p_{ij}}
    \end{equation*}
\end{frame}
### END LATEX EXPORT -->


### Mean Pixel Accuracy (MPA)

_Mean pixel accuracy_ ist eine Erweiterung der _pixel accuracy_. Das Verhältnis der korrekten Pixeln zu allen Pixeln wird pro Klasse berechnet und dann gemittelt über die Anzahl Klassen.

\begin{equation}
\text{MPA} = \frac{1}{K+1} \sum_{i=0}^K \frac{p_{ii}}{\sum_{j=0}^K p_{ij}}
\end{equation}

<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Mean Pixel Accuracy}
    \begin{equation*}
        \text{MPA} = \frac{1}{K+1} \sum_{i=0}^K \frac{p_{ii}}{\sum_{j=0}^K p_{ij}}
    \end{equation*}
    \scriptsize
    Accuracy gemittelt über alle Klassen (gleich gewichtet).
\end{frame}
### END LATEX EXPORT -->


### Intersection over Union (IoU)

Diese Metrik wird oft verwendet in Semantic segmentation. Das ist die Fläche der _intersection_ von Vorhersage und _ground truth_, geteilt durch die _union_ zwischen Vorhersage und _ground truth_.

\begin{equation}
\text{IoU} = \frac{\lvert A \cap B \rvert}{\lvert A \cup B \rvert}
\end{equation}

<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Intersection over Union (IoU)}
    \begin{equation*}
        \text{IoU} = \frac{\lvert A \cap B \rvert}{\lvert A \cup B \rvert}
    \end{equation*}
    \scriptsize
    Dieselbe Metrik wie bei Object Detection, nur dass die Regionen nicht quadratisch sind.
        
\end{frame}
### END LATEX EXPORT -->

### Mean Intersection over Union (M-IoU)

M-IoU ist die durchschnittliche _IoU_ über alle Klassen.

### Precision / Recall / F1

_Precision_ ist der Anteil der als positiv klassifizierten Sample, die effektiv positiv sind:

$\text{Precision} = \frac{TP}{TP + FP}$

_Recall_ ist der Anteil der positiven Samples, die man korrekt als solche erkannt hat:

$\text{Recall} = \frac{TP}{TP + FN}$

_F-1_ ist das harmonische Mittel von _precision_ und _recall_:

$\text{F1} = \frac{2 \text{Precision Recall}}{\text{Precision} + \text{Recall}}$

<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Precision / Recall / F1}

    $\text{Precision} = \frac{TP}{TP + FP}$ \\


    $\text{Recall} = \frac{TP}{TP + FN}$ \\
    
    F-1 ist das harmonische Mittel von precision und recall: \\
    
    $\text{F1} = \frac{2 \text{Precision Recall}}{\text{Precision} + \text{Recall}}$
        
\end{frame}
### END LATEX EXPORT -->

### Dice Koeffizient

Der _Dice coefficient_ ist 2 Mal die _intersection_ von Vorhersage und _ground truth_, dividiert durch die gesamte Anzahl Pixeln. Der _Dice coefficient_ ist somit ähnlich wie der _IoU_.

\begin{equation}
\text{Dice} = \frac{2 \lvert A \cap B \rvert}{\lvert A \rvert + \lvert  B \rvert}
\end{equation}

<!-- ### BEGIN LATEX EXPORT
\begin{frame}{Dice Koeffizient}
    \begin{equation*}
        \text{Dice} = \frac{2 \lvert A \cap B \rvert}{\lvert A \rvert + \lvert  B \rvert}
    \end{equation*}
    \scriptsize
    Sehr ähnlich wie IoU. Benutzung ist domänenspezifisch, z.B. Medizin.
\end{frame}
### END LATEX EXPORT -->

## PyTorch

Es gibt verschiedene Möglichkeiten _segmenation_ in PyTorch anzuwenden. Es empfiehlt sich ein _segmentation / object_detection_ Framework zu verwenden. 

Ein Beispiel ist [Detectron2](https://github.com/facebookresearch/detectron2). Dort gibt es vortrainierte Modelle die man direkt einsetzten kann, oder die man auf einen eigenen Datensatz adaptieren kann.

Auch mit [torchvision](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) kann man Segmentation ausführen.

<!-- ### BEGIN LATEX EXPORT
%------------------------------------------------
\section{References}
%------------------------------------------------

\begin{frame}[allowframebreaks]{References} 
    \bibliographystyle{unsrt}
    \scriptsize
    \bibliography{references}
\end{frame}

\end{document}
### END LATEX EXPORT -->