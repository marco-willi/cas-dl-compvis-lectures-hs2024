---
title: "Neural Networks"
params:
   images_path: "/assets/images/neural_networks/"
---

# Biologische Neuronale Netzwerke

## Neuronen

![Schematische Darstellung von verbundenen Neuronen. Source: @phillips_speed_2015](/images/neural_networks/connected_neurons.png){width=600,#fig-nn-neurons}



## Visueller Cortex

![Representation von Transformationen im visuellen Cortex. Source: @kubilius_ventral_2017](/images/neural_networks/ventralvisualstream_v2.png){width=600,#fig-nn-visual-cortex}


## Multilayer Perceptron

![Ein neuronales Netzwerk mit zwei _Hidden Layer_. Die Linien zeigen Verbindungen zwischen den Neuronen. Source: @li_cs231n_2022](/images/neural_networks/mlp.jpeg){width=600,#fig-mlp-structure}




# Von Linearen Modellen zu Neuronalen Netzwerken



## Lineares Modell

Ein lineares Modell hat folgende Form:

\begin{equation}
   f(\mathbf{x}^{(i)}) = \mathbf{W} \mathbf{x}^{(i)}  +  \mathbf{b}
\end{equation}


## Neuronales Netzwerk

\begin{equation*}
   f(\mathbf{x}^{(i)}) = \mathbf{W}^{(2)} g\big(\mathbf{W}^{(1)} \mathbf{x}^{(i)}  +  \mathbf{b}^{(1)} \big)  +  \mathbf{b}^{(2)}
\end{equation*}


## Activation Function

\begin{equation}
\text{ReLU}(x) = \begin{cases}
x, & \text{if } x \geq 0 \\
0, & \text{if } x < 0
\end{cases}
\end{equation}



```{python}
#| label: fig-nn-relu_vs_linear
#| fig-cap: "Linear (left) vs non-linear (right) activation function."

from matplotlib import pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.neural_network import MLPClassifier
from mlxtend.plotting import plot_decision_regions


def plot_non_linear_vs_linear():
    fig, ax = plt.subplots(figsize=(12, 6), ncols=2)

    X, y = make_blobs(
        n_samples=100,
        n_features=2,
        random_state=0,
        cluster_std=0.5,
        centers=[(-1, -1), (1, 1), (-1, 1), (1, -1)],
    )
    y = np.where(y < 2, 1, 0)

    clf = MLPClassifier(
        hidden_layer_sizes=[10],
        activation="identity",
        max_iter=200,
        random_state=123,
        learning_rate_init=1.0,
    ).fit(X, y)
    _ = plot_decision_regions(X, y, clf, ax=ax[0])

    clf = MLPClassifier(
        hidden_layer_sizes=[10],
        activation="relu",
        max_iter=400,
        random_state=123,
        learning_rate_init=0.1,
    ).fit(X, y)
    _ = plot_decision_regions(X, y, clf, ax=ax[1])

    _ = ax[0].set_title("Linear: $g(x) = x$")
    _ = ax[1].set_title("Non-Linear: $g(x) = ReLU(x)$")

    return fig

fig = plot_non_linear_vs_linear()
fig.show()
```




# References

::: {#refs}
:::

