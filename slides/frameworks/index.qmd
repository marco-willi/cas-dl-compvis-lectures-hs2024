---
title: "Deep Learning Frameworks"
params:
   images_path: "/assets/images/frameworks/"
---

# Overview

- Software
- PyTorch
- Hardware
- References


# Software

## Deep Learning Frameworks

Components / important features of such frameworks are:

- Fast development and testing of neural networks
- Automatic differentiation of operations
- Efficient execution on diverse hardware

## Frameworks

![Frameworks (from @li_cs231n_2022).]({{< meta params.images_path >}}frameworks.png){width=100% height=70%}

## Computational Graph & Autograd

The core of neural networks is the *Computational Graph*. Dependent operations are automatically integrated into a *directed acyclic graph (DAG)*. Gradients are tracked as needed, allowing variables to be efficiently updated/trained.

## Computational Graph

\begin{equation*}
    f(\m{A}, \m{B}, \m{C}) =  \sum_{ij} \big((\m{A} \odot \m{B}) + \m{C}\big)_{ij}
\end{equation*}

![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\linewidth}

## Computational Graph & Autograd

\begin{align*}
    f(\m{A}, \m{B}, \m{C}) &=  \sum_{ij} \big((\m{A} \odot \m{B}) + \m{C}\big)_{ij} \\
    \m{D} &= \m{A} \odot \m{B} \\
    \m{E} &= D + \m{C} \\
    F &= \sum_{ij} \m{E}_{ij} 
\end{align*}

**Partial Derivative with Autograd and Chain Rule**

\begin{equation*}
    \frac{\partial F}{\partial A_{ij}} = \frac{\partial F}{\partial \m{E}} \frac{\partial \m{E}}{\partial \m{D}} \frac{\partial \m{D}}{\partial \m{A}_{ij}}
\end{equation*}

![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\linewidth}


## Numpy Example

```{python}
#| eval: true
#| echo: true

import numpy as np

np.random.seed(123)

H, W = 2, 3

a = np.random.random(size=(H, W))
b = np.random.random(size=(H, W))
c = np.random.random(size=(H, W))

d = a * b
e = d + c
f = e.sum()

df_de = 1.0
de_dd = 1.0
de_dc = c
dd_da = b

df_da = df_de * de_dd * dd_da

print(df_da)
```

![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\linewidth}


## PyTorch Example

```{python}
#| eval: true
#| echo: true

import torch

np.random.seed(123)

H, W = 2, 3

a = torch.tensor(a, requires_grad=True)
b = torch.tensor(b, requires_grad=True)
c = torch.tensor(c, requires_grad=True)

d = a * b
e = d + c
f = e.sum()

f.backward()
print(a.grad)
```

![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\linewidth}


# PyTorch

## PyTorch - Why?

In this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources.

## Fundamental Concepts

- Tensor: N-dimensional array, like [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html)
- Autograd: Functionality to create *computational graphs* and compute gradients.
- Module: Class to define components of neural networks

## Tensors

The central data structure in PyTorch is [torch.Tensor](https://pytorch.org/docs/stable/tensors.html).

It is very similar to *numpy.array* but can be easily loaded onto GPUs. Tensors can be created in various ways, for example from lists:

```{python}
#| eval: false
#| echo: true

import torch

data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)
```

## Autograd

With [torch.autograd](https://pytorch.org/docs/stable/autograd.html), gradients can be automatically computed for a *computational graph*.

```{python}
#| eval: true
#| echo: true

import torch

x = torch.ones(5)  # input tensor
y = torch.zeros(3)  # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)

loss.backward()
```

## torch.nn

With [torch.nn](https://pytorch.org/docs/stable/nn.html), components of neural networks can be defined. These components provide methods and have a state to store data such as weights and gradients.

## torch.nn - Example

```{python}
#| eval: false
#| echo: true

from torch import nn

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
```

## torch.optim

With [torch.optim](https://pytorch.org/docs/stable/optim.html), model parameters can be optimized using various algorithms.

```{python}
#| eval: false
#| echo: true

from torch import optim
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

loss_fn = torch.nn.CrossEntropyLoss()
for i in range(0, 3):
    input, target = torch.rand(1, 28, 28), torch.randint(low=0, high=10, size=(1, ))
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```

## Training Loops

A training loop iterates over *mini-batches* and optimizes the model parameters.

```{python}
#| eval: false
#| echo: true

def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
```

## Training Loops 2

```{python}
#| eval: false
#| echo: true

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epochs = 10
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print("Done!")
```

## PyTorch Ecosystem

There are many packages that extend PyTorch with functionalities. One example is [PyTorch-Lightning](https://www.pytorchlightning.ai/) which simplifies managing training loops.

## Other Frameworks and Tools

- [TensorFlow with Keras](https://www.tensorflow.org)
- [Scikit-Learn](https://scikit-learn.org/stable/)
- [ONNX](https://onnx.ai/)
- [Monitoring: TensorBoard](https://www.tensorflow.org/tensorboard)
- [Monitoring: Weights & Biases](https://wandb.ai/site)

# Hardware

## Tensor Operations

![Matrix Multiplication (from @li_cs231n_2022).]({{< meta params.images_path >}}matrix_mult.png){width=100% height=70%}

## CPU vs GPU

![CPU vs GPU example (from @li_cs231n_2022).]({{< meta params.images_path >}}cpu_vs_gpu.png){width=100% height=70%}

## Speed Comparison

![Speed comparison (from @li_cs231n_2022), data from [Link](https://github.com/jcjohnson/cnn-benchmarks).]({{< meta params.images_path >}}speed_gpu_cpu.png){width=100% height=70%}

## Data Loading

A critical bottleneck in practice is transferring data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is called *GPU starvation*. Solutions include:

- Reading data into RAM
- Using fast disks like SSDs
- Utilizing multiple CPU threads to read data in parallel and keep it in RAM (pre-fetching)

## GPU Starvation

![The Y-axis shows the GPU utilization in percent, while the X-axis represents time. [Source](https://stackoverflow.com/questions/44598246/tensorflow-data-starved-gpu).]({{< meta params.images_path >}}gpu_starvation.png){width=100% height=70%}

## GPU Parallelism

![Data and Model Parallelism (from @li_cs231n_2022).]({{< meta params.images_path >}}parallelism.jpg){width=100% height=70%}


# References

::: {#refs}
:::
