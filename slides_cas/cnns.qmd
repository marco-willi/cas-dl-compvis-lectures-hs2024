
---
title: "Convolutional Neural Networks"
params:
   images_path: "/assets/images/cnns/"
---

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Overview

- Introduction & Motivation
- Convolutional Layers
- Properties
- Variants and Layers
- Visualizations and Architectures


# Introduction & Motivation

## Properties of Image Data

- High-Dimensional: An RGB image of size $224 \times 224$  (height, width) has = $150'528$ values.
- Locality: Nearby pixels are statistically related
- Stability under transformations: Interpretation of an image does not change under many geomoetric transformations.


![[Image Source](https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk)]({{< meta params.images_path >}}cows2_small.png)

**How does a Multilayer-Perceptron handle images?**


## Multilayer-Perceptron and Images



::: {#fig-cnns-mlp-example}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_example.png)

:::


## Multilayer-Perceptrons and High-Dimensional Inputs

::: {#fig-cnns-mlp-example2}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_example_weights.png)

:::

Dimensionality of weight matrix $\m{W}$ scales with input size.  $\m{W} \in \mathbb{R}^{d \times k}$, while $d$ the dimensionality of the inputs, and $k$ the number of neurons in the first hidden layer. $k$ must be sufficiently large to learn the necessary patterns. This can lead to practical problems regarding memory and compute.

<!-- For each neuron in the first hidden layer, the model must learn as many weights as there are input neurons. For an RGB image of size $224 \times 224$  (height, width) each neuron connects to each input via $150'528$ weights. To learn many and varying patterns a large number of neurons are often required, leading to practical problems (memory and compute) when training an MLP on such inputs. -->


## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example3}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_example_weights_as_images.png)

:::

The columns of the weight matrix $\m{W}$ (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned by reshaping them to the dimensionality of the input images $\vect{x} \in \mathbb{R}^{h \times w}$.



## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example4}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_example_pattern_matching.png)

:::


## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example-permutation}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_examples_permutation.png)

:::

Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e. $f(P(\vect{x}))$?



## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example-permutation}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_examples_permuted_weights.png)

:::

Yes! An MLP has no notion of distance and treats every connection between every input equally.


## Multilayer-Perceptrons and Translations

::: {#fig-cnns-mlp-example5}

![]({{< meta params.images_path >}}mlp_images_example_pattern_shifts.png)

:::

Often the patterns we want to learn are not spatially constraint. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. $g(\vect{x})$ where $g()$ is a spatial translation.

**How do MLPs deal with this?**

## MLP and Images

::: {#fig-cnns-mlp-example6}

![]({{< meta params.images_path >}}mlp_images_example_neuron_per_pattern.png)

:::

MLPs need to (re-) learn the same pattern at all positions, which is incredibly inefficient. This also means the training data must contain all patterns at all feasible positions, otherwise the MLP won't be able to learn how to recognize them.


## MLPs and Images

- High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.
- Locality: MLPs have no notion of locality and thus can't exploit this inherent bias in natural images.
- Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.

**What do we need?**


## Invariance and Equivariance

For many tasks small variations in the input should either **not** change the model output (invariance) or should change the output **in tandem** with the input changes (equivariance).



A function $f(\vect{x})$ (such as a layer in a neural network) of an image $\vect{x}$ is **equivariant** with respect to a transformation $g(\vect{\vect{x}})$ if:

\begin{align}
f(g(\vect{x})) = g(f(\vect{x}))
\end{align}


A function $f(\vect{x})$ is **invariant** to a transformation $g(\vect{x})$ if:

\begin{align}
f(g(\vect{x})) = f(\vect{x})
\end{align}


## Example Invariance

Example where invariance is required:


::: {#fig-cnns-invariance-example}

![When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model  $f(\vect{x})$ is thus invariant to spatial translations. ]({{< meta params.images_path >}}cows2_small_invariance_collage.png)

:::


## Example Equivariance

Example where equivariance is required:

::: {#fig-cnns-equivariance-example}

![When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model  $f(\vect{x})$ that produces the bounding boxes is thus equivariant with respect to spatial translations.]({{< meta params.images_path >}}cows2_small_detection_collage.png)

:::


# Convolutional Neural Networks

## Biological Inspiration


::: {#fig-cnns-biological-inspiration}

![Schematische Darstellung von verbundenen Neuronen. Source: @phillips_speed_2015]({{< meta params.images_path >}}connected_neurons.png)

:::


## Experiments on Cats

::: {#fig-cnns-huber-wiesel}

![Illustration [Source](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1)]({{< meta params.images_path >}}huber_wiesel_cat_experiment.jpg)

:::



## Visual Cortex

::: {#fig-cnns-visual-cortex}

![Representation of transformations in the visual cortex. Source: @kubilius_ventral_2017]({{< meta params.images_path >}}ventralvisualstream_v2.png)

:::


## CNNs

::: {#fig-cnns-intro-example}

![The activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: @li_cs231n_2022]({{< meta params.images_path >}}convnet.jpeg)

:::

## Convolutional Layers



## Convolution Operation

Convolution in Deep Learning is typically implemented as cross-correlation.

\begin{equation}
S(i, j) = (K * I)(i, j) = b + \sum_m \sum_n I(i + m, j + n) K(m, n)
\end{equation}


$I$ is the input (for example an image), $K$ is the kernel (typically smaller than $I$) and $b$ is a bias term which is being added to the weighted sum.

If $I$ is an RGB image (in the first layer of a CNN for example), the kernel $K$ would have dimensionality $3 \times K \times K$ (assuming a square kernel). More generally we learn kernels of the dimensionality $\C_{in} \times K \times K$.

Multiple kernels, let's say $$C_o$ kernels, can be grouped together: $C_o \times C_{in} \times K \times K$.

We often referr to such tensors as filters or filter banks.

## Convolution on RGB Images

::: {#fig-cnns-intro-example}

![A 2D convolution applied to an image. Illustrated is the element-wise multiplication of the kernel weights $\Omega$ with the corresponding RGB input values at a specific position. Source:  @prince_understanding_2023]({{< meta params.images_path >}}convolution_prince_rgb.png)

:::



<!-- ## Convolution on rGB Images

::: {#fig-cnns-intro-example}

![Illustration Convolution. Source:  @prince_understanding_2023]({{< meta params.images_path >}}convolution_prince_example.png)

:::  -->




## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}cnn_conv_one_number.jpg){width=100% height=70%}


## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map.jpg){width=100% height=70%}



## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map2.jpg){width=100% height=70%}



## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map3.jpg){width=100% height=70%}



## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map4.jpg){width=100% height=70%}



## Hyper-Parameters

Convolutional Layers are parameterized:

- Depth: How many activation maps?
- Padding: How much padding is added to the input?
- Stride: What is the step size of the convolution?
- Kernel-Size: What is the kernel size?



## Padding: Why?

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}padding_issue.jpg){width=100% height=70%}


## Padding

![Left: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output.]({{< meta params.images_path >}}padding.png){width=100% height=70%}


## Padding

![Left: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output.]({{< meta params.images_path >}}padding.png){width=100% height=70%}


## Padding and Stride

![Stride with Padding. Red indicates the position of the corresponding filter value on the input activations.]({{< meta params.images_path >}}stride_and_padding.png){width=100% height=70%}


## Padding and Stride: Animations

@dumoulin_guide_2016 has created some animations to better understand convolutions, available here: [Link]({{< meta params.images_path >}}https://github.com/vdumoulin/conv_arithmetic).



## Calculations

You can calculate the dimensionality of the activation maps with the following formulas:

- $i$: Side length of the input activations (assumption: square inputs)
- $k$: Kernel size (assumption: square kernel)
- $o$: Side length of the output activation maps
- $s$: Stride (assumption: same stride along spatial dimensions)
- $p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)


## Calculations

This formula covers all scenarios!

**Size of Activation Map**

\begin{equation}
o = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1
\end{equation}



## Quiz

**Scenario:**

- Input: 3 x 32 x 32
- Convolution: 10 filters with 5x5 kernel size, stride=1, pad=2

What is the size of the activation map?

How many weights are there?

**Size of Activation Map**

\begin{equation}
o = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1
\end{equation}


# Properties

## Sparse Connectivity and Parameter Sharing

**Local (Sparse) Connectivity**: Neurons are only locally connected.

**Parameter Sharing**: Weights of a neuron are applied locally but are the same across the entire input.




## Convolution: Is Parameter Sharing Always Useful?

**Question**: Is parameter sharing always useful?





## MLP Parameters

```{python}
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class MLP(nn.Module):

    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)
        self.hidden_layer2 = nn.Linear(64, 32)
        self.output_layer = nn.Linear(32, 10)

    def forward(self, x):
        x = self.flatten(x)
        x = torch.relu(self.hidden_layer1(x))
        x = torch.relu(self.hidden_layer2(x))
        x = self.output_layer(x)
        return x

net = MLP()
print(torchinfo.summary(net, input_size=(1, 3, 32, 32)))
```


## CNN Parameters

```{python}
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class CNN(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)
        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)
        self.flatten = nn.Flatten()
        self.output_layer = nn.Linear(16 * 8 * 8, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.flatten(x)
        x = self.output_layer(x)
        return x

cnn = CNN()
print(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))
```



## Quiz: Linear Transformation vs Convolution

![Input in 2-D (top left), the flat version (bottom left), expected output (right), and unknown transformation (center).]({{< meta params.images_path >}}linear_transf.png){width=100% height=70%}


## Translation Invariance / Equivariance

Given a translation $g()$, which spatially shifts inputs:

- Translation invariance: $f(g(x))=f(x)$
- Translation equivariance: $f(g(x))=g(f(x))$

Convolutions are translation equivariant: [Example Video]({{< meta params.images_path >}}https://www.youtube.com/embed/qoWAFBYOtoU?start=50)

## Stacking Convolutions

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_stacking.jpg){width=100% height=70%}


## Receptive Field

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}receptive_field.jpg){width=100% height=70%}



## Receptive Field

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}receptive_field2.jpg){width=100% height=70%}




# Variants and Layers

## Dilated Convolutions

::: {#fig-cnn-dilated-conv1}

![Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.]({{< meta params.images_path >}}dilation1.png)

:::

## Dilated Convolutions

::: {#fig-cnn-dilated-conv2}

![Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.]({{< meta params.images_path >}}dilation2.png)

:::

## 1x1 Convolutions

::: {#fig-cnn-1x1}

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}1x1_conv.jpg){width=100% height=70%}

:::

## Depthwise Separable Convolutions

::: {#fig-cnn-depthwise-separable}
![Source: [https://paperswithcode.com/method/depthwise-convolution]({{< meta params.images_path >}}https://paperswithcode.com/method/depthwise-convolution)]({{< meta params.images_path >}}depthwise.png){width=100% height=70%}

:::

## Depthwise Separable Convolutions

![Source: @yu_multi-scale_2016]({{< meta params.images_path >}}depthwise_separabel.png){width=100% height=70%}



## Pooling Layers

![Source: @li_cs231n_2022]({{< meta params.images_path >}}pool.jpeg){width=300px}


## Max Pooling

::: {#fig-cnn-max-pooling}

![Max pooling, input (left) and output (right).]({{< meta params.images_path >}}max_pooling.png)
:::

## Average Pooling

::: {#fig-cnn-avg-pooling}

![Average pooling, input (left) and output (right).]({{< meta params.images_path >}}average_pooling.png){width=100% height=70%}

:::

## Other Pooling Layers

**Global Average Pooling** is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.


## Global Average Pooling

::: {#fig-cnn-global-avg-pooling}

![Global Average pooling, input (left) and output (right).]({{< meta params.images_path >}}global_average_pooling.png){width=100% height=70%}

:::


# Visualizations and Architectures

## Learned Filters

![Source: @krizhevsky_imagenet_2012]({{< meta params.images_path >}}learned_filters.png){width=100% height=70%}



# References

::: {#refs}
:::
