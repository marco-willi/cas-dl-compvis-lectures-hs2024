---
title: "Convolutional Neural Networks"
params:
   images_path: "/assets/images/cnns/"
---

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

# Motivation

CNNs work similarly to MLPs: They consist of neurons with weights and biases arranged in layers. CNNs also have an output with which a differentiable loss function can be calculated so that the weights and biases can be adjusted using backpropagation.

Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).

The input to an MLP is a vector $\vect{x}^{(i)}$, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers), see @fig-cnn-mlp.

:::{#fig-cnn-mlp}

![]({{< meta params.images_path >}}mlp.jpeg){width=600}

Source: @li_cs231n_2022
:::

The fully connected layers can only process 1-D vectors. Therefore, images $\in \mathbb{R}^{H \times W \times C}$ must be flattened into 1-D vectors $\in \mathbb{R}^p$. Here, $p= H \times W \times C$. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see @fig-cnn-spatial-structure-mlp for an illustration in an MLP and @fig-cnn-mlp-images for an illustration on a linear model).

:::{#fig-cnn-mlp-images}

![]({{< meta params.images_path >}}mlp_images.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-spatial-structure-mlp}

![]({{< meta params.images_path >}}mlp-spatial-structure.png){width=600}

Source: @li_cs231n_2023
:::

For larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.

A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see @fig-cnn-cnn-spatial). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.

:::{#fig-cnn-cnn-spatial}

![]({{< meta params.images_path >}}cnn_spatial.jpg){width=200}

Source: @johnson_eecs_2019
:::

::: {.callout-note}
CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.
:::

## Architecture

CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.

@fig-cnn-convnet shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.

:::{#fig-cnn-convnet}

![]({{< meta params.images_path >}}convnet.jpeg){width=600}

The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: @li_cs231n_2022.
:::

Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.

# Convolutional Layers

Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension $7 \times 7 \times 3$ (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter's expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of $K$ filters produces activation maps with a depth of $K$.


::: {.callout-note}
Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.
:::


::: {.callout-note}
Convolution in deep learning is typically implemented as cross-correlation.

Given:

- Kernel $K$
- Image $I$

\begin{equation}
S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)
\end{equation}
:::


:::{#fig-cnn-conv-one-number}

![]({{< meta params.images_path >}}cnn_conv_one_number.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-conv-activation-map}

![]({{< meta params.images_path >}}conv_activation_map.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-conv-activation-map2}

![]({{< meta params.images_path >}}conv_activation_map2.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-conv-activation-map3}

![]({{< meta params.images_path >}}conv_activation_map3.jpg){width=600}

Source: @johnson_eecs_2019
:::

The data is processed in mini-batches, i.e., multiple images at once, as shown in @fig-cnn-conv-activation-map4.

:::{#fig-cnn-conv-activation-map4}

![]({{< meta params.images_path >}}conv_activation_map4.jpg){width=600}

Source: @johnson_eecs_2019
:::

## Hyper-Parameters

To define a convolutional layer, various hyperparameters need to be set. Some of the most important ones are:

- Depth
- Padding
- Stride
- Kernel Size

Depth determines how many filters are to be learned and thus defines the dimensionality ($C_{\text{out}}$) of the output volume (the number of activation maps).

Stride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation. If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height.

Padding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. @fig-cnn-padding-issue illustrates the problem. @fig-cnn-padding shows an example with padding.

@fig-cnn-stride-and-padding shows the interplay between stride and padding.

:::{#fig-cnn-padding-issue}

![]({{< meta params.images_path >}}padding_issue.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-padding}

![]({{< meta params.images_path >}}padding.png){width=600}

Left: Input (yellow) with zero-padding (1, 1) (white border), middle: Filter, right: Output.
:::

:::{#fig-cnn-stride-and-padding}

![]({{< meta params.images_path >}}stride_and_padding.png){width=600}

Stride with padding. The red mark indicates the filter value position on the input activations.
:::

@dumoulin_guide_2016 has created some animations for better understanding of convolutions and published them here: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic).

### Calculations

The dimensionality of the activation maps can be calculated using the following formulas:

- $i$: Side length of the input activations (assumption: square inputs)
- $k$: Kernel size (assumption: square kernels)
- $o$: Side length of the output activation maps
- $s$: Stride (assumption: same stride along the spatial dimensions)
- $p$: Number of paddings on each side (assumption: same number of paddings along the spatial dimensions)



**Scenario: _stride_ = 1 and without _padding_**

\begin{equation}
o = (i - k) + 1
\end{equation}


::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source @dumoulin_guide_2016]({{< meta params.images_path >}}no_padding_no_strides.gif){#fig-cnn-stride-and-padding-gif3 width=200}

:::

**Scenario: _stride_ = 1 with _padding_**

\begin{equation}
o = (i - k) + 2p + 1
\end{equation}

**Scenario: Half (same) Padding -> $o = i$**

Valid for any $i$ and for odd $k = 2n + 1, n ∈ N$, $s = 1$ and $p = \floor{k/2} = n$.

\begin{align}
o &= i + 2 \floor{k/2} − (k − 1) \\
o &= i + 2 n - 2 n \\
o &= i
\end{align}

::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input with 1x1 padding and stride 1x1. Source @dumoulin_guide_2016]({{< meta params.images_path >}}same_padding_no_strides.gif){#fig-cnn-same-padding-gif width=200}

:::

**Scenario: Full Padding**

The dimensionality of the output activation can also be increased.

Valid for any $i$ and $k$, $s = 1$ and $p = k - 1$.

\begin{align}
o &= i + 2 (k − 1) - (k - 1) \\
o &= i + (k - 1)
\end{align}

::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input with 2x2 padding and stride 1x1. Source @dumoulin_guide_2016]({{< meta params.images_path >}}full_padding_no_strides.gif){#fig-cnn-stride-and-padding-gif2 width=200}

:::

**Scenario: No Padding, _stride_ > 1**

Valid for any $i$ and $k$, $s > 1$ and $p = 0$.

\begin{equation}
o = \floor{\frac{i - k}{s}} + 1
\end{equation}

::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 2x2. Source @dumoulin_guide_2016]({{< meta params.images_path >}}no_padding_strides.gif){#fig-cnn-stride-and-padding-gif1 width=200}

:::


**Scenario: _padding_ and _stride_ > 1**

Valid for any $i, k, s, p$.

\begin{equation}
o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1
\end{equation}

::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input with 1x1 zero-padding and stride 2x2. Source @dumoulin_guide_2016]({{< meta params.images_path >}}padding_strides.gif){#fig-cnn-padding-strides-gif width=200}

:::


::: {.callout-note}
With this formula, all scenarios are covered!

\begin{equation}
o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1
\end{equation}
:::


### Quiz

::: {.callout-note appearance="simple"}

**Question**

Input: 3 x 32 x 32

Convolution: 10 filters with 5x5 kernel size, stride=1, and padding=2

What is the size of the activation map?

:::


::: {.callout-note appearance="simple"}

**Question**

Input: 3 x 32 x 32

Convolution: 10 filters with 5x5 kernel size, stride=1, and padding=2

How many weights are there?

:::



## Properties

### Local (Sparse) Connectivity & Parameter Sharing

Fully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.

Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.

::: {.callout-note}
Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).
:::



The following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.

```{python}
#| eval: true
#| echo: true
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class MLP(nn.Module):

    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)
        self.hidden_layer2 = nn.Linear(64, 32)
        self.output_layer = nn.Linear(32, 10)
     
    def forward(self, x):
        x = self.flatten(x)
        x = torch.relu(self.hidden_layer1(x))
        x = torch.relu(self.hidden_layer2(x))
        x = self.output_layer(x)
        return x

net = MLP()
print(torchinfo.summary(net, input_size=(1, 3, 32, 32)))
```

```{python}
#| eval: true
#| echo: true
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class CNN(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)
        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)
        self.flatten = nn.Flatten()
        self.output_layer = nn.Linear(16 * 8 * 8 , 10)
        
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.flatten(x)
        x = self.output_layer(x)
        return x

cnn = CNN()
print(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))
```

:::{#fig-cnn-linear-transf-calc}

![]({{< meta params.images_path >}}linear_transf.png){width=800}

Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).
:::

::: {.callout-note appearance="simple"}

**Question**

How should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?

::: 

### Translation Invariance / Equivariance

Translation invariant is a function that produces the same value under translations $g()$ of the input $x$:

\begin{equation}
f(g(x))=f(x)
\end{equation}

Translation equivariant is a function that produces the same value under translations $g()$ of the input $x$, provided that it is also shifted by $g()$:

\begin{equation}
f(g(x))=g(f(x))
\end{equation}

Convolutions are translation equivariant, as illustrated well in the following example:

{{< video https://www.youtube.com/embed/qoWAFBYOtoU start="50" >}}


<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
 -->


### Stacking & Receptive Field

Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. @fig-cnn-conv-stacking illustrates the result.

:::{#fig-cnn-conv-stacking}

![]({{< meta params.images_path >}}conv_stacking.jpg){width=600}

Source: @johnson_eecs_2019
:::

A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality $H \times W \times C$! (There are also variants in higher dimensions.)

However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).

The receptive field defines which inputs influence the activations of a neuron. @fig-cnn-receptive-field illustrates the concept.

:::{#fig-cnn-receptive-field}

![]({{< meta params.images_path >}}receptive_field.jpg){width=600}

Source: @johnson_eecs_2019
:::

Stacking multiple convolutions increases the receptive field of a neuron relative to the original input (see @fig-cnn-receptive-field2).

:::{#fig-cnn-receptive-field2}

![]({{< meta params.images_path >}}receptive_field2.jpg){width=600}

Source: @johnson_eecs_2019
:::

## Variations

### Dilated Convolutions

Dilated convolutions have an additional hyperparameter, the dilation. Dilated convolutions have holes between the rows and columns of a kernel. This increases the receptive field without having to learn more parameters. This variant is also called à trous convolution. @fig-cnn-dilation-gif, @fig-cnn-dilation1, and @fig-cnn-dilation2 show examples.

::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2. Source @dumoulin_guide_2016]({{< meta params.images_path >}}dilation.gif){#fig-cnn-dilation-gif width=200}

:::

:::{#fig-cnn-dilation1}

![]({{< meta params.images_path >}}dilation1.png){width=600}

Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.
:::

:::{#fig-cnn-dilation2}

![]({{< meta params.images_path >}}dilation2.png){width=600}

Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.
:::

### 1x1 (pointwise) Convolutions

1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number ($C$) of activation maps with few parameters. For example, activation maps of dimensionality ($C \times H \times W$) can be changed to a volume of ($C2 \times H \times W$) with $C2 * (C + 1)$. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels ($C2=3$) for image generation models. @fig-cnn-1x1-conv shows an example.

:::{#fig-cnn-1x1-conv}

![]({{< meta params.images_path >}}1x1_conv.jpg){width=600}

Source: @johnson_eecs_2019
:::

### Depthwise Separable Convolutions

Depthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality ($1 \times K \times K$). @fig-cnn-depthwise shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See @fig-cnn-depthwise-separabel for a comparison of 'normal' convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.

:::{#fig-cnn-depthwise}

![]({{< meta params.images_path >}}depthwise.png){width=600}

Source: [https://paperswithcode.com/method/depthwise-convolution](https://paperswithcode.com/method/depthwise-convolution)
:::

:::{#fig-cnn-depthwise-separabel}

![]({{< meta params.images_path >}}depthwise_separabel.png){width=600}

Source: @yu_multi-scale_2016
:::

## Pooling Layers

Often, the spatial dimensionality of the activation maps needs to be successively reduced in a CNN. This reduces the number of computations and memory required. Also, information is condensed and aggregated layer by layer: high spatial resolution is often no longer necessary. We have already seen that convolutional layers with a stride > 1 can achieve this goal. However, it is also possible to use pooling layers, which do not have (learnable) parameters.

A frequently used variant is the max-pooling layer. This layer operates independently on each slice along the depth dimension and returns the maximum value. The kernel size and stride must also be defined. A stride of $2 \times 2$ with a kernel of $2 \times 2$ halves the dimensionality of the activation maps along height and width.

For any $i,k,s$ and without padding:

\begin{equation}
o = \floor{\frac{i - k}{s}} + 1
\end{equation}

:::{#fig-cnn-pool}

![]({{< meta params.images_path >}}pool.jpeg){width=300}

Source: @li_cs231n_2022
:::



```{python}
#| eval: true
#| echo: false
#| layout: [[20, 10, 10, 10]]
#| label: fig-cnn-pooling-illustration
#| fig-cap: 
#|   - "Input"
#|   - "Max Pooling"
#|   - "Average Pooling"
#|   - "Global Average Pooling"

import torch
import torch.nn as nn
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Create a 2D input tensor
input_tensor = torch.tensor([[1, 2, 3, 4],
                             [5, 6, 7, 8],
                             [9, 10, 11, 12],
                             [13, 14, 15, 16]], dtype=torch.float32)
input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # Adding batch and channel dimensions

# Define pooling layers
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)
global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

# Apply pooling operations
max_pooled = max_pool(input_tensor)
avg_pooled = avg_pool(input_tensor)
global_avg_pooled = global_avg_pool(input_tensor).view(1, 1, 1)

# Function to plot side-by-side heatmaps with manual subplot positioning
def plot_heatmap(x, title, figsize=(3, 3)):
    x = np.atleast_2d(x.squeeze().numpy())
    fig, ax = plt.subplots(figsize=figsize)
    sns.heatmap(x, ax=ax, cbar=False, annot=True, fmt=".1f", cmap="YlGnBu")
    ax.set_title(title)
    ax.set_xticks([])
    ax.set_yticks([])


plot_heatmap(input_tensor, "Input", figsize=(6, 6))

plot_heatmap(max_pooled, "Max Pooling")

plot_heatmap(avg_pooled, "Average Pooling")

plot_heatmap(global_avg_pooled, "Global Average Pooling")

```

@fig-cnn-pooling-illustration shows the result of max-pooling, average-pooling and global average-pooling. In average-pooling, instead of choosing the maximum, the average activation is calculated. Otherwise, average-pooling works the same way as max-pooling. A crucial pooling variant is global average pooling. The average of the activations is calculated along the depth dimension (i.e., no kernel size or stride needs to be defined). Activation maps with ($C \times H \times W$) are reduced to ($C \times 1 \times 1$). This is useful, for example, to directly model logits in a classification problem with $C$ classes. This allows architectures to be created that do not use fully connected layers at all.

 

## PyTorch Examples

```{python}
#| eval: true
#| echo: true
import numpy as np
import torch
from torch.nn import functional as F
import torchshow as ts
from PIL import Image
from matplotlib import pyplot as plt
```

```{python}
#| eval: true
#| echo: true
#img = Image.open({{< meta params.images_path >}}'cat.jpg')
image_path = "../assets/images/cnns/cat.jpg"
img = Image.open(image_path)
img
```

```{python}
#| eval: true
#| echo: true
filter = torch.tensor(
    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R
        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G
        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B
    ]).unsqueeze(0).float()
ts.show(filter, show_axis=False)
```

```{python}
#| eval: true
#| echo: true
input = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)
input /= 255.0
input -= 1.0
result = F.conv2d(input, filter, stride=1, padding=0, dilation=1, groups=1)
```

```{python}
#| eval: true
#| echo: true
ts.show(result)
```

2D-Convolution:

```{python}
#| eval: true
#| echo: true
result = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)
ts.show(result)
```

Transposed convolution:

```{python}
#| eval: true
#| echo: true
result = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)
result = F.conv_transpose2d(result, weight=torch.ones_like(filter))
ts.show(result)
```

Max-Pooling:

```{python}
#| eval: true
#| echo: true
result = F.max_pool2d(input, kernel_size=8, stride=8)
ts.show(result)
```

# References

::: {#refs}
:::

