
---
title: "7 - Segmentation"
params:
   images_path: "/assets/images/segmentation/"
---

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::


# Introduction

In image segmentation, individual pixels in the input image are assigned to a known set of classes (semantic segmentation) or objects (instance segmentation). @fig-segmentation-overview illustrates the differences between image classification, object detection, and segmentation.

::: {#fig-segmentation-overview}

![]({{< meta params.images_path >}}overview.jpg){width=600}

Source: @johnson_eecs_2019.
:::

Semantic segmentation can be viewed as a classification problem where each pixel is individually classified. Thus, semantic segmentation is similar to image classification but more complex. @fig-segmentation-cityscapes shows an example from a dataset with segmented street scenes, for training models for self-driving cars.

::: {#fig-segmentation-cityscapes}

![]({{< meta params.images_path >}}road_segmentation_example.png){width=600}

Top: Photo, bottom: annotated segmentation map. Source: @cordts_cityscapes_2016.
:::

@fig-segmentation-chest-segmentation shows a medical example where a model was trained to segment chest X-rays.

::: {#fig-segmentation-chest-segmentation}

![]({{< meta params.images_path >}}chest_segmentation.jpg){width=600}

Source: @novikov_fully_2018.
:::

Instance segmentation is comparable to object detection but more complex because entire pixel masks must be predicted, defining the spatial extent of individual objects. @fig-segmentation-instance-segmentation shows an example.

::: {#fig-segmentation-instance-segmentation}

![]({{< meta params.images_path >}}instance_segmentation_example.jpg){width=800}

Instance segmentation. Source: @he_mask_2018.
:::

We will now look at methods for semantic segmentation and instance segmentation.

# Semantic Segmentation

## Sliding-Window

One method for semantic segmentation is to classify each pixel by classifying the pixel in the center using a sliding window approach. The sliding window would provide context information, allowing more accurate classification. @fig-segmentation-sliding-window illustrates the process.

::: {#fig-segmentation-sliding-window}

![]({{< meta params.images_path >}}sliding_window.jpg){width=600}

Source: @johnson_eecs_2019.
:::

However, this approach is very inefficient as a forward pass through the CNN would have to be performed for each pixel, and features extracted from overlapping sliding windows would not be reused.

## Fully Convolutional Networks

@shelhamer_fully_2016 proposed one of the first fully convolutional networks (FCNs). An FCN consists solely of convolutional layers (specifically, it has no fully connected/linear layers) and can thus process images of any spatial dimension and produce a segmentation map of the same dimension. By replacing fully connected/linear layers with convolutional layers, the dependency on a fixed input size can be eliminated.

@fig-segmentation-fcn illustrates an FCN. The FCN has an output of dimension $H \times W \times K$ (height, width, depth), where $K$ is the number of classes. The class-specific activation maps model the probability that a pixel belongs to the corresponding class. With the argmax function, each pixel could then be assigned to the class with the highest probability.

::: {#fig-segmentation-fcn}

![]({{< meta params.images_path >}}fully_conv_slide.jpg){width=600}

Source: @johnson_eecs_2019.
:::

The problem with this approach is that it requires a lot of compute (FLOPs) because the spatial dimensions of the deeper layers still correspond to the input dimension. Therefore, many operations must be performed as the filters must be convolved over a larger area.

The first layers in a CNN learn local structures (as the receptive field is very small, they cannot learn anything else), which are successively aggregated in further layers. The number of channels is typically increased to allow the CNN to recognize different variations of patterns, increasing the model's memory requirements. Additionally, sufficient layers are needed to ensure the receptive field (see @sec-cnn-receptive-field) is large enough for accurate segmentation.

In image classification, the global label of the image is modeled. Thus, this problem does not exist in image classification, as the spatial dimension of the activation maps can be gradually reduced, keeping the compute approximately constant across the network.

@shelhamer_fully_2016 solved the problem by gradually down-sampling the activation maps using convolutions with stride >2 or pooling layers (just like in image classification architectures) but then up-sampling the activation maps from various layers using an up-sampling method (see @sec-od-upsampling). They concatenate information from various layers to obtain activation maps containing rich features with local and global context. These are then reduced to the desired number of classes with $1 \times 1$ convolutions as needed. See @fig-segmentation-fcn-upsampling for an illustration.

::: {#fig-segmentation-fcn-upsampling}

![]({{< meta params.images_path >}}fcn_architecture.png){width=600}

Source: @tai_pca-aided_2017. Architecture as applied in the FCN paper @shelhamer_fully_2016.
:::

By using skip connections, which directly connect activation maps in the middle of the architecture with deeper layers, the segmentation map results were significantly improved. @fig-segmentation-improvements-skip-connections shows examples.

::: {#fig-segmentation-improvements-skip-connections}

![]({{< meta params.images_path >}}improvements_with_skip_connections.jpg){width=600}

From left to right, showing the results of models with skip connections to increasingly earlier layers. The far right is the ground truth. Source: @shelhamer_fully_2016.
:::

## Encoder-Decoder Networks

With the encoder-decoder architecture, the input (the image) is gradually reduced spatially (encoded) until a dense representation (encoding) is obtained. This encoding is then gradually expanded spatially with a decoder until the original dimension is reached. @fig-segmentation-fcn-deconv illustrates the process. This architecture is very compute-efficient and, due to the symmetry of the encoder and decoder, produces segmentation maps that correspond to the input resolution.

::: {#fig-segmentation-fcn-deconv}

![]({{< meta params.images_path >}}fully_conv_deconv.jpg){width=600}

Source: @johnson_eecs_2019.
:::

An extreme compression (encoding) was applied, for example, by @noh_learning_2015, see @fig-segmentation-fcn-deconv-paper. This makes the model significantly more efficient as the activation maps are relatively small.

::: {#fig-segmentation-fcn-deconv-paper}

![]({{< meta params.images_path >}}fcn_deconv.jpg){width=600}

Source: @noh_learning_2015.
:::

# Upsampling {#sec-od-upsampling}

In encoder-decoder architectures, the encoding of the input must be decoded so that the input's spatial dimension is reached again. Therefore, the network needs components that can upscale activation maps spatially (upsampling). There are several ways to do this.

The variants _Bed of Nails_ and _Nearest Neighbour_ are shown in @fig-segmentation-unpooling. Here, the inputs are simply copied and duplicated along the height/width or filled with zeros.

::: {#fig-segmentation-unpooling}

![]({{< meta params.images_path >}}unpooling.jpg){width=600}

Source: @johnson_eecs_2019.
:::

Another variant of upsampling, especially suitable for symmetric architectures such as encoder/decoder architectures, is to link max-pooling layers (in the encoder) with unpooling layers (in the decoder). In particular, one can remember where the maximum value appeared in the max-pooling layers. When unpooling, the corresponding value can be set to the same position instead of always at position $(0,0)$ as in Bed of Nails. This prevents the exact positions of the activations from being lost, which is important for pixel-accurate segmentation. To achieve this, one must save where the maximum value appeared during model training (and inference) in a _switch_ variable. See @fig-segmentation-unpooling-switch and @fig-segmentation-max-unpooling for an illustration.

::: {#fig-segmentation-unpooling-switch}

![]({{< meta params.images_path >}}unpooling_switch.jpg){width=600}

Source: @noh_learning_2015.
:::

::: {#fig-segmentation-max-unpooling}

![]({{< meta params.images_path >}}max_unpooling.jpg){width=600}

Source: @johnson_eecs_2019.
:::

Another method is interpolation. One can enlarge an input, as in image processing, with interpolation. @fig-segmentation-bilinear-interpolation illustrates an example using bilinear interpolation.

::: {#fig-segmentation-bilinear-interpolation}

![]({{< meta params.images_path >}}bilinear_interpolation.jpg){width=600}

Source: @johnson_eecs_2019.
:::

These upsampling methods all have in common that they are not learned and therefore have no parameters that could be optimized with gradient descent. A learnable variant of upsampling is transposed convolutions. Transposed convolutions (also fractionally strided convolutions or deconvolutions) achieve this effect. This operation does not define an inverse of the convolution.

@fig-segmentation-transposed-conv-simple illustrates a transposed convolution with stride 2, kernel 2, and an input with a side length of 2. The individual results at each input position and the added result are shown.

::: {#fig-segmentation-transposed-conv-simple}

![]({{< meta params.images_path >}}transposed_conv_example.png){width=600}

Transposed convolution with kernel size 2 and stride 2.
:::

@fig-segmentation-transposed-conv illustrates a transposed convolution with stride 2, kernel 3, and an input with a side length of 2. It shows that there are overlaps in the output, which are added.

::: {#fig-segmentation-transposed-conv}

![]({{< meta params.images_path >}}transposed_conv.jpg){width=600}

Source: @johnson_eecs_2019.
:::

@fig-cnn-transposed-gif shows an example where a transposed convolution is visualized as a convolution. A $3x3$ kernel is convolved over a $2x2$ input extended with $2x2$ padding. More complex transposed convolutions, e.g., with stride >1, can also be represented with convolutions if the input is adjusted accordingly.

::: {.content-hidden unless-format="html"}

![_transposed convolution_ of a 3x3 _kernel_ over a 2x2 input without _padding_ with _stride_ 1x1. Source @dumoulin_guide_2016.]({{< meta params.images_path >}}no_padding_no_strides_transposed.gif){#fig-cnn-transposed-gif width=200}

:::

The name transposed convolution comes from expressing a convolution with matrix multiplication and the transposed convolution with the corresponding transposed matrix. @fig-segmentation-transposed-conv-matrix shows an example.

::: {#fig-segmentation-transposed-conv-matrix}

![]({{< meta params.images_path >}}transposed_conv_as_matrix_mult.jpg){width=600}

$x$ is the kernel, $X$ the kernel as a matrix, $a$ the input. Source: @johnson_eecs_2019.
:::

::: {.callout-note appearance="simple"}
If you use transposed convolutions with PyTorch, you should read the documentation: [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html). There are formulas to accurately calculate the desired output dimension based on the parameterization.
:::

The following code shows an example in PyTorch.

```{python}
#| eval: true
#| echo: false

from typing import List

from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns
import torch
from torch.nn import functional as F

to_upsample = torch.tensor([[1, 2], [3, 4]]).unsqueeze(0).to(torch.float)

def display_arrays(arrays: List[np.ndarray], titles: List[str]):
    """ Display Arrays """
    num_arrays = len(arrays)
    kwargs = {
        'annot': True, 'cbar': False, 'vmin': 0, 'vmax': 10,
        'xticklabels': False, 'yticklabels': False}
    fig, ax = plt.subplots(
        figsize=(3 * num_arrays, 3), ncols=num_arrays)
    for i, (array, title) in enumerate(zip(arrays, titles)):
        sns.heatmap(array, **kwargs, ax=ax[i]).set(
            title=f"{title} - Shape {array.shape}")
    plt.show()

weight = torch.tensor(
    [[1, 2, 3], [0, 1, 2], [0, 1, 2]]).unsqueeze(
        0).unsqueeze(0).to(torch.float)
weight.shape

out = F.conv_transpose2d(
    input=to_upsample, weight=weight,
    stride=2, padding=0, output_padding=0)

arrays_to_plot = [np.array(x) for x in [
    to_upsample[0, : :], weight[0, 0, : :], out[0, : :]]]
display_arrays(
    arrays=arrays_to_plot,
    titles=["Input", "Filter", "Output"])
```

## UNet

A well-known architecture is U-Net @ronneberger_u-net_2015. It has been successfully used to segment images in medicine/biology. U-Net inspired architectures are also used in numerous other applications (e.g., image generation @rombach_high-resolution_2022). @fig-segmentation-unet-example2 shows examples of such segmentation.

::: {#fig-segmentation-unet-example2}

![]({{< meta params.images_path >}}unet_example2.jpg){width=600}

Source: @ronneberger_u-net_2015.
:::

The unique aspect of U-Net is that it uses an encoder/decoder architecture while simultaneously employing shortcut/skip connections to connect various layers directly. @fig-segmentation-unet shows the U-Net architecture (U-shaped, hence the name), including the copy and crop operations that connect the layers. These connections directly copy detailed low-level information to the output without passing through the bottleneck in the encoder, where there may not be enough capacity to preserve it. The bottleneck encodes global information relevant to all positions, making the segmentation more accurate in detail.

::: {#fig-segmentation-unet}

![]({{< meta params.images_path >}}unet.jpg){width=600}

Source: @ronneberger_u-net_2015.
:::

Additionally, when training the models, the individual pixels were weighted differently. The closer a pixel is to the edge of an object, the higher its loss was weighted. This allows U-Net to learn especially sharp separations between objects, which can be important in medicine when, for example, segmenting cells that are very close to each other.

## Loss

Since semantic segmentation essentially performs classification at the pixel level, the same loss function used in image classification can be applied at the pixel level. @fig-segmentation-pixel-level-softmax shows that the softmax function is applied individually to all pixel positions to obtain probability distributions per pixel.

::: {#fig-segmentation-pixel-level-softmax}

![]({{< meta params.images_path >}}pixel_level_softmax.jpg){width=600}

Pixel-level softmax for a single pixel illustrated. Output is $H \times W \times K$. 
:::

Often, per-pixel cross-entropy is used as the loss function, where $N$ refers to the total number of pixels:

\begin{align}
CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
\end{align}

# Instance Segmentation

In instance segmentation, the goal is to detect and segment all objects in an image. The approach is simple: perform object detection and then model a segmentation mask in addition to the bounding box.

## Mask R-CNN

One of the most well-known models is an extension of Faster R-CNN: Mask R-CNN. @fig-segmentation-maskrcnn illustrates the additional output head responsible for mask prediction.

::: {#fig-segmentation-maskrcnn}

![]({{< meta params.images_path >}}mask_rcnn.jpg){width=600}

Source: @he_mask_2018.
:::

Mask R-CNN models the masks with an output size of $NxNxK$, where $NxN$ is the spatial dimension of the RoI pooling of the individual objects. $K$ is the number of classes. Masks are always generated for all classes. When training the models, the mask of the ground truth class $k$ is evaluated, and the binary pixel-wise cross-entropy loss is calculated accordingly.

\begin{align}
\text{binary CE} = - \sum_{i=1}^{N^2}  \Big( (\log \hat{y}_k^{(i)})^{y_k^{(i)}} + (\log (1-\hat{y}_k^{(i)}))^{(1 - y_k^{(i)})} \Big) 
\end{align}

Mask R-CNN also uses an improved version of RoI pooling, called RoI align, to more precisely align the masks with the object in the input (since the spatial resolution of the RoI is much smaller than the input object).

::: {#fig-segmentation-mask-output}

![]({{< meta params.images_path >}}mask_rcnn_output.jpg){width=600}

Source: @he_mask_2018.
:::

@fig-segmentation-mask-targets shows examples of training data. Note that the ground truth masks are each cropped relative to the predicted bounding box.

::: {#fig-segmentation-mask-targets}

![]({{< meta params.images_path >}}mask_rcnn_targets.jpg){width=600}

Source: @johnson_eecs_2019.
:::

Mask R-CNN works remarkably well, as results from @he_mask_2018 show, see @fig-segmentation-mask-results.

::: {#fig-segmentation-mask-results}

![]({{< meta params.images_path >}}mask_rcnn_results.jpg){width=600}

Source: @he_mask_2018.
:::

# Panoptic Segmentation

In panoptic segmentation, the goal is to fully segment an image by combining semantic segmentation and instance segmentation. The distinction is made between things (objects) and stuff (the rest, like the background, etc.). @fig-segmentation-things-and-stuff shows an example.

::: {#fig-segmentation-things-and-stuff}

![]({{< meta params.images_path >}}things_and_stuff.jpg){width=600}

Source: @johnson_eecs_2019.
:::

The output of such a model can be seen in @fig-segmentation-panoptic.

::: {#fig-segmentation-panoptic}

![]({{< meta params.images_path >}}panoptic_segmentation.jpg){width=600}

Source: @johnson_eecs_2019.
:::

# Metrics

## Pixel Accuracy (PA)

Pixel accuracy is the ratio of correctly classified pixels to the total number of pixels. For $K + 1$ classes (including the background class),

 pixel accuracy is defined as:

\begin{equation}
\text{PA} = \frac{\sum_{i=0}^Kp_{ii}}{\sum_{i=0}^K\sum_{j=0}^K p_{ij}}
\end{equation}

where $p_{ij}$ is the number of pixels of class $i$ predicted as class $j$.

## Mean Pixel Accuracy (MPA)

Mean pixel accuracy is an extension of pixel accuracy. The ratio of correct pixels to all pixels is calculated for each class and then averaged over the number of classes.

\begin{equation}
\text{MPA} = \frac{1}{K+1} \sum_{i=0}^K \frac{p_{ii}}{\sum_{j=0}^K p_{ij}}
\end{equation}

## Intersection over Union (IoU)

This metric is often used in semantic segmentation. It is the area of the intersection of the prediction and ground truth, divided by the union of the prediction and ground truth.

\begin{equation}
\text{IoU} = \frac{\lvert A \cap B \rvert}{\lvert A \cup B \rvert}
\end{equation}

## Mean Intersection over Union (M-IoU)

M-IoU is the average IoU over all classes.

## Precision / Recall / F1

Precision is the proportion of samples classified as positive that are actually positive:

$\text{Precision} = \frac{TP}{TP + FP}$

Recall is the proportion of positive samples that are correctly identified:

$\text{Recall} = \frac{TP}{TP + FN}$

F1 is the harmonic mean of precision and recall:

$\text{F1} = \frac{2 \text{Precision Recall}}{\text{Precision} + \text{Recall}}$

## Dice Coefficient

The Dice coefficient is twice the intersection of the prediction and ground truth, divided by the total number of pixels. The Dice coefficient is thus similar to the IoU.

\begin{equation}
\text{Dice} = \frac{2 \lvert A \cap B \rvert}{\lvert A \rvert + \lvert  B \rvert}
\end{equation}

# PyTorch

There are several ways to apply segmentation in PyTorch. It is advisable to use a segmentation/object detection framework.

An example is [Detectron2](https://github.com/facebookresearch/detectron2). There are pre-trained models that can be used directly or adapted to your dataset.

Segmentation can also be performed with [torchvision](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).

# References

::: {#refs}
:::
