<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>4 - Convolutional Neural Networks – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../pages/classification.html" rel="next">
<link href="../pages/neural_networks.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SCHRGR3LNM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SCHRGR3LNM', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../pages/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../pages/cnns.html">4 - Convolutional Neural Networks</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2024" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/cnns.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/segmentation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Segmentation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 - Foundation Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides_cas/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Einführung Computer Vision mit Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/slides_cas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Slides</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Demos</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../demos/cross_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cross-Entropy Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../demos/clip.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CLIP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/mini_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mini Projects</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation">Motivation</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  </ul></li>
  <li><a href="#convolutional-layers" id="toc-convolutional-layers" class="nav-link" data-scroll-target="#convolutional-layers">Convolutional Layers</a>
  <ul class="collapse">
  <li><a href="#hyper-parameters" id="toc-hyper-parameters" class="nav-link" data-scroll-target="#hyper-parameters">Hyper-Parameters</a>
  <ul class="collapse">
  <li><a href="#calculations" id="toc-calculations" class="nav-link" data-scroll-target="#calculations">Calculations</a></li>
  <li><a href="#quiz" id="toc-quiz" class="nav-link" data-scroll-target="#quiz">Quiz</a></li>
  </ul></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties">Properties</a>
  <ul class="collapse">
  <li><a href="#local-sparse-connectivity-parameter-sharing" id="toc-local-sparse-connectivity-parameter-sharing" class="nav-link" data-scroll-target="#local-sparse-connectivity-parameter-sharing">Local (Sparse) Connectivity &amp; Parameter Sharing</a></li>
  <li><a href="#translation-invariance-equivariance" id="toc-translation-invariance-equivariance" class="nav-link" data-scroll-target="#translation-invariance-equivariance">Translation Invariance / Equivariance</a></li>
  <li><a href="#sec-cnn-receptive-field" id="toc-sec-cnn-receptive-field" class="nav-link" data-scroll-target="#sec-cnn-receptive-field">Stacking &amp; Receptive Field</a></li>
  </ul></li>
  <li><a href="#variations" id="toc-variations" class="nav-link" data-scroll-target="#variations">Variations</a>
  <ul class="collapse">
  <li><a href="#dilated-convolutions" id="toc-dilated-convolutions" class="nav-link" data-scroll-target="#dilated-convolutions">Dilated Convolutions</a></li>
  <li><a href="#x1-pointwise-convolutions" id="toc-x1-pointwise-convolutions" class="nav-link" data-scroll-target="#x1-pointwise-convolutions">1x1 (pointwise) Convolutions</a></li>
  <li><a href="#depthwise-separable-convolutions" id="toc-depthwise-separable-convolutions" class="nav-link" data-scroll-target="#depthwise-separable-convolutions">Depthwise Separable Convolutions</a></li>
  </ul></li>
  <li><a href="#pooling-layers" id="toc-pooling-layers" class="nav-link" data-scroll-target="#pooling-layers">Pooling Layers</a></li>
  <li><a href="#pytorch-examples" id="toc-pytorch-examples" class="nav-link" data-scroll-target="#pytorch-examples">PyTorch Examples</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2024/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="cnns.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../pages/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../pages/cnns.html">4 - Convolutional Neural Networks</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">4 - Convolutional Neural Networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="motivation" class="level1">
<h1>Motivation</h1>
<p>CNNs work similarly to MLPs: They consist of neurons with weights and biases arranged in layers. CNNs also have an output with which a differentiable loss function can be calculated so that the weights and biases can be adjusted using backpropagation.</p>
<p>Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).</p>
<p>The input to an MLP is a vector <span class="math inline">\(\mathbf{x}^{(i)}\)</span>, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers), see <a href="#fig-cnn-mlp" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-cnn-mlp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp.jpeg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>The fully connected layers can only process 1-D vectors. Therefore, images <span class="math inline">\(\in \mathbb{R}^{H \times W \times C}\)</span> must be flattened into 1-D vectors <span class="math inline">\(\in \mathbb{R}^p\)</span>. Here, <span class="math inline">\(p= H \times W \times C\)</span>. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see <a href="#fig-cnn-spatial-structure-mlp" class="quarto-xref">Figure&nbsp;3</a> for an illustration in an MLP and <a href="#fig-cnn-mlp-images" class="quarto-xref">Figure&nbsp;2</a> for an illustration on a linear model).</p>
<div id="fig-cnn-mlp-images" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp_images.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-spatial-structure-mlp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-spatial-structure-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp-spatial-structure.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-spatial-structure-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Source: <span class="citation" data-cites="li_cs231n_2023">Li (<a href="#ref-li_cs231n_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>For larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.</p>
<p>A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see <a href="#fig-cnn-cnn-spatial" class="quarto-xref">Figure&nbsp;4</a>). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.</p>
<div id="fig-cnn-cnn-spatial" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-cnn-spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cnn_spatial.jpg" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-cnn-spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.</p>
</div>
</div>
<section id="architecture" class="level2">
<h2 class="anchored" data-anchor-id="architecture">Architecture</h2>
<p>CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.</p>
<p><a href="#fig-cnn-convnet" class="quarto-xref">Figure&nbsp;5</a> shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.</p>
<div id="fig-cnn-convnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-convnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/convnet.jpeg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-convnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.</p>
</section>
</section>
<section id="convolutional-layers" class="level1">
<h1>Convolutional Layers</h1>
<p>Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension <span class="math inline">\(7 \times 7 \times 3\)</span> (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter’s expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of <span class="math inline">\(K\)</span> filters produces activation maps with a depth of <span class="math inline">\(K\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Convolution in deep learning is typically implemented as cross-correlation.</p>
<p>Given:</p>
<ul>
<li>Kernel <span class="math inline">\(K\)</span></li>
<li>Image <span class="math inline">\(I\)</span></li>
</ul>
<p><span class="math display">\[\begin{equation}
S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)
\end{equation}\]</span></p>
</div>
</div>
<div id="fig-cnn-conv-one-number" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-one-number-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cnn_conv_one_number.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-one-number-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-conv-activation-map" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-conv-activation-map2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map2.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-conv-activation-map3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map3.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>The data is processed in mini-batches, i.e., multiple images at once, as shown in <a href="#fig-cnn-conv-activation-map4" class="quarto-xref">Figure&nbsp;10</a>.</p>
<div id="fig-cnn-conv-activation-map4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map4.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<section id="hyper-parameters" class="level2">
<h2 class="anchored" data-anchor-id="hyper-parameters">Hyper-Parameters</h2>
<p>To define a convolutional layer, various hyperparameters need to be set. Some of the most important ones are:</p>
<ul>
<li>Depth</li>
<li>Padding</li>
<li>Stride</li>
<li>Kernel Size</li>
</ul>
<p>Depth determines how many filters are to be learned and thus defines the dimensionality (<span class="math inline">\(C_{\text{out}}\)</span>) of the output volume (the number of activation maps).</p>
<p>Stride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation. If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height.</p>
<p>Padding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. <a href="#fig-cnn-padding-issue" class="quarto-xref">Figure&nbsp;11</a> illustrates the problem. <a href="#fig-cnn-padding" class="quarto-xref">Figure&nbsp;12</a> shows an example with padding.</p>
<p><a href="#fig-cnn-stride-and-padding" class="quarto-xref">Figure&nbsp;13</a> shows the interplay between stride and padding.</p>
<div id="fig-cnn-padding-issue" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-padding-issue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/padding_issue.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-padding-issue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-padding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/padding.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Left: Input (yellow) with zero-padding (1, 1) (white border), middle: Filter, right: Output.
</figcaption>
</figure>
</div>
<div id="fig-cnn-stride-and-padding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride-and-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/stride_and_padding.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride-and-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Stride with padding. The red mark indicates the filter value position on the input activations.
</figcaption>
</figure>
</div>
<p><span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span> has created some animations for better understanding of convolutions and published them here: <a href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a>.</p>
<section id="calculations" class="level3">
<h3 class="anchored" data-anchor-id="calculations">Calculations</h3>
<p>The dimensionality of the activation maps can be calculated using the following formulas:</p>
<ul>
<li><span class="math inline">\(i\)</span>: Side length of the input activations (assumption: square inputs)</li>
<li><span class="math inline">\(k\)</span>: Kernel size (assumption: square kernels)</li>
<li><span class="math inline">\(o\)</span>: Side length of the output activation maps</li>
<li><span class="math inline">\(s\)</span>: Stride (assumption: same stride along the spatial dimensions)</li>
<li><span class="math inline">\(p\)</span>: Number of paddings on each side (assumption: same number of paddings along the spatial dimensions)</li>
</ul>
<p><strong>Scenario: <em>stride</em> = 1 and without <em>padding</em></strong></p>
<p><span class="math display">\[\begin{equation}
o = (i - k) + 1
\end{equation}\]</span></p>
<div id="fig-cnn-stride-and-padding-gif3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride-and-padding-gif3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/no_padding_no_strides.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride-and-padding-gif3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p><strong>Scenario: <em>stride</em> = 1 with <em>padding</em></strong></p>
<p><span class="math display">\[\begin{equation}
o = (i - k) + 2p + 1
\end{equation}\]</span></p>
<p><strong>Scenario: Half (same) Padding -&gt; <span class="math inline">\(o = i\)</span></strong></p>
<p>Valid for any <span class="math inline">\(i\)</span> and for odd <span class="math inline">\(k = 2n + 1, n ∈ N\)</span>, <span class="math inline">\(s = 1\)</span> and <span class="math inline">\(p = \lfloor k/2\rfloor = n\)</span>.</p>
<p><span class="math display">\[\begin{align}
o &amp;= i + 2 \lfloor k/2\rfloor − (k − 1) \\
o &amp;= i + 2 n - 2 n \\
o &amp;= i
\end{align}\]</span></p>
<div id="fig-cnn-same-padding-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-same-padding-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/same_padding_no_strides.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-same-padding-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Convolving a 3x3 kernel over a 5x5 input with 1x1 padding and stride 1x1. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p><strong>Scenario: Full Padding</strong></p>
<p>The dimensionality of the output activation can also be increased.</p>
<p>Valid for any <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span>, <span class="math inline">\(s = 1\)</span> and <span class="math inline">\(p = k - 1\)</span>.</p>
<p><span class="math display">\[\begin{align}
o &amp;= i + 2 (k − 1) - (k - 1) \\
o &amp;= i + (k - 1)
\end{align}\]</span></p>
<div id="fig-cnn-stride-and-padding-gif2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride-and-padding-gif2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/full_padding_no_strides.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride-and-padding-gif2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Convolving a 3x3 kernel over a 5x5 input with 2x2 padding and stride 1x1. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p><strong>Scenario: No Padding, <em>stride</em> &gt; 1</strong></p>
<p>Valid for any <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span>, <span class="math inline">\(s &gt; 1\)</span> and <span class="math inline">\(p = 0\)</span>.</p>
<p><span class="math display">\[\begin{equation}
o = \lfloor\frac{i - k}{s}\rfloor + 1
\end{equation}\]</span></p>
<div id="fig-cnn-stride-and-padding-gif1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride-and-padding-gif1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/no_padding_strides.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride-and-padding-gif1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 2x2. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p><strong>Scenario: <em>padding</em> and <em>stride</em> &gt; 1</strong></p>
<p>Valid for any <span class="math inline">\(i, k, s, p\)</span>.</p>
<p><span class="math display">\[\begin{equation}
o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1
\end{equation}\]</span></p>
<div id="fig-cnn-padding-strides-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-padding-strides-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/padding_strides.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-padding-strides-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Convolving a 3x3 kernel over a 5x5 input with 1x1 zero-padding and stride 2x2. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>With this formula, all scenarios are covered!</p>
<p><span class="math display">\[\begin{equation}
o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1
\end{equation}\]</span></p>
</div>
</div>
</section>
<section id="quiz" class="level3">
<h3 class="anchored" data-anchor-id="quiz">Quiz</h3>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>Input: 3 x 32 x 32</p>
<p>Convolution: 10 filters with 5x5 kernel size, stride=1, and padding=2</p>
<p>What is the size of the activation map?</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>Input: 3 x 32 x 32</p>
<p>Convolution: 10 filters with 5x5 kernel size, stride=1, and padding=2</p>
<p>How many weights are there?</p>
</div>
</div>
</div>
</section>
</section>
<section id="properties" class="level2">
<h2 class="anchored" data-anchor-id="properties">Properties</h2>
<section id="local-sparse-connectivity-parameter-sharing" class="level3">
<h3 class="anchored" data-anchor-id="local-sparse-connectivity-parameter-sharing">Local (Sparse) Connectivity &amp; Parameter Sharing</h3>
<p>Fully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.</p>
<p>Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).</p>
</div>
</div>
<p>The following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.</p>
<div id="de23ff19" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer1 <span class="op">=</span> nn.Linear(<span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer1(x))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer2(x))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MLP()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(net, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MLP                                      [1, 10]                   --
├─Flatten: 1-1                           [1, 3072]                 --
├─Linear: 1-2                            [1, 64]                   196,672
├─Linear: 1-3                            [1, 32]                   2,080
├─Linear: 1-4                            [1, 10]                   330
==========================================================================================
Total params: 199,082
Trainable params: 199,082
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.20
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.00
Params size (MB): 0.80
Estimated Total Size (MB): 0.81
==========================================================================================</code></pre>
</div>
</div>
<div id="1b621003" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span> , <span class="dv">10</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> CNN()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(cnn, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
CNN                                      [1, 10]                   --
├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368
├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320
├─Flatten: 1-3                           [1, 1024]                 --
├─Linear: 1-4                            [1, 10]                   10,250
==========================================================================================
Total params: 14,938
Trainable params: 14,938
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.76
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.04
Params size (MB): 0.06
Estimated Total Size (MB): 0.11
==========================================================================================</code></pre>
</div>
</div>
<div id="fig-cnn-linear-transf-calc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-linear-transf-calc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/linear_transf.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-linear-transf-calc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>How should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?</p>
</div>
</div>
</div>
</section>
<section id="translation-invariance-equivariance" class="level3">
<h3 class="anchored" data-anchor-id="translation-invariance-equivariance">Translation Invariance / Equivariance</h3>
<p>Translation invariant is a function that produces the same value under translations <span class="math inline">\(g()\)</span> of the input <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{equation}
f(g(x))=f(x)
\end{equation}\]</span></p>
<p>Translation equivariant is a function that produces the same value under translations <span class="math inline">\(g()\)</span> of the input <span class="math inline">\(x\)</span>, provided that it is also shifted by <span class="math inline">\(g()\)</span>:</p>
<p><span class="math display">\[\begin{equation}
f(g(x))=g(f(x))
\end{equation}\]</span></p>
<p>Convolutions are translation equivariant, as illustrated well in the following example:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
 -->
</section>
<section id="sec-cnn-receptive-field" class="level3">
<h3 class="anchored" data-anchor-id="sec-cnn-receptive-field">Stacking &amp; Receptive Field</h3>
<p>Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. <a href="#fig-cnn-conv-stacking" class="quarto-xref">Figure&nbsp;20</a> illustrates the result.</p>
<div id="fig-cnn-conv-stacking" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_stacking.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality <span class="math inline">\(H \times W \times C\)</span>! (There are also variants in higher dimensions.)</p>
<p>However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).</p>
<p>The receptive field defines which inputs influence the activations of a neuron. <a href="#fig-cnn-receptive-field" class="quarto-xref">Figure&nbsp;21</a> illustrates the concept.</p>
<div id="fig-cnn-receptive-field" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/receptive_field.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>Stacking multiple convolutions increases the receptive field of a neuron relative to the original input (see <a href="#fig-cnn-receptive-field2" class="quarto-xref">Figure&nbsp;22</a>).</p>
<div id="fig-cnn-receptive-field2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/receptive_field2.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="variations" class="level2">
<h2 class="anchored" data-anchor-id="variations">Variations</h2>
<section id="dilated-convolutions" class="level3">
<h3 class="anchored" data-anchor-id="dilated-convolutions">Dilated Convolutions</h3>
<p>Dilated convolutions have an additional hyperparameter, the dilation. Dilated convolutions have holes between the rows and columns of a kernel. This increases the receptive field without having to learn more parameters. This variant is also called à trous convolution. <a href="#fig-cnn-dilation-gif" class="quarto-xref">Figure&nbsp;23</a>, <a href="#fig-cnn-dilation1" class="quarto-xref">Figure&nbsp;24</a>, and <a href="#fig-cnn-dilation2" class="quarto-xref">Figure&nbsp;25</a> show examples.</p>
<div id="fig-cnn-dilation-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-dilation-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/dilation.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-dilation-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-dilation1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-dilation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/dilation1.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-dilation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.
</figcaption>
</figure>
</div>
<div id="fig-cnn-dilation2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-dilation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/dilation2.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-dilation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.
</figcaption>
</figure>
</div>
</section>
<section id="x1-pointwise-convolutions" class="level3">
<h3 class="anchored" data-anchor-id="x1-pointwise-convolutions">1x1 (pointwise) Convolutions</h3>
<p>1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (<span class="math inline">\(C\)</span>) of activation maps with few parameters. For example, activation maps of dimensionality (<span class="math inline">\(C \times H \times W\)</span>) can be changed to a volume of (<span class="math inline">\(C2 \times H \times W\)</span>) with <span class="math inline">\(C2 * (C + 1)\)</span>. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (<span class="math inline">\(C2=3\)</span>) for image generation models. <a href="#fig-cnn-1x1-conv" class="quarto-xref">Figure&nbsp;26</a> shows an example.</p>
<div id="fig-cnn-1x1-conv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-1x1-conv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/1x1_conv.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-1x1-conv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="depthwise-separable-convolutions" class="level3">
<h3 class="anchored" data-anchor-id="depthwise-separable-convolutions">Depthwise Separable Convolutions</h3>
<p>Depthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (<span class="math inline">\(1 \times K \times K\)</span>). <a href="#fig-cnn-depthwise" class="quarto-xref">Figure&nbsp;27</a> shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See <a href="#fig-cnn-depthwise-separabel" class="quarto-xref">Figure&nbsp;28</a> for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.</p>
<div id="fig-cnn-depthwise" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-depthwise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/depthwise.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-depthwise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Source: <a href="https://paperswithcode.com/method/depthwise-convolution">https://paperswithcode.com/method/depthwise-convolution</a>
</figcaption>
</figure>
</div>
<div id="fig-cnn-depthwise-separabel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-depthwise-separabel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/depthwise_separabel.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-depthwise-separabel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Source: <span class="citation" data-cites="yu_multi-scale_2016">Yu and Koltun (<a href="#ref-yu_multi-scale_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="pooling-layers" class="level2">
<h2 class="anchored" data-anchor-id="pooling-layers">Pooling Layers</h2>
<p>Often, the spatial dimensionality of the activation maps needs to be successively reduced in a CNN. This reduces the number of computations and memory required. Also, information is condensed and aggregated layer by layer: high spatial resolution is often no longer necessary. We have already seen that convolutional layers with a stride &gt; 1 can achieve this goal. However, it is also possible to use pooling layers, which do not have (learnable) parameters.</p>
<p>A frequently used variant is the max-pooling layer. This layer operates independently on each slice along the depth dimension and returns the maximum value. The kernel size and stride must also be defined. A stride of <span class="math inline">\(2 \times 2\)</span> with a kernel of <span class="math inline">\(2 \times 2\)</span> halves the dimensionality of the activation maps along height and width.</p>
<p>For any <span class="math inline">\(i,k,s\)</span> and without padding:</p>
<p><span class="math display">\[\begin{equation}
o = \lfloor\frac{i - k}{s}\rfloor + 1
\end{equation}\]</span></p>
<div id="fig-cnn-pool" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-pool-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/pool.jpeg" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-pool-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<div>

</div>
<div id="cell-fig-cnn-pooling-illustration" class="cell quarto-layout-panel" data-execution_count="3" data-layout="[[20,10,10,10]]">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 40.0%;justify-content: flex-start;">
<div id="fig-cnn-pooling-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cnns_files/figure-html/fig-cnn-pooling-illustration-output-1.png" width="466" height="483" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: Input
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div id="fig-cnn-pooling-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cnns_files/figure-html/fig-cnn-pooling-illustration-output-2.png" width="242" height="261" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31: Max Pooling
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div id="fig-cnn-pooling-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cnns_files/figure-html/fig-cnn-pooling-illustration-output-3.png" width="242" height="261" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32: Average Pooling
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div id="fig-cnn-pooling-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cnns_files/figure-html/fig-cnn-pooling-illustration-output-4.png" width="242" height="261" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-pooling-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: Global Average Pooling
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><a href="#fig-cnn-pooling-illustration" class="quarto-xref">Figure&nbsp;33</a> shows the result of max-pooling, average-pooling and global average-pooling. In average-pooling, instead of choosing the maximum, the average activation is calculated. Otherwise, average-pooling works the same way as max-pooling. A crucial pooling variant is global average pooling. The average of the activations is calculated along the depth dimension (i.e., no kernel size or stride needs to be defined). Activation maps with (<span class="math inline">\(C \times H \times W\)</span>) are reduced to (<span class="math inline">\(C \times 1 \times 1\)</span>). This is useful, for example, to directly model logits in a classification problem with <span class="math inline">\(C\)</span> classes. This allows architectures to be created that do not use fully connected layers at all.</p>
</section>
<section id="pytorch-examples" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-examples">PyTorch Examples</h2>
<div id="5d18f47e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchshow <span class="im">as</span> ts</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5fc8cafd" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"../assets/images/cnns/cat.jpg"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="43f3ad52" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">filter</span> <span class="op">=</span> torch.tensor(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    [   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># R</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># G</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># B</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>ts.show(<span class="bu">filter</span>, show_axis<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-7-output-1.png" width="470" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="a5d9db96" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor(np.array(img)).unsqueeze(<span class="dv">0</span>).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>).<span class="bu">float</span>() <span class="co"># (N, C, H, W)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">/=</span> <span class="fl">255.0</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">-=</span> <span class="fl">1.0</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7f6d1d00" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/torchshow/visualization.py:388: UserWarning:

Original input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-9-output-2.png" width="621" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>2D-Convolution:</p>
<div id="2a6a1ed4" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-10-output-1.png" width="618" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Transposed convolution:</p>
<div id="c2eb7659" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv_transpose2d(result, weight<span class="op">=</span>torch.ones_like(<span class="bu">filter</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-11-output-1.png" width="616" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Max-Pooling:</p>
<div id="847b83b8" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.max_pool2d(<span class="bu">input</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, stride<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-12-output-1.png" width="621" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-dumoulin_guide_2016" class="csl-entry" role="listitem">
Dumoulin, Vincent, and Francesco Visin. 2016. <span>“A Guide to Convolution Arithmetic for Deep Learning.”</span> <em>ArXiv e-Prints</em>, March.
</div>
<div id="ref-johnson_eecs_2019" class="csl-entry" role="listitem">
Johnson, Justin. 2019. <span>“<span>EECS</span> 498-007 / 598-005: <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/</a>.
</div>
<div id="ref-li_cs231n_2022" class="csl-entry" role="listitem">
Li, Fei-Fei. 2022. <span>“<span>CS231n</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span> for <span>Visual</span> <span>Recognition</span>.”</span> Lecture {Notes}. <a href="https://cs231n.github.io">https://cs231n.github.io</a>.
</div>
<div id="ref-li_cs231n_2023" class="csl-entry" role="listitem">
———. 2023. <span>“<span>CS231n</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span> for <span>Visual</span> <span>Recognition</span>.”</span> Lecture {Notes}. <a href="http://cs231n.stanford.edu/schedule.html">http://cs231n.stanford.edu/schedule.html</a>.
</div>
<div id="ref-yu_multi-scale_2016" class="csl-entry" role="listitem">
Yu, Fisher, and Vladlen Koltun. 2016. <span>“Multi-<span>Scale</span> <span>Context</span> <span>Aggregation</span> by <span>Dilated</span> <span>Convolutions</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1511.07122">http://arxiv.org/abs/1511.07122</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2024\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../pages/neural_networks.html" class="pagination-link" aria-label="3 - Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">3 - Neural Networks</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../pages/classification.html" class="pagination-link" aria-label="5 - Image Classification">
        <span class="nav-page-text">5 - Image Classification</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "4 - Convolutional Neural Networks"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/cnns/"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Motivation</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>CNNs work similarly to MLPs: They consist of neurons with weights and biases arranged in layers. CNNs also have an output with which a differentiable loss function can be calculated so that the weights and biases can be adjusted using backpropagation.</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>The input to an MLP is a vector $\vect{x}^{(i)}$, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers), see @fig-cnn-mlp.</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp}</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp.jpeg)</span>{width=600}</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>Source: @li_cs231n_2022</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>The fully connected layers can only process 1-D vectors. Therefore, images $\in \mathbb{R}^{H \times W \times C}$ must be flattened into 1-D vectors $\in \mathbb{R}^p$. Here, $p= H \times W \times C$. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see @fig-cnn-spatial-structure-mlp for an illustration in an MLP and @fig-cnn-mlp-images for an illustration on a linear model).</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp-images}</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images.jpg)</span>{width=600}</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-spatial-structure-mlp}</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp-spatial-structure.png)</span>{width=600}</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>Source: @li_cs231n_2023</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>For larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see @fig-cnn-cnn-spatial). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-cnn-spatial}</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_spatial.jpg)</span>{width=200}</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="fu">## Architecture</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>@fig-cnn-convnet shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-convnet}</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}convnet.jpeg)</span>{width=600}</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: @li_cs231n_2022.</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a><span class="fu"># Convolutional Layers</span></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension $7 \times 7 \times 3$ (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter's expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of $K$ filters produces activation maps with a depth of $K$.</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>Convolution in deep learning is typically implemented as cross-correlation.</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>Given:</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel $K$</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image $I$</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-one-number}</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_conv_one_number.jpg)</span>{width=600}</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map}</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map.jpg)</span>{width=600}</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map2}</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map2.jpg)</span>{width=600}</span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map3}</span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map3.jpg)</span>{width=600}</span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>The data is processed in mini-batches, i.e., multiple images at once, as shown in @fig-cnn-conv-activation-map4.</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map4}</span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map4.jpg)</span>{width=600}</span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyper-Parameters</span></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a>To define a convolutional layer, various hyperparameters need to be set. Some of the most important ones are:</span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Depth</span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Padding</span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stride</span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel Size</span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a>Depth determines how many filters are to be learned and thus defines the dimensionality ($C_{\text{out}}$) of the output volume (the number of activation maps).</span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>Stride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation. If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height.</span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a>Padding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. @fig-cnn-padding-issue illustrates the problem. @fig-cnn-padding shows an example with padding.</span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a>@fig-cnn-stride-and-padding shows the interplay between stride and padding.</span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-padding-issue}</span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}padding_issue.jpg)</span>{width=600}</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-padding}</span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}padding.png)</span>{width=600}</span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>Left: Input (yellow) with zero-padding (1, 1) (white border), middle: Filter, right: Output.</span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-stride-and-padding}</span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stride_and_padding.png)</span>{width=600}</span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a>Stride with padding. The red mark indicates the filter value position on the input activations.</span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a>@dumoulin_guide_2016 has created some animations for better understanding of convolutions and published them here: <span class="co">[</span><span class="ot">https://github.com/vdumoulin/conv_arithmetic</span><span class="co">](https://github.com/vdumoulin/conv_arithmetic)</span>.</span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculations</span></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a>The dimensionality of the activation maps can be calculated using the following formulas:</span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$i$: Side length of the input activations (assumption: square inputs)</span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k$: Kernel size (assumption: square kernels)</span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$o$: Side length of the output activation maps</span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s$: Stride (assumption: same stride along the spatial dimensions)</span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p$: Number of paddings on each side (assumption: same number of paddings along the spatial dimensions)</span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a>**Scenario: _stride_ = 1 and without _padding_**</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>o = (i - k) + 1</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}no_padding_no_strides.gif)</span>{#fig-cnn-stride-and-padding-gif3 width=200}</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>**Scenario: _stride_ = 1 with _padding_**</span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>o = (i - k) + 2p + 1</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a>**Scenario: Half (same) Padding -&gt; $o = i$**</span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a>Valid for any $i$ and for odd $k = 2n + 1, n ∈ N$, $s = 1$ and $p = \floor{k/2} = n$.</span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a>o &amp;= i + 2 \floor{k/2} − (k − 1) <span class="sc">\\</span></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a>o &amp;= i + 2 n - 2 n <span class="sc">\\</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>o &amp;= i</span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input with 1x1 padding and stride 1x1. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}same_padding_no_strides.gif)</span>{#fig-cnn-same-padding-gif width=200}</span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>**Scenario: Full Padding**</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a>The dimensionality of the output activation can also be increased.</span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>Valid for any $i$ and $k$, $s = 1$ and $p = k - 1$.</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a>o &amp;= i + 2 (k − 1) - (k - 1) <span class="sc">\\</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a>o &amp;= i + (k - 1)</span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input with 2x2 padding and stride 1x1. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}full_padding_no_strides.gif)</span>{#fig-cnn-stride-and-padding-gif2 width=200}</span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a>**Scenario: No Padding, _stride_ &gt; 1**</span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a>Valid for any $i$ and $k$, $s &gt; 1$ and $p = 0$.</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>o = \floor{\frac{i - k}{s}} + 1</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 2x2. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}no_padding_strides.gif)</span>{#fig-cnn-stride-and-padding-gif1 width=200}</span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>**Scenario: _padding_ and _stride_ &gt; 1**</span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a>Valid for any $i, k, s, p$.</span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a>o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1</span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input with 1x1 zero-padding and stride 2x2. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}padding_strides.gif)</span>{#fig-cnn-padding-strides-gif width=200}</span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>With this formula, all scenarios are covered!</span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="fu">### Quiz</span></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>Input: 3 x 32 x 32</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a>Convolution: 10 filters with 5x5 kernel size, stride=1, and padding=2</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>What is the size of the activation map?</span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>Input: 3 x 32 x 32</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a>Convolution: 10 filters with 5x5 kernel size, stride=1, and padding=2</span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a>How many weights are there?</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties</span></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="fu">### Local (Sparse) Connectivity &amp; Parameter Sharing</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a>Fully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.</span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a>Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.</span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a>Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).</span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a>The following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer1 <span class="op">=</span> nn.Linear(<span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer1(x))</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer2(x))</span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MLP()</span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(net, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span> , <span class="dv">10</span>)</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> CNN()</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(cnn, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-linear-transf-calc}</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}linear_transf.png)</span>{width=800}</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a>Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a>How should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?</span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a><span class="fu">### Translation Invariance / Equivariance</span></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a>Translation invariant is a function that produces the same value under translations $g()$ of the input $x$:</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a>f(g(x))=f(x)</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a>Translation equivariant is a function that produces the same value under translations $g()$ of the input $x$, provided that it is also shifted by $g()$:</span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a>f(g(x))=g(f(x))</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a>Convolutions are translation equivariant, as illustrated well in the following example:</span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/qoWAFBYOtoU start="50" &gt;}}</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;</span></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a><span class="co"> --&gt;</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stacking &amp; Receptive Field  {#sec-cnn-receptive-field}</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. @fig-cnn-conv-stacking illustrates the result.</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-stacking}</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_stacking.jpg)</span>{width=600}</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a>A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality $H \times W \times C$! (There are also variants in higher dimensions.)</span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a>However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a>The receptive field defines which inputs influence the activations of a neuron. @fig-cnn-receptive-field illustrates the concept.</span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field}</span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}receptive_field.jpg)</span>{width=600}</span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a>Stacking multiple convolutions increases the receptive field of a neuron relative to the original input (see @fig-cnn-receptive-field2).</span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field2}</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}receptive_field2.jpg)</span>{width=600}</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a><span class="fu">## Variations</span></span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dilated Convolutions</span></span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a>Dilated convolutions have an additional hyperparameter, the dilation. Dilated convolutions have holes between the rows and columns of a kernel. This increases the receptive field without having to learn more parameters. This variant is also called à trous convolution. @fig-cnn-dilation-gif, @fig-cnn-dilation1, and @fig-cnn-dilation2 show examples.</span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}dilation.gif)</span>{#fig-cnn-dilation-gif width=200}</span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-dilation1}</span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}dilation1.png)</span>{width=600}</span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a>Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.</span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-dilation2}</span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}dilation2.png)</span>{width=600}</span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a>Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.</span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1x1 (pointwise) Convolutions</span></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a>1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number ($C$) of activation maps with few parameters. For example, activation maps of dimensionality ($C \times H \times W$) can be changed to a volume of ($C2 \times H \times W$) with $C2 * (C + 1)$. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels ($C2=3$) for image generation models. @fig-cnn-1x1-conv shows an example.</span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-1x1-conv}</span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}1x1_conv.jpg)</span>{width=600}</span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a><span class="fu">### Depthwise Separable Convolutions</span></span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a>Depthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality ($1 \times K \times K$). @fig-cnn-depthwise shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See @fig-cnn-depthwise-separabel for a comparison of 'normal' convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.</span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-depthwise}</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}depthwise.png)</span>{width=600}</span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">https://paperswithcode.com/method/depthwise-convolution</span><span class="co">](https://paperswithcode.com/method/depthwise-convolution)</span></span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-depthwise-separabel}</span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}depthwise_separabel.png)</span>{width=600}</span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a>Source: @yu_multi-scale_2016</span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pooling Layers</span></span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a>Often, the spatial dimensionality of the activation maps needs to be successively reduced in a CNN. This reduces the number of computations and memory required. Also, information is condensed and aggregated layer by layer: high spatial resolution is often no longer necessary. We have already seen that convolutional layers with a stride &gt; 1 can achieve this goal. However, it is also possible to use pooling layers, which do not have (learnable) parameters.</span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a>A frequently used variant is the max-pooling layer. This layer operates independently on each slice along the depth dimension and returns the maximum value. The kernel size and stride must also be defined. A stride of $2 \times 2$ with a kernel of $2 \times 2$ halves the dimensionality of the activation maps along height and width.</span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a>For any $i,k,s$ and without padding:</span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a>o = \floor{\frac{i - k}{s}} + 1</span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-pool}</span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}pool.jpeg)</span>{width=300}</span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a>Source: @li_cs231n_2022</span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout: [[20, 10, 10, 10]]</span></span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cnn-pooling-illustration</span></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: </span></span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Input"</span></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Max Pooling"</span></span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Average Pooling"</span></span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Global Average Pooling"</span></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2D input tensor</span></span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>],</span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">12</span>],</span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">13</span>, <span class="dv">14</span>, <span class="dv">15</span>, <span class="dv">16</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> input_tensor.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)  <span class="co"># Adding batch and channel dimensions</span></span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a><span class="co"># Define pooling layers</span></span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a>max_pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a>avg_pool <span class="op">=</span> nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>global_avg_pool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply pooling operations</span></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a>max_pooled <span class="op">=</span> max_pool(input_tensor)</span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a>avg_pooled <span class="op">=</span> avg_pool(input_tensor)</span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a>global_avg_pooled <span class="op">=</span> global_avg_pool(input_tensor).view(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to plot side-by-side heatmaps with manual subplot positioning</span></span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_heatmap(x, title, figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)):</span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.atleast_2d(x.squeeze().numpy())</span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>figsize)</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(x, ax<span class="op">=</span>ax, cbar<span class="op">=</span><span class="va">False</span>, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">".1f"</span>, cmap<span class="op">=</span><span class="st">"YlGnBu"</span>)</span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([])</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([])</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>plot_heatmap(input_tensor, <span class="st">"Input"</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>plot_heatmap(max_pooled, <span class="st">"Max Pooling"</span>)</span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a>plot_heatmap(avg_pooled, <span class="st">"Average Pooling"</span>)</span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a>plot_heatmap(global_avg_pooled, <span class="st">"Global Average Pooling"</span>)</span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>@fig-cnn-pooling-illustration shows the result of max-pooling, average-pooling and global average-pooling. In average-pooling, instead of choosing the maximum, the average activation is calculated. Otherwise, average-pooling works the same way as max-pooling. A crucial pooling variant is global average pooling. The average of the activations is calculated along the depth dimension (i.e., no kernel size or stride needs to be defined). Activation maps with ($C \times H \times W$) are reduced to ($C \times 1 \times 1$). This is useful, for example, to directly model logits in a classification problem with $C$ classes. This allows architectures to be created that do not use fully connected layers at all.</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a><span class="fu">## PyTorch Examples</span></span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchshow <span class="im">as</span> ts</span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a><span class="co">#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')</span></span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"../assets/images/cnns/cat.jpg"</span></span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a>img</span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a><span class="bu">filter</span> <span class="op">=</span> torch.tensor(</span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a>    [   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># R</span></span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># G</span></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># B</span></span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a>    ]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>ts.show(<span class="bu">filter</span>, show_axis<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor(np.array(img)).unsqueeze(<span class="dv">0</span>).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>).<span class="bu">float</span>() <span class="co"># (N, C, H, W)</span></span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">/=</span> <span class="fl">255.0</span></span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">-=</span> <span class="fl">1.0</span></span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a>2D-Convolution:</span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a>Transposed convolution:</span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv_transpose2d(result, weight<span class="op">=</span>torch.ones_like(<span class="bu">filter</span>))</span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a>Max-Pooling:</span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.max_pool2d(<span class="bu">input</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, stride<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2024/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>