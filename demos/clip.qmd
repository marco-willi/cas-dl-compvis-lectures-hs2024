---
title: "CLIP"
code-links: binder
jupyter: python3
---

Import libraries.

```{python}
#| eval: false
#| echo: true
from PIL import Image
import requests

from transformers import CLIPProcessor, CLIPModel
```

Specify cache dir to which the models are downloaded.

```{python}
#| eval: false
#| echo: true
cache_dir="/home/jovyan/work/data/hf_cache"
```

```{python}
#| eval: false
#| echo: true
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32", cache_dir=cache_dir)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32", cache_dir=cache_dir)
```

Download an image.

```{python}
#| eval: false
#| echo: true
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
```

```{python}
#| eval: false
#| echo: true
image
```

Create two prompts and process them along with the image.

```{python}
#| eval: false
#| echo: true
inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)
```

Now we create embeddings for the prompts and the image.

```{python}
#| eval: false
#| echo: true
outputs = model(**inputs)
```

We evaluate the similarities between the text and the image embeddings.

```{python}
#| eval: false
#| echo: true
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

```{python}
#| eval: false
#| echo: true
probs
```

