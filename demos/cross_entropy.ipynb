{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross-Entropy Loss\n",
        "\n",
        "Marco Willi (Institute for Data Science I4DS, FHNW)\n",
        "\n",
        "# Softmax"
      ],
      "id": "a637b87b-9ceb-4eeb-adaa-0aba425add0a"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(123)\n",
        "logits = np.random.normal(size=(10, )) * 2\n",
        "softmax_output = np.exp(logits) / np.exp(logits).sum()\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2, figsize=(12, 4))\n",
        "_ = sns.barplot(x=[i for i in range(0, 10)], y=logits, ax=ax[0]).set(\n",
        "    title=\"Logits\")\n",
        "_ = sns.barplot(x=[i for i in range(0, 10)], y=softmax_output, ax=ax[1]).set(\n",
        "    title=\"Softmax\", ylim=(0, 1))"
      ],
      "id": "cfa79777"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Likelihood"
      ],
      "id": "e63543d4-8e44-441d-95a5-5695612e2111"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = np.array(\n",
        "    [0, 1, 0, 1, 1]\n",
        ")\n",
        "\n",
        "y_pred = np.array(\n",
        "    [0.1, 0.8, 0.2, 0.7, 0.9]\n",
        ")\n",
        "\n",
        "# Perfect Prediction\n",
        "#y_pred = np.array([0, 1, 0, 1, 1])\n",
        "\n",
        "# Larger Dataset\n",
        "#y_true = np.repeat(y_true, 10, axis=0)\n",
        "#y_pred = np.repeat(y_pred, 10, axis=0)\n",
        "\n",
        "\n",
        "def calculate_likelihood(y_true: float, y_pred: float) -> float:\n",
        "    return (y_pred ** y_true) * ((1 - y_pred) ** (1 - y_true))\n",
        " \n",
        "\n",
        "sample_likelihoods = [calculate_likelihood(float(yt), float(yp)) for yt, yp in zip(y_true, y_pred)]\n",
        "likelihood = np.prod(sample_likelihoods)\n",
        "print(f\"Total Likelihood: {likelihood:.6f}\")"
      ],
      "id": "d9ece3ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Entropy"
      ],
      "id": "67a4cd3d-c101-4e8b-b333-1bb852163e34"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(123)\n",
        "k = 5\n",
        "logits = np.random.normal(size=(5, )) * 2\n",
        "y_hat = np.exp(logits) / np.exp(logits).sum()\n",
        "y_true = np.zeros_like(logits)\n",
        "y_true[1] = 1\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2, figsize=(12, 4))\n",
        "_ = sns.barplot(x=[i for i in range(0, k)], y=y_true, ax=ax[0]).set(\n",
        "    title=\"True Distribution\")\n",
        "_ = sns.barplot(x=[i for i in range(0, k)], y=y_hat, ax=ax[1]).set(\n",
        "    title=\"Predicted Distribution\", ylim=(0, 1))\n",
        "\n",
        "cross_entropy = (- y_true * np.log(y_hat)).sum()\n",
        "print(f\"Cross Entropy: {cross_entropy}\")"
      ],
      "id": "e87eb0ae"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.11.9/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  }
}