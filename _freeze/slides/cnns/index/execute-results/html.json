{
  "hash": "da9f57197ae5b058e6460d50f8b71f89",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Convolutional Neural Networks\"\nparams:\n   images_path: \"/images/cnns/\"\n---\n\n\n\n## Overview\n\n- Introduction & Motivation\n- Convolutional Layers\n- Properties\n- Variants and Layers\n- Visualizations and Architectures\n\n\n# Introduction & Motivation\n\n## Multilayer Perceptron\n\n![Source: @li_cs231n_2022]({{< meta params.images_path >}}mlp.jpeg){width=100% height=70%}\n\n\n## MLPs on Images\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}mlp_images.jpg){width=100% height=70%}\n\n\n## MLPs on Images\n\n![Source: @li_cs231n_2023]({{< meta params.images_path >}}mlp-spatial-structure.png){width=100% height=70%}\n\n\n## CNNs\n\n![The activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: @li_cs231n_2022]({{< meta params.images_path >}}convnet.jpeg){width=100% height=70%}\n\n\n\n# Convolutional Layers\n\n## Convolution?\n\nConvolution in Deep Learning is typically implemented as cross-correlation.\n\n\\[\nS(i, j) = (K * I)(i, j) = \\sum_m \\sum_n I(i + m, j + n) K(m, n)\n\\]\n\n\n## Convolutional Layers\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}cnn_conv_one_number.jpg){width=100% height=70%}\n\n\n## Convolutional Layers\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map.jpg){width=100% height=70%}\n\n\n\n## Convolutional Layers\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map2.jpg){width=100% height=70%}\n\n\n\n## Convolutional Layers\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map3.jpg){width=100% height=70%}\n\n\n\n## Convolutional Layers\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map4.jpg){width=100% height=70%}\n\n\n\n## Hyper-Parameters\n\nConvolutional Layers are parameterized:\n\n- Depth: How many activation maps?\n- Padding: How much padding is added to the input?\n- Stride: What is the step size of the convolution?\n- Kernel-Size: What is the kernel size?\n\n\n\n## Padding: Why?\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}padding_issue.jpg){width=100% height=70%}\n\n\n## Padding\n\n![Left: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output.]({{< meta params.images_path >}}padding.png){width=100% height=70%}\n\n\n## Padding\n\n![Left: Input (Yellow) with Zero-Padding (1, 1) (White border), Middle: Filter, Right: Output.]({{< meta params.images_path >}}padding.png){width=100% height=70%}\n\n\n## Padding and Stride\n\n![Stride with Padding. Red indicates the position of the corresponding filter value on the input activations.]({{< meta params.images_path >}}stride_and_padding.png){width=100% height=70%}\n\n\n## Padding and Stride: Animations\n\n@dumoulin_guide_2016 has created some animations to better understand convolutions, available here: [Link]({{< meta params.images_path >}}https://github.com/vdumoulin/conv_arithmetic).\n\n\n\n## Calculations\n\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n- $i$: Side length of the input activations (assumption: square inputs)\n- $k$: Kernel size (assumption: square kernel)\n- $o$: Side length of the output activation maps\n- $s$: Stride (assumption: same stride along spatial dimensions)\n- $p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)\n\n\n## Calculations\n\nThis formula covers all scenarios!\n\n**Size of Activation Map**\n\n\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n\\end{equation}\n\n\n\n## Quiz\n\n**Scenario:**\n\n- Input: 3 x 32 x 32\n- Convolution: 10 filters with 5x5 kernel size, stride=1, pad=2\n\nWhat is the size of the activation map?\n\nHow many weights are there?\n\n**Size of Activation Map**\n\n\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n\\end{equation}\n\n\n# Properties\n\n## Sparse Connectivity and Parameter Sharing\n\n**Local (Sparse) Connectivity**: Neurons are only locally connected.\n\n**Parameter Sharing**: Weights of a neuron are applied locally but are the same across the entire input.\n\n\n\n\n## Convolution: Is Parameter Sharing Always Useful?\n\n**Question**: Is parameter sharing always useful?\n\n\n\n\n\n## MLP Parameters\n\n::: {#c6e6214c .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n==========================================================================================\n```\n:::\n:::\n\n\n## CNN Parameters\n\n::: {#6438f7c0 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n==========================================================================================\n```\n:::\n:::\n\n\n## Quiz: Linear Transformation vs Convolution\n\n![Input in 2-D (top left), the flat version (bottom left), expected output (right), and unknown transformation (center).]({{< meta params.images_path >}}linear_transf.png){width=100% height=70%}\n\n\n## Translation Invariance / Equivariance\n\nGiven a translation $g()$, which spatially shifts inputs:\n\n- Translation invariance: $f(g(x))=f(x)$\n- Translation equivariance: $f(g(x))=g(f(x))$\n\nConvolutions are translation equivariant: [Example Video]({{< meta params.images_path >}}https://www.youtube.com/embed/qoWAFBYOtoU?start=50)\n\n## Stacking Convolutions\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_stacking.jpg){width=100% height=70%}\n\n\n## Receptive Field\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}receptive_field.jpg){width=100% height=70%}\n\n\n\n## Receptive Field\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}receptive_field2.jpg){width=100% height=70%}\n\n\n\n\n# Variants and Layers\n\n## Dilated Convolutions\n\n![Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 1.]({{< meta params.images_path >}}dilation1.png){width=100% height=70%}\n\n\n\n## Dilated Convolutions\n\n![Convolving a 3x3 kernel over a 7x7 input without padding with stride 1x1 and dilation 2.]({{< meta params.images_path >}}dilation2.png){width=100% height=70%}\n\n\n\n## 1x1 Convolutions\n\n![Source: @johnson_eecs_2019]({{< meta params.images_path >}}1x1_conv.jpg){width=100% height=70%}\n\n\n\n## Depthwise Separable Convolutions\n\n![Source: [https://paperswithcode.com/method/depthwise-convolution]({{< meta params.images_path >}}https://paperswithcode.com/method/depthwise-convolution)]({{< meta params.images_path >}}depthwise.png){width=100% height=70%}\n\n\n\n## Depthwise Separable Convolutions\n\n![Source: @yu_multi-scale_2016]({{< meta params.images_path >}}depthwise_separabel.png){width=100% height=70%}\n\n\n\n## Pooling Layers\n\n![Source: @li_cs231n_2022]({{< meta params.images_path >}}pool.jpeg){width=100% height=70%}\n\n\n## Max Pooling\n\n![Max pooling, input (left) and output (right).]({{< meta params.images_path >}}max_pooling.png){width=100% height=70%}\n\n\n## Average Pooling\n\n![Average pooling, input (left) and output (right).]({{< meta params.images_path >}}average_pooling.png){width=100% height=70%}\n\n\n## Other Pooling Layers\n\n**Global Average Pooling** is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.\n\n\n## Global Average Pooling\n\n![Global Average pooling, input (left) and output (right).]({{< meta params.images_path >}}global_average_pooling.png){width=100% height=70%}\n\n\n# Visualizations and Architectures\n\n## Learned Filters\n\n![Source: @krizhevsky_imagenet_2012]({{< meta params.images_path >}}learned_filters.png){width=100% height=70%}\n\n\n\n# References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}