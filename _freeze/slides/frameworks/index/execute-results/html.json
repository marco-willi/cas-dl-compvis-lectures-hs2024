{
  "hash": "889459d434fb7e8610c94c302661e88f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deep Learning Frameworks\"\nparams:\n   images_path: \"/assets/images/frameworks/\"\n---\n\n\n# Overview\n\n- Software\n- PyTorch\n- Hardware\n- References\n\n\n# Software\n\n## Deep Learning Frameworks\n\nComponents / important features of such frameworks are:\n\n- Fast development and testing of neural networks\n- Automatic differentiation of operations\n- Efficient execution on diverse hardware\n\n## Frameworks\n\n![Frameworks (from @li_cs231n_2022).]({{< meta params.images_path >}}frameworks.png){width=100% height=70%}\n\n## Computational Graph & Autograd\n\nThe core of neural networks is the *Computational Graph*. Dependent operations are automatically integrated into a *directed acyclic graph (DAG)*. Gradients are tracked as needed, allowing variables to be efficiently updated/trained.\n\n## Computational Graph\n\n\\begin{equation*}\n    f(\\m{A}, \\m{B}, \\m{C}) =  \\sum_{ij} \\big((\\m{A} \\odot \\m{B}) + \\m{C}\\big)_{ij}\n\\end{equation*}\n\n![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\\linewidth}\n\n## Computational Graph & Autograd\n\n\\begin{align*}\n    f(\\m{A}, \\m{B}, \\m{C}) &=  \\sum_{ij} \\big((\\m{A} \\odot \\m{B}) + \\m{C}\\big)_{ij} \\\\\n    \\m{D} &= \\m{A} \\odot \\m{B} \\\\\n    \\m{E} &= D + \\m{C} \\\\\n    F &= \\sum_{ij} \\m{E}_{ij} \n\\end{align*}\n\n**Partial Derivative with Autograd and Chain Rule**\n\n\\begin{equation*}\n    \\frac{\\partial F}{\\partial A_{ij}} = \\frac{\\partial F}{\\partial \\m{E}} \\frac{\\partial \\m{E}}{\\partial \\m{D}} \\frac{\\partial \\m{D}}{\\partial \\m{A}_{ij}}\n\\end{equation*}\n\n![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\\linewidth}\n\n\n## Numpy Example\n\n::: {#869ae8bc .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0\nde_dd = 1.0\nde_dc = c\ndd_da = b\n\ndf_da = df_de * de_dd * dd_da\n\nprint(df_da)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\n```\n:::\n:::\n\n\n![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\\linewidth}\n\n\n## PyTorch Example\n\n::: {#2b2e6158 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n```\n:::\n:::\n\n\n![Computational Graph]({{< meta params.images_path >}}comp-graph2.jpg){width=0.5\\linewidth}\n\n\n# PyTorch\n\n## PyTorch - Why?\n\nIn this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources.\n\n## Fundamental Concepts\n\n- Tensor: N-dimensional array, like [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n- Autograd: Functionality to create *computational graphs* and compute gradients.\n- Module: Class to define components of neural networks\n\n## Tensors\n\nThe central data structure in PyTorch is [torch.Tensor](https://pytorch.org/docs/stable/tensors.html).\n\nIt is very similar to *numpy.array* but can be easily loaded onto GPUs. Tensors can be created in various ways, for example from lists:\n\n::: {#40279d8f .cell execution_count=3}\n``` {.python .cell-code}\nimport torch\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n```\n:::\n\n\n## Autograd\n\nWith [torch.autograd](https://pytorch.org/docs/stable/autograd.html), gradients can be automatically computed for a *computational graph*.\n\n::: {#e46e84b7 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\nloss.backward()\n```\n:::\n\n\n## torch.nn\n\nWith [torch.nn](https://pytorch.org/docs/stable/nn.html), components of neural networks can be defined. These components provide methods and have a state to store data such as weights and gradients.\n\n## torch.nn - Example\n\n::: {#ab4d465e .cell execution_count=5}\n``` {.python .cell-code}\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n```\n:::\n\n\n## torch.optim\n\nWith [torch.optim](https://pytorch.org/docs/stable/optim.html), model parameters can be optimized using various algorithms.\n\n::: {#877b6e48 .cell execution_count=6}\n``` {.python .cell-code}\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nloss_fn = torch.nn.CrossEntropyLoss()\nfor i in range(0, 3):\n    input, target = torch.rand(1, 28, 28), torch.randint(low=0, high=10, size=(1, ))\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n```\n:::\n\n\n## Training Loops\n\nA training loop iterates over *mini-batches* and optimizes the model parameters.\n\n::: {#1a49304d .cell execution_count=7}\n``` {.python .cell-code}\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n```\n:::\n\n\n## Training Loops 2\n\n::: {#812f3001 .cell execution_count=8}\n``` {.python .cell-code}\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n```\n:::\n\n\n## PyTorch Ecosystem\n\nThere are many packages that extend PyTorch with functionalities. One example is [PyTorch-Lightning](https://www.pytorchlightning.ai/) which simplifies managing training loops.\n\n## Other Frameworks and Tools\n\n- [TensorFlow with Keras](https://www.tensorflow.org)\n- [Scikit-Learn](https://scikit-learn.org/stable/)\n- [ONNX](https://onnx.ai/)\n- [Monitoring: TensorBoard](https://www.tensorflow.org/tensorboard)\n- [Monitoring: Weights & Biases](https://wandb.ai/site)\n\n# Hardware\n\n## Tensor Operations\n\n![Matrix Multiplication (from @li_cs231n_2022).]({{< meta params.images_path >}}matrix_mult.png){width=100% height=70%}\n\n## CPU vs GPU\n\n![CPU vs GPU example (from @li_cs231n_2022).]({{< meta params.images_path >}}cpu_vs_gpu.png){width=100% height=70%}\n\n## Speed Comparison\n\n![Speed comparison (from @li_cs231n_2022), data from [Link](https://github.com/jcjohnson/cnn-benchmarks).]({{< meta params.images_path >}}speed_gpu_cpu.png){width=100% height=70%}\n\n## Data Loading\n\nA critical bottleneck in practice is transferring data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is called *GPU starvation*. Solutions include:\n\n- Reading data into RAM\n- Using fast disks like SSDs\n- Utilizing multiple CPU threads to read data in parallel and keep it in RAM (pre-fetching)\n\n## GPU Starvation\n\n![The Y-axis shows the GPU utilization in percent, while the X-axis represents time. [Source](https://stackoverflow.com/questions/44598246/tensorflow-data-starved-gpu).]({{< meta params.images_path >}}gpu_starvation.png){width=100% height=70%}\n\n## GPU Parallelism\n\n![Data and Model Parallelism (from @li_cs231n_2022).]({{< meta params.images_path >}}parallelism.jpg){width=100% height=70%}\n\n\n# References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}