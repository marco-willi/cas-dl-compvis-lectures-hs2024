{
  "hash": "ab37f684dc44fc96bd1bda9785fdce9f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Practical Considerations\"\nparams:\n   images_path: \"/assets/images/practical/\"\n---\n\n\n# Overview\n\n- A Recipe\n- Data\n- Baselines\n- Overfit\n- Regularization\n- Tuning\n- Squeeze\n\n\n# A Recipe\n\n## Leaky Abstraction\n\n::: {#7a0aea96 .cell execution_count=1}\n``` {.python .cell-code}\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\n```\n:::\n\n\n## Silent Failure\n\nTraining neural networks *fails silently*!\n\n# 1 - Data\n\n## 1) Get to Know the Data\n\nThoroughly inspect the data!\n\n## Camera Traps: Errors\n\n![Examples of images from camera traps.]({{< meta params.images_path >}}camera_traps_corrupt.jpg){width=100% height=70%}\n\n## Camera Traps: Difficulties\n\n![Examples of images from camera traps. Source: @Beery2018]({{< meta params.images_path >}}cam_trap_difficulties.png){width=100% height=70%}\n\n## Rare Classes\n\n![An image of a serval. Below are the model confidences.]({{< meta params.images_path >}}wrong_predictions_serval.png){height=600px}\n\n## Multiple Classes\n\n![Examples of an image from a camera trap with different species.]({{< meta params.images_path >}}multi_species.JPG){width=80%}\n\n\n# 2 - Baselines\n\n## 2) Baselines\n\nEvaluation pipeline, metrics, experiment tracking, and baseline model.\n\n## ML Process\n\n![The components of a typical machine learning process. Source: @raschka_python_2020]({{< meta params.images_path >}}ml_process_raschka.png){width=100% height=70%}\n\n## Experiment Tracking\n\n![Weights and Biases experiment tracking.]({{< meta params.images_path >}}wb_example.png){height=70%}\n\n## 2) Baselines\n\nEnsure reproducibility.\n\n```python\nimport torch\ntorch.manual_seed(0)\n```\n\n## 2) Baselines\n\nAvoid unnecessary techniques and complexities. Reduce error susceptibility.\n\n## 2) Baselines\n\nIf possible, use a *human baseline*. How good can the model be?\n\n## Difficult Cases\n\n![An image from a camera trap that is difficult to classify. Here, annotators had a 96.6% agreement with experts.]({{< meta params.images_path >}}camera_trap_difficult.jpeg){width=50%}\n\n## 2) Baselines\n\nTrain an input-independent baseline. Is the model learning anything at all?\n\n## 2) Baselines\n\nOverfit the model on a batch of data. Does the optimization work?\n\n## 2) Baselines\n\nVisualize what goes into the model. Is my preprocessing working?\n\n```python\ny_hat = model(x)\n```\n\n## Fixed Sample: Segmentation Example\n\n![Example of a segmentation problem: input on the left and output on the right.]({{< meta params.images_path >}}wb_comparison.jpg){width=100% height=70%}\n\n\n# 3 - Overfit\n\n## 3) Overfit\n\nAt this point, you should have a good understanding of the dataset, high confidence in the evaluation pipeline, and initial baselines from simple models. Now, look for a model that performs well on the training set.\n\n## 3) Overfit\n\nLook for a good model architecture. Follow the principle \"Don't be a hero\". Prefer already implemented/established architectures.\n\n\n# 4 - Regularization\n\n## 4) Regularization\n\nAt this point, you should have achieved good performance on the training set. Now, focus on the validation set.\n\n## 4) Regularization\n\nThe simplest measure to achieve better performance (and also reduce overfitting) is to collect more training data. However, this is often expensive!\n\n## Learning Curve\n\nIs it worth collecting more data?\n\n![Example of a learning curve. X-axis: Performance, Y-axis: Number of training samples. Left panel with Gaussian Naive Bayes and right panel with Support Vector Classifier.]({{< meta params.images_path >}}learning_curve.jpg){width=70%}\n\n## 4) Regularization\n\nAnother possibility is data augmentation. New data points are generated from existing ones by making random changes to the data. Typically, data points are augmented on-the-fly.\n\n## Data Augmentation: Augly\n\n![AugLy]({{< meta params.images_path >}}augly.jpg){width=100% height=70%}\n\n## Data Augmentation: Albumentations\n\n![Albumentations]({{< meta params.images_path >}}albumentations.jpg){width=600px}\n\n## Data Augmentation: Kornia\n\n![Kornia]({{< meta params.images_path >}}kornia.png){width=100% height=70%}\n\n## Data Augmentation: Example\n\n![Data augmentation example.]({{< meta params.images_path >}}augmentation_rhino.png){width=100% height=70%}\n\n## Data Augmentation: Synthetic Data\n\n![From @Beery2020. Synthetic and semi-synthetic data.]({{< meta params.images_path >}}synthetic_camera_trap_images.jpg){width=100% height=70%}\n\n## 4) Regularization\n\nWith early stopping, a model is trained and periodically evaluated on a validation set, e.g., after each epoch. Training is stopped if no significant improvement is achieved after x evaluation cycles.\n\n## Early Stopping\n\n![Source: [Link](https://www.oreilly.com/library/view/hands-on-transfer-learning/9781788831307/41172567-9482-4cad-ac87-1cfbd46026df.xhtml)]({{< meta params.images_path >}}early_stopping.png){width=100% height=70%}\n\n## 4) Regularization\n\nEarly stopping in PyTorch.\n\n```python\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)\n```\n\n## 4) Regularization\n\nWith *weight decay*, a model can be regularized. The update step in gradient descent is modified.\n\n\\begin{equation}\n\\theta_{t+1} = \\theta_t (1 - \\lambda) - \\eta \\nabla J(\\theta)\n\\end{equation}\n\nWhere $t$ is the iteration, $\\theta$ the model parameters, $\\eta$ the learning rate, and $\\lambda$ the decay parameter.\n\n## 4) Regularization\n\nTransfer learning involves adapting a pre-trained model on a large dataset (e.g., ImageNet) to a new task. The last layer is removed and replaced according to the new task. The network is then further trained. Layers can be frozen (weights not updated) or fine-tuned (weights further trained).\n\n## Transfer Learning\n\n![Source: @johnson_eecs_2021]({{< meta params.images_path >}}transfer_learning.jpg){width=100% height=70%}\n\n## 4) Regularization\n\nIn PyTorch, you can freeze the parameters:\n\n```python\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n```\n\n# 5 - Tuning\n\n## 5) Hyper-Parameter Tuning\n\nIn this step, different hyperparameters and architectures are systematically evaluated. Techniques such as grid search or random search can be used, with random search being preferred.\n\n## 5) Hyper-Parameter Tuning\n\nParameterized architecture:\n\n```python\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```\n\n# 6 - Squeeze\n\n## 6) Squeeze out the Juice\n\nAfter finding the best architectures and hyperparameters, there are further ways to squeeze out more performance.\n\n## 6) Squeeze out the Juice\n\nModel ensembling.\n\n## 6) Squeeze out the Juice\n\nTrain longer.\n\n## Double Descent\n\n![Source: @nakkiran_deep_2019]({{< meta params.images_path >}}double_descent.jpg){width=70%}\n\n## 6) Squeeze out the Juice\n\nOther training techniques:\n\n- Special optimizer (AdamW)\n- Complex data augmentation techniques (Mixup, Cutmix, RandAugment)\n- Regularization techniques (Stochastic Depth)\n- Label smoothing\n\n## HuggingFace\n\n[HuggingFace](https://huggingface.co/)\n\n## timm\n\n[PyTorch Image Models (timm)](https://github.com/rwightman/pytorch-image-models)\n\n## Links\n\n- [DS-cookie cutter](https://github.com/drivendata/cookiecutter-data-science)\n- [PyTorch Lightning](https://github.com/PyTorchLightning/deep-learning-project-template)\n- [Hydra](https://hydra.cc/docs/intro/)\n- [Weights & Biases](https://wandb.ai/site)\n- [Neptune AI](https://neptune.ai/experiment-tracking)\n- [Version Control Systems for ML Projects](https://dvc.org/)\n\n# References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}